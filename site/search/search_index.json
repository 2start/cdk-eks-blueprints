{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"ci-cd/","text":"CI/CD \u00b6 IaC Pipeline \u00b6 (work in progress) Example of IaC self-mutating pipeline based on CodePipeline can be found in the lib/pipelineStack.ts .","title":"CI/CD"},{"location":"ci-cd/#cicd","text":"","title":"CI/CD"},{"location":"ci-cd/#iac-pipeline","text":"(work in progress) Example of IaC self-mutating pipeline based on CodePipeline can be found in the lib/pipelineStack.ts .","title":"IaC Pipeline"},{"location":"cluster-management/","text":"// TODO","title":"Cluster Management"},{"location":"core-concepts/","text":"","title":"Core Concepts"},{"location":"getting-started/","text":"Getting Started \u00b6 This getting started guide will walk you through setting up a new CDK project which leverages the cdk-eks-blueprint NPM module to deploy a simple SSP. Project setup \u00b6 The quickstart leverages AWS Cloud Development Kit (CDK) . Install CDK via the following. npm install -g aws-cdk Verify the installation. cdk --version Create a new typescript CDK project in an empty directory. cdk init app --language typescript Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is an process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. Bootstrap your environment with the following command. cdk bootstrap aws://<AWS_ACCOUNT_ID>/<AWS_REGION> Deploy a Blueprint EKS Cluster \u00b6 Install the cdk-eks-blueprint NPM package via the following. npm i @shapirov/cdk-eks-blueprint Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import { EksBluepint , Addons } from '@shapirov/cdk-eks-blueprint' ; const addOns : Array < ClusterAddOn > = [ new Addons . ArgoCDAddOn , ]; const app = new cdk . App (); const opts = { id : 'east-test-1' , addOns } new EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXX' , region : 'us-east-2' }, }); Deploy the stack using the following command cdk deploy Congratulations! You have deployed your first EKS cluster with cdk-eks-blueprint . Onboard a Team \u00b6 Now that we have our cluster deployed, it is time to onboard our first team. In its simplest form, a team is represented in a Kubernetes cluster by a namespace. In order to create a new team and namespace within your cluster do the following: Create a new file called team-awesome . touch team-awesome.tsx Paste the following code the file. import { ClusterInfo , ApplicationTeam } from '../../stacks/eks-blueprint-stack' ; export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } Replace the contents of bin/main.tsx with the following: import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import { EksBluepint , Addons , Team } from '@shapirov/cdk-eks-blueprint' ; import { TeamAwesome } from '../teams/team-awesome' const addOns : Array < ClusterAddOn > = [ new Addons . ArgoCDAddOn , ]; const teams : Array < Team > = [ new TeamAwesome ] const app = new cdk . App (); const opts = { id : 'east-test-1' , addOns , teams } new EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXX' , region : 'us-east-2' }, }); Deploy the stack using the following command cdk deploy To confirm your team has been added to the cluster, run: kubectl get ns team-awesome Deploy workloads with ArgoCD \u00b6 Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads arcoss multiple namespaces. The sample app of apps repository that we use in this walkthrough can be found here . Install ArgoCD CLI \u00b6 Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd : v2 . 0.1 + 33 eaf11 . dirty Exposing ArgoCD \u00b6 To access the ArgoCD running in your Kubernetes cluster, we simply need leverage port-forwarding. To do so, first capture the service name in an environment variable. export ARGO_SERVER =$ ( kubectl get svc - n argocd - l app . kubernetes . io / name = argocd - server - o name ) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the Argo login screen. Logging Into ArgoCD \u00b6 ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD =$ ( kubectl - n argocd get secret argocd - initial - admin - secret - o jsonpath = \"{.data.password}\" | base64 - d ) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD Deploy workloads to your cluster \u00b6 Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc -s https://github.com/kcoleman731/argo-apps.git Create the application within Argo by running the following command argocd app create sample-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/kcoleman731/argo-apps.git \\ --path \".\" Sync the apps by running the following command argocd app sync sample-apps Validate deployments. \u00b6 To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-burnham . kubectl port-forward svc/guestbook-ui -n team-burnham 4040:80 Open up localhost:4040 in your browser and you should see the application.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This getting started guide will walk you through setting up a new CDK project which leverages the cdk-eks-blueprint NPM module to deploy a simple SSP.","title":"Getting Started"},{"location":"getting-started/#project-setup","text":"The quickstart leverages AWS Cloud Development Kit (CDK) . Install CDK via the following. npm install -g aws-cdk Verify the installation. cdk --version Create a new typescript CDK project in an empty directory. cdk init app --language typescript Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is an process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. Bootstrap your environment with the following command. cdk bootstrap aws://<AWS_ACCOUNT_ID>/<AWS_REGION>","title":"Project setup"},{"location":"getting-started/#deploy-a-blueprint-eks-cluster","text":"Install the cdk-eks-blueprint NPM package via the following. npm i @shapirov/cdk-eks-blueprint Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import { EksBluepint , Addons } from '@shapirov/cdk-eks-blueprint' ; const addOns : Array < ClusterAddOn > = [ new Addons . ArgoCDAddOn , ]; const app = new cdk . App (); const opts = { id : 'east-test-1' , addOns } new EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXX' , region : 'us-east-2' }, }); Deploy the stack using the following command cdk deploy Congratulations! You have deployed your first EKS cluster with cdk-eks-blueprint .","title":"Deploy a Blueprint EKS Cluster"},{"location":"getting-started/#onboard-a-team","text":"Now that we have our cluster deployed, it is time to onboard our first team. In its simplest form, a team is represented in a Kubernetes cluster by a namespace. In order to create a new team and namespace within your cluster do the following: Create a new file called team-awesome . touch team-awesome.tsx Paste the following code the file. import { ClusterInfo , ApplicationTeam } from '../../stacks/eks-blueprint-stack' ; export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } Replace the contents of bin/main.tsx with the following: import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import { EksBluepint , Addons , Team } from '@shapirov/cdk-eks-blueprint' ; import { TeamAwesome } from '../teams/team-awesome' const addOns : Array < ClusterAddOn > = [ new Addons . ArgoCDAddOn , ]; const teams : Array < Team > = [ new TeamAwesome ] const app = new cdk . App (); const opts = { id : 'east-test-1' , addOns , teams } new EksBlueprint ( app , opts , { env : { account : 'XXXXXXXXXXXX' , region : 'us-east-2' }, }); Deploy the stack using the following command cdk deploy To confirm your team has been added to the cluster, run: kubectl get ns team-awesome","title":"Onboard a Team"},{"location":"getting-started/#deploy-workloads-with-argocd","text":"Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads arcoss multiple namespaces. The sample app of apps repository that we use in this walkthrough can be found here .","title":"Deploy workloads with ArgoCD"},{"location":"getting-started/#install-argocd-cli","text":"Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd : v2 . 0.1 + 33 eaf11 . dirty","title":"Install ArgoCD CLI"},{"location":"getting-started/#exposing-argocd","text":"To access the ArgoCD running in your Kubernetes cluster, we simply need leverage port-forwarding. To do so, first capture the service name in an environment variable. export ARGO_SERVER =$ ( kubectl get svc - n argocd - l app . kubernetes . io / name = argocd - server - o name ) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the Argo login screen.","title":"Exposing ArgoCD"},{"location":"getting-started/#logging-into-argocd","text":"ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD =$ ( kubectl - n argocd get secret argocd - initial - admin - secret - o jsonpath = \"{.data.password}\" | base64 - d ) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD","title":"Logging Into ArgoCD"},{"location":"getting-started/#deploy-workloads-to-your-cluster","text":"Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc -s https://github.com/kcoleman731/argo-apps.git Create the application within Argo by running the following command argocd app create sample-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/kcoleman731/argo-apps.git \\ --path \".\" Sync the apps by running the following command argocd app sync sample-apps","title":"Deploy workloads to your cluster"},{"location":"getting-started/#validate-deployments","text":"To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-burnham . kubectl port-forward svc/guestbook-ui -n team-burnham 4040:80 Open up localhost:4040 in your browser and you should see the application.","title":"Validate deployments."},{"location":"overview/","text":"Overview \u00b6 Welcome to the Amazon EKS SSP Quickstart documentation site. Amazon EKS SSP Quickstart provides a framework and methodology that makes it easy for customers build Shared Services Platform (SSP) on top of Amazon EKS . What is an SSP? \u00b6 A Shared Services Platform (SSP) is an interenal development platform that abstracts the complexities of cloud infrastrucuture from developers, and allows them to deploy workloads with ease. As SSP is typically composed of multiple AWS or open source products and services, including services for running containers, CI/CD pipelines, capturing logs/metrics, and security enforcement. The SSP packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, SSPs allow companies to consolidate tools and best practices for securing, scaling, monitoring, and operating containerized infrastructure into a central platform that can then be used by developers across an enterprise. What can I do with the Quickstart? \u00b6 Customers can use Amazon EKS SSP Quickstart to: Deploy batteries included EKS clusters across multiple accounts and regions. Manage configuration for all of your cluster from a single Git repository. Manage the set of addons that are provisioned in each cluster. Leverage Gitops-based workflows to onboard and manage workloads. Define teams, namespaces, and their associated access permissions. Integrate cluster access with IAM or OIDC provider of your choosing.","title":"Overview"},{"location":"overview/#overview","text":"Welcome to the Amazon EKS SSP Quickstart documentation site. Amazon EKS SSP Quickstart provides a framework and methodology that makes it easy for customers build Shared Services Platform (SSP) on top of Amazon EKS .","title":"Overview"},{"location":"overview/#what-is-an-ssp","text":"A Shared Services Platform (SSP) is an interenal development platform that abstracts the complexities of cloud infrastrucuture from developers, and allows them to deploy workloads with ease. As SSP is typically composed of multiple AWS or open source products and services, including services for running containers, CI/CD pipelines, capturing logs/metrics, and security enforcement. The SSP packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, SSPs allow companies to consolidate tools and best practices for securing, scaling, monitoring, and operating containerized infrastructure into a central platform that can then be used by developers across an enterprise.","title":"What is an SSP?"},{"location":"overview/#what-can-i-do-with-the-quickstart","text":"Customers can use Amazon EKS SSP Quickstart to: Deploy batteries included EKS clusters across multiple accounts and regions. Manage configuration for all of your cluster from a single Git repository. Manage the set of addons that are provisioned in each cluster. Leverage Gitops-based workflows to onboard and manage workloads. Define teams, namespaces, and their associated access permissions. Integrate cluster access with IAM or OIDC provider of your choosing.","title":"What can I do with the Quickstart?"},{"location":"teams/","text":"Teams \u00b6 The cdk-eks-blueprint framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team . ApplicationTeam \u00b6 To create an ApplicationTeam for your cluster, simplye implement a class that extends ApplicationTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace. PlatformTeam \u00b6 To create an PlatformTeam for your cluster, simplye implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRole : Role.fromRoleArn ( ` ${ YOUR_ROLE_ARN } ` ); }) DefaultTeamRoles \u00b6 The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services Team Benefits \u00b6 By managing teams via infrastrucutre as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access. Cluster Access ( kubectl ) \u00b6 The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${ teamname } teamrole arn:aws:iam:: ${ account } :role/west-dev- ${ teamname } AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam:: ${ account } :role/west-dev- ${ platform - team - name } AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam:: ${ account } :role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam:: ${ account } :role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example. Console Access \u00b6 Provided that each team has recieved the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team. Examples \u00b6 There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Teams"},{"location":"teams/#teams","text":"The cdk-eks-blueprint framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team .","title":"Teams"},{"location":"teams/#applicationteam","text":"To create an ApplicationTeam for your cluster, simplye implement a class that extends ApplicationTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace.","title":"ApplicationTeam"},{"location":"teams/#platformteam","text":"To create an PlatformTeam for your cluster, simplye implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRole : Role.fromRoleArn ( ` ${ YOUR_ROLE_ARN } ` ); })","title":"PlatformTeam"},{"location":"teams/#defaultteamroles","text":"The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services","title":"DefaultTeamRoles"},{"location":"teams/#team-benefits","text":"By managing teams via infrastrucutre as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access.","title":"Team Benefits"},{"location":"teams/#cluster-access-kubectl","text":"The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${ teamname } teamrole arn:aws:iam:: ${ account } :role/west-dev- ${ teamname } AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam:: ${ account } :role/west-dev- ${ platform - team - name } AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam:: ${ account } :role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam:: ${ account } :role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example.","title":"Cluster Access (kubectl)"},{"location":"teams/#console-access","text":"Provided that each team has recieved the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team.","title":"Console Access"},{"location":"teams/#examples","text":"There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Examples"},{"location":"addons/cluster-autoscaler/","text":"Cluster Autoscaler Add-on \u00b6 Usage \u00b6 import { ClusterAutoScaler } from '@shapirov/cdk-eks-blueprint' ; readonly myClusterAutoscaler = new ClusterAutoscaler ( \"v1.19.1\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Functionality \u00b6 Adds proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) to the NodeGroup IAM role. Configures service account, cluster roles, roles, role bindings and deployment. Resolves proper CA image to pull based on the Kubernetes version. Configuration allows passing a specific version of the image to pull. Applies proper tags for discoverability to the EC2 instances.","title":"Cluster Autoscaler"},{"location":"addons/cluster-autoscaler/#cluster-autoscaler-add-on","text":"","title":"Cluster Autoscaler Add-on"},{"location":"addons/cluster-autoscaler/#usage","text":"import { ClusterAutoScaler } from '@shapirov/cdk-eks-blueprint' ; readonly myClusterAutoscaler = new ClusterAutoscaler ( \"v1.19.1\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/cluster-autoscaler/#functionality","text":"Adds proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) to the NodeGroup IAM role. Configures service account, cluster roles, roles, role bindings and deployment. Resolves proper CA image to pull based on the Kubernetes version. Configuration allows passing a specific version of the image to pull. Applies proper tags for discoverability to the EC2 instances.","title":"Functionality"},{"location":"addons/container-insights/","text":"Container Insights Addon \u00b6 The ContainerInsights addon adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Usage \u00b6 import { addons } from '@shapirov/cdk-eks-blueprint' ; const myClusterAutoscaler = new addons . ContainerInsights (); const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Container Insights"},{"location":"addons/container-insights/#container-insights-addon","text":"The ContainerInsights addon adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices.","title":"Container Insights Addon"},{"location":"addons/container-insights/#usage","text":"import { addons } from '@shapirov/cdk-eks-blueprint' ; const myClusterAutoscaler = new addons . ContainerInsights (); const addOns : Array < ClusterAddOn > = [ myClusterAutoscaler ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/overview/","text":"Addons \u00b6 Supported Addons \u00b6 AddOn Description AppMeshAddon Adds an AppMesh controller and CRDs (pending validation on the latest version of CDK) ArgoCDAddon Adds an ArgoCD controller CalicoAddon Adds the Calico 1.7.1 CNI/Network policy engine ContainerInsightsAddon Adds Container Insights support integrating monitoring with CloudWatch ClusterAutoscalerAddon Adds the standard cluster autoscaler ( Karpenter is coming) MetricsServerAddon Adds metrics server (pre-req for HPA and other monitoring tools) NginxAddon Adds NGINX ingress controller","title":"Overview"},{"location":"addons/overview/#addons","text":"","title":"Addons"},{"location":"addons/overview/#supported-addons","text":"AddOn Description AppMeshAddon Adds an AppMesh controller and CRDs (pending validation on the latest version of CDK) ArgoCDAddon Adds an ArgoCD controller CalicoAddon Adds the Calico 1.7.1 CNI/Network policy engine ContainerInsightsAddon Adds Container Insights support integrating monitoring with CloudWatch ClusterAutoscalerAddon Adds the standard cluster autoscaler ( Karpenter is coming) MetricsServerAddon Adds metrics server (pre-req for HPA and other monitoring tools) NginxAddon Adds NGINX ingress controller","title":"Supported Addons"},{"location":"cluster-providers/ec2-cluster-provider/","text":"EC2 Cluster Provider \u00b6 Stack Configuration \u00b6 Supports context variables (specify in cdk.json, cdk.context.json or pass with -c command line option): instanceType : (defaulted to \"t3.medium\") Type of instance for the EKS cluster, must be a valid instance type like t3.medium vpc : Specifies whether to use an existing VPC (if specified) or create a new one if not specified. minSize : Min cluster size, must be positive integer greater than 0 (default 1). maxSize : Max cluster size, must be greater than minSize. vpcSubnets : List of VPC subnets for cluster provisioning (unsupported yet)","title":"EC2 Cluster Provider"},{"location":"cluster-providers/ec2-cluster-provider/#ec2-cluster-provider","text":"","title":"EC2 Cluster Provider"},{"location":"cluster-providers/ec2-cluster-provider/#stack-configuration","text":"Supports context variables (specify in cdk.json, cdk.context.json or pass with -c command line option): instanceType : (defaulted to \"t3.medium\") Type of instance for the EKS cluster, must be a valid instance type like t3.medium vpc : Specifies whether to use an existing VPC (if specified) or create a new one if not specified. minSize : Min cluster size, must be positive integer greater than 0 (default 1). maxSize : Max cluster size, must be greater than minSize. vpcSubnets : List of VPC subnets for cluster provisioning (unsupported yet)","title":"Stack Configuration"},{"location":"cluster-providers/fargate-cluster-provider/","text":"","title":"Fargate Cluster Provider"},{"location":"cluster-providers/outpost-cluster-provider/","text":"","title":"Outpost Cluster Provider"},{"location":"internal/readme-internal/","text":"Description \u00b6 This is an internal readme for development processes that should be followed for this repository. Local Development \u00b6 This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build. Publishing \u00b6 At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): npm run build (compile) npm publish (this will require credentials to npm) Submitting Changes \u00b6 For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Description"},{"location":"internal/readme-internal/#description","text":"This is an internal readme for development processes that should be followed for this repository.","title":"Description"},{"location":"internal/readme-internal/#local-development","text":"This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build.","title":"Local Development"},{"location":"internal/readme-internal/#publishing","text":"At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): npm run build (compile) npm publish (this will require credentials to npm)","title":"Publishing"},{"location":"internal/readme-internal/#submitting-changes","text":"For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Submitting Changes"}]}