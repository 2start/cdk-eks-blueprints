{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Welcome to the Amazon EKS SSP Quick Start documentation site. Amazon EKS SSP Quick Start provides AWS customers with a framework and methodology that makes it easy to build a Shared Services Platform (SSP) on top of Amazon EKS . What is an SSP? \u00b6 A Shared Services Platform (SSP) is an internal development platform that abstracts the complexities of cloud infrastructure from developers, and allows them to deploy workloads with ease. As SSP is typically composed of multiple AWS or open source products and services, including services for running containers, CI/CD pipelines, capturing logs/metrics, and security enforcement. The SSP packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, SSPs allow companies to consolidate tools and best practices for securing, scaling, monitoring, and operating containerized infrastructure into a central platform that can then be used by developers across an enterprise. What can I do with this QuickStart? \u00b6 Customers can use this Quick Start to easily architect and deploy a multi-tenant SSP built on EKS. Specifically, customers can leverage the ssp-amazon-eks module to: Deploy Well-Architected EKS clusters across any number of accounts and regions. Manage cluster configuration, including add-ons that run in each cluster, from a single Git repository. Define teams, namespaces, and their associated access permissions for your clusters. Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure. Leverage GitOps-based workflows for onboarding and managing workloads for your teams. Examples \u00b6 To view a library of examples for how you can leverage the ssp-amazon-eks , please see our SSP Patterns Repository .","title":"Overview"},{"location":"#overview","text":"Welcome to the Amazon EKS SSP Quick Start documentation site. Amazon EKS SSP Quick Start provides AWS customers with a framework and methodology that makes it easy to build a Shared Services Platform (SSP) on top of Amazon EKS .","title":"Overview"},{"location":"#what-is-an-ssp","text":"A Shared Services Platform (SSP) is an internal development platform that abstracts the complexities of cloud infrastructure from developers, and allows them to deploy workloads with ease. As SSP is typically composed of multiple AWS or open source products and services, including services for running containers, CI/CD pipelines, capturing logs/metrics, and security enforcement. The SSP packages these tools into a cohesive whole and makes them available to development teams as a service. From an operational perspective, SSPs allow companies to consolidate tools and best practices for securing, scaling, monitoring, and operating containerized infrastructure into a central platform that can then be used by developers across an enterprise.","title":"What is an SSP?"},{"location":"#what-can-i-do-with-this-quickstart","text":"Customers can use this Quick Start to easily architect and deploy a multi-tenant SSP built on EKS. Specifically, customers can leverage the ssp-amazon-eks module to: Deploy Well-Architected EKS clusters across any number of accounts and regions. Manage cluster configuration, including add-ons that run in each cluster, from a single Git repository. Define teams, namespaces, and their associated access permissions for your clusters. Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure. Leverage GitOps-based workflows for onboarding and managing workloads for your teams.","title":"What can I do with this QuickStart?"},{"location":"#examples","text":"To view a library of examples for how you can leverage the ssp-amazon-eks , please see our SSP Patterns Repository .","title":"Examples"},{"location":"cluster-management/","text":"Multi-Cluster Management \u00b6 Multi-cluster management refers to the strategies associated with managing and updating cluster configuration across many Amazon EKS clusters. Infrastructure as code (IaC) tools like the AWS CDK provides the ability to bring automation and consistency when deploying your clusters. You need the ability to apply the same configurations to as many of your clusters as necessary and by defining all of your resources via Infrastructure as Code (IaC), it removes the problem of having to generate or apply custom YAML files for each one of your clusters allowing your teams to move faster. Defining your clusters resources using the AWS CDK allows your teams to focus on the underlying workloads as the infrastructure components will be taken care of via the AWS CDK. The main benefits organizations can see using the AWS CDK to manage their Amazon EKS clusters can be summarized as follows: - Consistency across all clusters and environments - Streamlining access control across your organization - Ease of management for multiple clusters - Access to GitOps methodologies and best practices - Automated lifecycle management for cluster deployment The Amazon EKS SSP Quick Start references the ssp-eks-patterns repository repository that includes examples of different deployment patterns and options which includes patterns for multi-cluster that can be deployed across multiple regions. If you take a look at the main.ts file in the patterns repository, you will notice that the stacks that define our Amazon EKS clusters and associated pipelines that are deployed to different regions as shown in the snippet below: #!/usr/bin/env node import * as cdk from '@aws-cdk/core' ; const app = new cdk . App (); //------------------------------------------- // Single Cluster with multiple teams. //------------------------------------------- import MultiTeamConstruct from '../lib/multi-team-construct' new MultiTeamConstruct ( app , 'multi-team' ); //------------------------------------------- // Multiple clusters, multiple regions. //------------------------------------------- import MultiRegionConstruct from '../lib/multi-region-construct' new MultiRegionConstruct ( app , 'multi-region' ); //------------------------------------------- // Single Fargate cluster. //------------------------------------------- import FargateConstruct from '../lib/fargate-construct' new FargateConstruct ( app , 'fargate' ); //------------------------------------------- // Multiple clusters with deployment pipeline. //------------------------------------------- import PipelineStack from '../lib/pipeline-stack' const account = process . env . CDK_DEFAULT_ACCOUNT const region = process . env . CDK_DEFAULT_REGION const env = { account , region } new PipelineStack ( app , 'pipeline' , { env }); Using the AWS CDK, you can define the specific region to deploy your clusters using environment variables as a construct in Typescript as shown in the example above. If you were to deploy all the stacks in your main.ts file you would be able to view your running clusters by region by running the following command aws eks list-cluster --region <insert region> If for example you chose the region us-west-2, you would get a similar output: { \"clusters\" : [ \"all clusters in this region\" ] } Multi-Region Management \u00b6 In a production environment, it is common to have clusters that reside in different locations. This could be in different regions, on-prem, or follow a hybrid cloud model. Some of the common design patterns that come in to play when it comes to multi-cluster management across these different operational models include things like high availability, data replication, networking, traffic routing, and the underlying management of those clusters. In the ssp-ek-patterns/lib/multi-region-construct directory, you will find the index.ts file which shows a concrete example of how to deploy multiple clusters to different regions as shown below import * as cdk from '@aws-cdk/core' ; // SSP Lib import * as ssp from '@aws-quickstart/ssp-amazon-eks' // Team implementations import * as team from '../teams' export default class MultiRegionConstruct extends cdk . Construct { constructor ( scope : cdk.Construct , id : string ) { super ( scope , id ); // Setup platform team const accountID = process . env . CDK_DEFAULT_ACCOUNT ! const platformTeam = new team . TeamPlatform ( accountID ) const teams : Array < ssp . Team > = [ platformTeam ]; // AddOns for the cluster. const addOns : Array < ssp . ClusterAddOn > = [ new ssp . addons . NginxAddOn , new ssp . addons . ArgoCDAddOn , new ssp . addons . CalicoAddOn , new ssp . addons . MetricsServerAddOn , new ssp . addons . ClusterAutoScalerAddOn , new ssp . addons . ContainerInsightsAddOn , new ssp . addons . VpcCniAddOn (), new ssp . addons . CoreDnsAddOn (), new ssp . addons . KubeProxyAddOn () ]; const east = 'blueprint-us-east-2' new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ east } ` , addOns , teams }, { env : { region : east } }); const central = 'blueprint-us-central-2' new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ central } ` , addOns , teams }, { env : { region : central } }); const west = 'blueprint-us-west-2' new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ west } ` , addOns , teams }, { env : { region : west } }); } } This construct imports all of the core components of the Shared Services Platform library, Teams construct, and Addons as modules which then deploys our clusters to different regions. The main point to take away from this is that we are adding automation and consistency to our clusters as we deploy multiple clusters to multiple regions since all of our components have already been defined in the Shared Services Platform library along with Teams and Addons.","title":"Multi-Cluster Management"},{"location":"cluster-management/#multi-cluster-management","text":"Multi-cluster management refers to the strategies associated with managing and updating cluster configuration across many Amazon EKS clusters. Infrastructure as code (IaC) tools like the AWS CDK provides the ability to bring automation and consistency when deploying your clusters. You need the ability to apply the same configurations to as many of your clusters as necessary and by defining all of your resources via Infrastructure as Code (IaC), it removes the problem of having to generate or apply custom YAML files for each one of your clusters allowing your teams to move faster. Defining your clusters resources using the AWS CDK allows your teams to focus on the underlying workloads as the infrastructure components will be taken care of via the AWS CDK. The main benefits organizations can see using the AWS CDK to manage their Amazon EKS clusters can be summarized as follows: - Consistency across all clusters and environments - Streamlining access control across your organization - Ease of management for multiple clusters - Access to GitOps methodologies and best practices - Automated lifecycle management for cluster deployment The Amazon EKS SSP Quick Start references the ssp-eks-patterns repository repository that includes examples of different deployment patterns and options which includes patterns for multi-cluster that can be deployed across multiple regions. If you take a look at the main.ts file in the patterns repository, you will notice that the stacks that define our Amazon EKS clusters and associated pipelines that are deployed to different regions as shown in the snippet below: #!/usr/bin/env node import * as cdk from '@aws-cdk/core' ; const app = new cdk . App (); //------------------------------------------- // Single Cluster with multiple teams. //------------------------------------------- import MultiTeamConstruct from '../lib/multi-team-construct' new MultiTeamConstruct ( app , 'multi-team' ); //------------------------------------------- // Multiple clusters, multiple regions. //------------------------------------------- import MultiRegionConstruct from '../lib/multi-region-construct' new MultiRegionConstruct ( app , 'multi-region' ); //------------------------------------------- // Single Fargate cluster. //------------------------------------------- import FargateConstruct from '../lib/fargate-construct' new FargateConstruct ( app , 'fargate' ); //------------------------------------------- // Multiple clusters with deployment pipeline. //------------------------------------------- import PipelineStack from '../lib/pipeline-stack' const account = process . env . CDK_DEFAULT_ACCOUNT const region = process . env . CDK_DEFAULT_REGION const env = { account , region } new PipelineStack ( app , 'pipeline' , { env }); Using the AWS CDK, you can define the specific region to deploy your clusters using environment variables as a construct in Typescript as shown in the example above. If you were to deploy all the stacks in your main.ts file you would be able to view your running clusters by region by running the following command aws eks list-cluster --region <insert region> If for example you chose the region us-west-2, you would get a similar output: { \"clusters\" : [ \"all clusters in this region\" ] }","title":"Multi-Cluster Management"},{"location":"cluster-management/#multi-region-management","text":"In a production environment, it is common to have clusters that reside in different locations. This could be in different regions, on-prem, or follow a hybrid cloud model. Some of the common design patterns that come in to play when it comes to multi-cluster management across these different operational models include things like high availability, data replication, networking, traffic routing, and the underlying management of those clusters. In the ssp-ek-patterns/lib/multi-region-construct directory, you will find the index.ts file which shows a concrete example of how to deploy multiple clusters to different regions as shown below import * as cdk from '@aws-cdk/core' ; // SSP Lib import * as ssp from '@aws-quickstart/ssp-amazon-eks' // Team implementations import * as team from '../teams' export default class MultiRegionConstruct extends cdk . Construct { constructor ( scope : cdk.Construct , id : string ) { super ( scope , id ); // Setup platform team const accountID = process . env . CDK_DEFAULT_ACCOUNT ! const platformTeam = new team . TeamPlatform ( accountID ) const teams : Array < ssp . Team > = [ platformTeam ]; // AddOns for the cluster. const addOns : Array < ssp . ClusterAddOn > = [ new ssp . addons . NginxAddOn , new ssp . addons . ArgoCDAddOn , new ssp . addons . CalicoAddOn , new ssp . addons . MetricsServerAddOn , new ssp . addons . ClusterAutoScalerAddOn , new ssp . addons . ContainerInsightsAddOn , new ssp . addons . VpcCniAddOn (), new ssp . addons . CoreDnsAddOn (), new ssp . addons . KubeProxyAddOn () ]; const east = 'blueprint-us-east-2' new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ east } ` , addOns , teams }, { env : { region : east } }); const central = 'blueprint-us-central-2' new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ central } ` , addOns , teams }, { env : { region : central } }); const west = 'blueprint-us-west-2' new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ west } ` , addOns , teams }, { env : { region : west } }); } } This construct imports all of the core components of the Shared Services Platform library, Teams construct, and Addons as modules which then deploys our clusters to different regions. The main point to take away from this is that we are adding automation and consistency to our clusters as we deploy multiple clusters to multiple regions since all of our components have already been defined in the Shared Services Platform library along with Teams and Addons.","title":"Multi-Region Management"},{"location":"core-concepts/","text":"Core Concepts \u00b6 This document provides a high level overview of the Core Concepts that are embedded in the ssp-amazon-eks framework. For the purposes of this document, we will assume the reader is familiar with Git, Docker, Kubernetes and AWS. Concept Description Blueprint A blueprint combines clusters , add-ons , and teams into a cohesive object that can deployed as a whole Cluster A Well-Architected EKS Cluster. Resource Provider Resource providers are abstractions that supply external AWS resources to the cluster (e.g. hosted zones, VPCs, etc.) Add-on Allow you to configure, deploy, and update the operational software, or add-ons, that provide key functionality to support your Kubernetes applications. Team A logical grouping of IAM identities that have access to a Kubernetes namespace(s). Pipeline Continuous Delivery pipelines for deploying clusters and add-ons . Application An application that runs within an EKS Cluster. Blueprint \u00b6 The ssp-amazon-eks framework allows you to configure and deploy what we call blueprint clusters. A blueprint consists of an EKS cluster, a set of add-ons that will be deployed into the cluster, and a set of teams who will have access to a cluster. Once a blueprint is configured, it can be easily deployed across any number of AWS accounts and regions. Blueprints also leverage GitOps tooling to facilitate cluster bootstrapping and workload onboarding. To view sample blueprint implementations, please see our patterns repository . Cluster \u00b6 A cluster is simply an EKS cluster. The ssp-amazon-eks framework provides for customizing the compute options you leverage with your clusters . The framework currently supports EC2 , Fargate and BottleRocket instances. To specify the type of compute you want to use for your cluster , you supply a ClusterProvider object to your blueprint . The framework defaults to leveraging the EC2ClusterProvider . Each ClusterProvider provides additional configuration options as well. For example, the MngClusterProvider allows you to configure instance types, min and max instance counts, and amiType, among other options. See our Cluster Providers documentation page for detailed information. Resource Provider \u00b6 A resource is a CDK construct that implements IResource interface from @aws-cdk/core which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 IHostedZone , an ACM certificate ICertificate , a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams. ResourceProviders enable customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint. See our Resource Providers documentation page for detailed information. Add-on \u00b6 Add-ons allow you to configure the tools and services that you would like to run in order to support your EKS workloads. When you configure add-ons for a blueprint , the add-ons will be provisioned at deploy time. Add-ons can deploy both Kubernetes specific resources and AWS resources needed to support add-on functionality. For example, the MetricsServerAddOn only deploys the Kubernetes manifests that are needed to run the Kubernetes Metrics Server (as a Helm chart). By contrast, the AWSLoadBalancerControllerAddon deploys Kubernetes resources, in addition to creating resources via AWS APIs that are needed to support the AWS Load Balancer Controller. The most common case to address via an add-on is configuration of IAM roles and permissions and the Kubernetes service account, leveraging IRSA to access AWS resources. See our Add-ons documentation page for detailed information. Team \u00b6 Teams allow you to configure the logical grouping of users that have access to your EKS clusters, in addition to the access permissions they are granted. The framework currently supports two types of teams : ApplicationTeam and PlatformTeam . ApplicationTeam members are granted access to specific namespaces. PlatformTeam members are granted administrative access to your clusters. See our Teams documentation page for detailed information. Pipeline \u00b6 Pipelines allow you to configure Continuous Delivery (CD) pipelines for your cluster blueprints that are directly integrated with your Git provider. See our Pipelines documentation page for detailed information. Application \u00b6 Applications represent the actual workloads that run within a Kubernetes cluster. The framework leverages a GitOps approach for deploying applications onto clusters. See our Workload Bootstrapping documentation for detailed information.","title":"Core Concepts"},{"location":"core-concepts/#core-concepts","text":"This document provides a high level overview of the Core Concepts that are embedded in the ssp-amazon-eks framework. For the purposes of this document, we will assume the reader is familiar with Git, Docker, Kubernetes and AWS. Concept Description Blueprint A blueprint combines clusters , add-ons , and teams into a cohesive object that can deployed as a whole Cluster A Well-Architected EKS Cluster. Resource Provider Resource providers are abstractions that supply external AWS resources to the cluster (e.g. hosted zones, VPCs, etc.) Add-on Allow you to configure, deploy, and update the operational software, or add-ons, that provide key functionality to support your Kubernetes applications. Team A logical grouping of IAM identities that have access to a Kubernetes namespace(s). Pipeline Continuous Delivery pipelines for deploying clusters and add-ons . Application An application that runs within an EKS Cluster.","title":"Core Concepts"},{"location":"core-concepts/#blueprint","text":"The ssp-amazon-eks framework allows you to configure and deploy what we call blueprint clusters. A blueprint consists of an EKS cluster, a set of add-ons that will be deployed into the cluster, and a set of teams who will have access to a cluster. Once a blueprint is configured, it can be easily deployed across any number of AWS accounts and regions. Blueprints also leverage GitOps tooling to facilitate cluster bootstrapping and workload onboarding. To view sample blueprint implementations, please see our patterns repository .","title":"Blueprint"},{"location":"core-concepts/#cluster","text":"A cluster is simply an EKS cluster. The ssp-amazon-eks framework provides for customizing the compute options you leverage with your clusters . The framework currently supports EC2 , Fargate and BottleRocket instances. To specify the type of compute you want to use for your cluster , you supply a ClusterProvider object to your blueprint . The framework defaults to leveraging the EC2ClusterProvider . Each ClusterProvider provides additional configuration options as well. For example, the MngClusterProvider allows you to configure instance types, min and max instance counts, and amiType, among other options. See our Cluster Providers documentation page for detailed information.","title":"Cluster"},{"location":"core-concepts/#resource-provider","text":"A resource is a CDK construct that implements IResource interface from @aws-cdk/core which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 IHostedZone , an ACM certificate ICertificate , a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams. ResourceProviders enable customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint. See our Resource Providers documentation page for detailed information.","title":"Resource Provider"},{"location":"core-concepts/#add-on","text":"Add-ons allow you to configure the tools and services that you would like to run in order to support your EKS workloads. When you configure add-ons for a blueprint , the add-ons will be provisioned at deploy time. Add-ons can deploy both Kubernetes specific resources and AWS resources needed to support add-on functionality. For example, the MetricsServerAddOn only deploys the Kubernetes manifests that are needed to run the Kubernetes Metrics Server (as a Helm chart). By contrast, the AWSLoadBalancerControllerAddon deploys Kubernetes resources, in addition to creating resources via AWS APIs that are needed to support the AWS Load Balancer Controller. The most common case to address via an add-on is configuration of IAM roles and permissions and the Kubernetes service account, leveraging IRSA to access AWS resources. See our Add-ons documentation page for detailed information.","title":"Add-on"},{"location":"core-concepts/#team","text":"Teams allow you to configure the logical grouping of users that have access to your EKS clusters, in addition to the access permissions they are granted. The framework currently supports two types of teams : ApplicationTeam and PlatformTeam . ApplicationTeam members are granted access to specific namespaces. PlatformTeam members are granted administrative access to your clusters. See our Teams documentation page for detailed information.","title":"Team"},{"location":"core-concepts/#pipeline","text":"Pipelines allow you to configure Continuous Delivery (CD) pipelines for your cluster blueprints that are directly integrated with your Git provider. See our Pipelines documentation page for detailed information.","title":"Pipeline"},{"location":"core-concepts/#application","text":"Applications represent the actual workloads that run within a Kubernetes cluster. The framework leverages a GitOps approach for deploying applications onto clusters. See our Workload Bootstrapping documentation for detailed information.","title":"Application"},{"location":"extensibility/","text":"Extensibility \u00b6 This guide provides an overview of extensibility options focusing on add-on extensions as the primary mechanism for the partners and customers. Overview \u00b6 SSP Framework is designed to be extensible. In the context of this guide, extensibility refers to the ability of customers and partners to both add new capabilities to the framework or platforms based on SSP as well as customize existing behavior, including the ability to modify or override existing behavior. The following abstractions can be leveraged to add new features to the framework: Add-on . Customers and partners can implement new add-ons which could be leveraged exactly the same way as the core add-ons (supplied by the framework). Resource Provider . This construct allows customers to create resources that can be reused across multiple add-ons and/or teams. For example, IAM roles, VPC, hosted zone. Cluster Provider . This construct allows creation of custom code that provisions an EKS cluster with node groups. It can be leveraged to extend behavior such as control plane customization, custom settings for node groups. Team . This abstraction allows to create team templates for application and platform teams and set custom setting for network isolation, policies (network, security), software wiring (auto injection of proxies, team specific service mesh configuration) and other extensions pertinent to the teams. Add-on Extensions \u00b6 In a general case, implementation of an add-on is a class which implements the ClusterAddOn interface. export declare interface ClusterAddOn { id? : string ; deploy ( clusterInfo : types.ClusterInfo ) : Promise < Construct > | void ; } Note : The add-on implementation can optionally supply the id attribute if the target add-on can be added to a blueprint more than once. Implementation of the add-on is expected to be an exported class that implements the interface and supplies the implementation of the deploy method. In order for the add-on to receive the deployment contextual information about the provisioned cluster, region, resource providers and/or other add-ons, the deploy method takes the ClusterInfo parameter (see types ), which represents a structure defined in the SPI (service provider interface) contracts. The API for the cluster info structure is stable and provides access to the provisioned EKS cluster, scheduled add-ons (that have not been installed yet but are part of the blueprint) or provisioned add-ons and other contexts. Post Deployment Hooks \u00b6 In certain cases, add-on provisioning may require logic to be executed after provisioning of the add-ons (and teams) is complete. For such cases, add-on can optionally implement ClusterPostDeploy interface. /** * Optional interface to allow cluster bootstrapping after provisioning of add-ons and teams is complete. * Can be leveraged to bootstrap workloads, perform cluster checks. * ClusterAddOn implementation may implement this interface in order to get post deployment hook point. */ export declare interface ClusterPostDeploy { postDeploy ( clusterInfo : types.ClusterInfo , teams : Team []) : void ; } This capability is leveraged for example in ArgoCD add-on to bootstrap workloads after all add-ons finished provisioning. Note, in this case unlike the standard deploy method implementation, the add-on also gets access to the provisioned teams. Helm Add-ons \u00b6 Helm add-ons are the most common case that generally combines provisioning of a helm chart as well as supporting infrastructure such as wiring of proper IAM policies for the Kubernetes service account, provisioning or configuring other AWS resources (VPC, subnets, node groups). In order to provide consistency across all Helm add-ons supplied by the SSP framework all Helm add-ons are implemented as derivatives of the HelmAddOn base class and support properties based on HelmAddOnUserProps . See the example extension section below for more details. Use cases that are enabled by leveraging the base HelmAddOn class: Consistency across all helm based add-on will reduce effort to understand how to apply and configure standard add-on options. Ability to override helm chart repository can enable leveraging private helm chart repository by the customer and facilitate add-on usage for private EKS clusters. Extensibility mechanisms available in the SSP framework can allow to intercept helm deployments and leverage GitOps driven add-on configuration. Non-helm Add-ons \u00b6 Add-ons that don't leverage helm but require to install arbitrary Kubernetes manifests will not be able to leverage the benefits provided by the HelmAddOn however, they are still relatively easy to implement. Deployment of arbitrary kubernetes manifests can leverage the following construct: import { KubernetesManifest } from \"@aws-cdk/aws-eks\" ; import { ClusterAddOn , ClusterInfo } from \"../../spi\" ; import { loadYaml , readYamlDocument } from \"../../utils/yaml-utils\" ; export class MyNonHelmAddOn implements ClusterAddOn { deploy ( clusterInfo : ClusterInfo ) : void { const cluster = clusterInfo . cluster ; // Apply manifest const doc = readYamlDocument ( __dirname + '/my-product.yaml' ); // ... apply any substitutions for dynamic values const manifest = docArray . split ( \"---\" ). map ( e => loadYaml ( e )); new KubernetesManifest ( cluster . stack , \"myproduct-manifest\" , { cluster , manifest , overwrite : true }); } } Note: When leveraging this approach consider how customers can apply the add-on for fully private clusters. It may be reasonable to bundle the manifest with the add-on in the npm package. Add-on Dependencies \u00b6 Add-ons can depend on other add-ons and that dependency may be soft or hard. Hard dependency implies that add-on provisioning must fail if the dependency is not available. For example, if an add-on requires access to AWS Secrets Manager for a secret containing a license key, credentials or other sensitive information, it can declare dependency on the CSI Secret Store Driver. Dependency management for direct hard dependency are implemented using a decorator @dependable . Example: import { Construct } from \"@aws-cdk/core\" ; import { ClusterInfo } from \"../../spi\" ; import { dependable } from \"../../utils\" ; import { HelmAddOn , HelmAddOnUserProps } from \"../helm-addon\" ; export class MyProductAddOn extends HelmAddOn { readonly options : MyProductAddOnProps ; // extends HelmAddOnUserProps ... @dependable ( 'AwsLoadBalancerControllerAddOn' ) // depends on AwsLoadBalancerController deploy ( clusterInfo : ClusterInfo ) : Promise < Construct > { ... } Passing Secrets to Add-ons \u00b6 Secrets from the AWS Secrets Manager or AWS Systems Manager Parameter Store can be made available as files mounted in Amazon EKS pods. It can be achieved with the help of AWS Secrets and Configuration Provider (ASCP) for the Kubernetes Secrets Store CSI Driver . The ASCP works with Amazon Elastic Kubernetes Service (Amazon EKS) 1.17+. More information on general concepts for leveraging ASCP can be found here . SSP Framework provides support for both Secrets Store CSI Driver as well as ASCP with the Secrets Store Add-on . Add-ons requiring support for secrets can declare dependency on the secret store add-on: export class MyAddOn extends ssp . addons . HelmAddOn { ... // Declares dependency on secret store add-on if secrets are needed. // Customers will have to explicitly add this add-on to the blueprint. @ssp . utils . dependable ( ssp . SecretsStoreAddOn . name ) deploy ( clusterInfo : ssp.ClusterInfo ) : Promise < Construct > { ... } In order to propagate the secret from the Secrets Manager to the Kubernetes cluster, the add-on should create a SecretProviderClass Kubernetes object. leveraging the ssp.addons.SecretProviderClass . The framework will take care of wiring the Kubernetes service account with the correct IAM permissions to pull the secret: const sa = clusterInfo . cluster . addServiceAccount (...); const csiSecret : ssp.addons.CsiSecretProps = { secretProvider : new ssp . LookupSecretsManagerSecretByName ( licenseKeySecret ), // the secret must be defined upfront and available in the region with the name specified in the licenseKeySecret kubernetesSecret : { secretName : 'my-addon-license-secret' , data : [ { key : 'licenseKey' } ] } }; const secretProviderClass = new ssp . addons . SecretProviderClass ( clusterInfo , sa , \"my-addon-license-secret-class\" , csiSecret ); Note: you can also leverage LookupSecretsManagerSecretByArn , LookupSsmSecretByAttrs or a custom implementation of the secret provider interface ssp.addons.SecretProvider . After the secret provider class is created, it should be mounted on any pod in the namespace to make the secret accessible. Mounting the secret volume also creates a regular Kubernetes Secret object based on the supplied description ( my-addon-license-secret ). This capability is controlled by the configuration of the SSP Secret Store add-on and is enabled by default. Many Helm charts provide options to mount additional volumes and mounts to the provisioned product. For example, a Helm chart (ArgoCD, FluentBit) allows specifying volumes and volumeMounts as the helm chart values. Mounting the secret in such cases is simple and does not require an additional pod for secrets. Here is an example of a secret volume and volume mount passed as values to a Helm chart: const chart = this . addHelmChart ( clusterInfo , { ... // standard values , volumes : [ { name : \"secrets-store-inline\" , csi : { driver : \"secrets-store.csi.k8s.io\" , readOnly : true , volumeAttributes : { secretProviderClass : \"my-addon-license-secret-class\" } } } ], volumeMounts : [ { name : \"secrets-store-inline\" , mountPath : \"/mnt/secret-store\" } ] }); After the secret volume is mounted (on any pod), you will see that a Kubernetes secret (for example my-addon-license-secret ) is also created in the target namespace. See the supplied code example for more details. Private Extensions \u00b6 Extensions specific to a customer instance of SSPs can be implemented inline with the blueprint in the same codebase. Such extensions are scoped to the customer base and cannot be reused. Example of a private extension: class MyAddOn extends HelmAddOn { constructor () { super ({ chart : 'mychart' , ... }); } deploy ( clusterInfo : ssp.ClusterInfo ) : Promise < Construct > { return Promise . resolve ( this . addHelmChart ( clusterInfo , {})); } } ssp . EksBlueprint . builder () . addOns ( new MyAddOn ()) . build ( app , 'my-extension-test-blueprint' ); Public Extensions \u00b6 The life-cycle of a public extension should be decoupled from the life-cycle of the SSP Quickstart main repository . When decoupled, extensions can be released at any arbitrary cadence specific to the extension, enabling better agility when it comes to new features or bug fixes. In order to enable this model the following workflow outline steps required to create and release a public extension: Public extensions are created in a separate repository. Public GitHub repository is preferred as it aligns with the open-source spirit of the framework and enables external reviews/feedback. Extensions are released and consumed as distinct public NPM packages. Public Extensions are expected to have sufficient documentation to allow customers to consume them independently. Documentation can reside in GitHub or external resources referenced in the documentation bundled with the extension. Public extensions are expected to be tested and validated against released SSP versions, e.g. with a CICD pipeline. Pipeline can be created with the pipelines support from the SSP framework or leveraging customer/partner specific tools. Partner Extensions \u00b6 Partner extensions (APN Partner) are expected to comply with the public extension workflow and additional items required to ensure proper validation and documentation support for a partner extension. Documentation PR should be created to the main SSP Quickstart repository to update the AddOns section. Example of add-on documentation can be found here along with the list of other add-ons. An example that shows a ready to use pattern leveraging the add-on should be submitted to the SSP Patterns Repository . This step will enable AWS PSAs to validate the add-on as well as provide a ready to use pattern to the customers, that could be copied/cloned in their SSP implementation. Example Extension \u00b6 Example extension contains a sample implementation of a FluentBit log forwarder add-on and covers the following aspects of an extension workflow: Pre-requisite configuration related to nodejs, npm, typescript. Project template with support to build, test and run the extension. Example blueprint (can be found in ./bin/main.ts) that references the add-on. Example of configuring a Kubernetes service account with IRSA (IAM roles for service accounts) and required IAM policies. Example of the helm chart provisioning. Example of passing secret values to the add-on (such as credentials and/or licenseKeys) by leveraging CSI Secret Store Driver. Outlines support to build, package and publish the add-on in an NPM repository.","title":"Extensibility"},{"location":"extensibility/#extensibility","text":"This guide provides an overview of extensibility options focusing on add-on extensions as the primary mechanism for the partners and customers.","title":"Extensibility"},{"location":"extensibility/#overview","text":"SSP Framework is designed to be extensible. In the context of this guide, extensibility refers to the ability of customers and partners to both add new capabilities to the framework or platforms based on SSP as well as customize existing behavior, including the ability to modify or override existing behavior. The following abstractions can be leveraged to add new features to the framework: Add-on . Customers and partners can implement new add-ons which could be leveraged exactly the same way as the core add-ons (supplied by the framework). Resource Provider . This construct allows customers to create resources that can be reused across multiple add-ons and/or teams. For example, IAM roles, VPC, hosted zone. Cluster Provider . This construct allows creation of custom code that provisions an EKS cluster with node groups. It can be leveraged to extend behavior such as control plane customization, custom settings for node groups. Team . This abstraction allows to create team templates for application and platform teams and set custom setting for network isolation, policies (network, security), software wiring (auto injection of proxies, team specific service mesh configuration) and other extensions pertinent to the teams.","title":"Overview"},{"location":"extensibility/#add-on-extensions","text":"In a general case, implementation of an add-on is a class which implements the ClusterAddOn interface. export declare interface ClusterAddOn { id? : string ; deploy ( clusterInfo : types.ClusterInfo ) : Promise < Construct > | void ; } Note : The add-on implementation can optionally supply the id attribute if the target add-on can be added to a blueprint more than once. Implementation of the add-on is expected to be an exported class that implements the interface and supplies the implementation of the deploy method. In order for the add-on to receive the deployment contextual information about the provisioned cluster, region, resource providers and/or other add-ons, the deploy method takes the ClusterInfo parameter (see types ), which represents a structure defined in the SPI (service provider interface) contracts. The API for the cluster info structure is stable and provides access to the provisioned EKS cluster, scheduled add-ons (that have not been installed yet but are part of the blueprint) or provisioned add-ons and other contexts.","title":"Add-on Extensions"},{"location":"extensibility/#post-deployment-hooks","text":"In certain cases, add-on provisioning may require logic to be executed after provisioning of the add-ons (and teams) is complete. For such cases, add-on can optionally implement ClusterPostDeploy interface. /** * Optional interface to allow cluster bootstrapping after provisioning of add-ons and teams is complete. * Can be leveraged to bootstrap workloads, perform cluster checks. * ClusterAddOn implementation may implement this interface in order to get post deployment hook point. */ export declare interface ClusterPostDeploy { postDeploy ( clusterInfo : types.ClusterInfo , teams : Team []) : void ; } This capability is leveraged for example in ArgoCD add-on to bootstrap workloads after all add-ons finished provisioning. Note, in this case unlike the standard deploy method implementation, the add-on also gets access to the provisioned teams.","title":"Post Deployment Hooks"},{"location":"extensibility/#helm-add-ons","text":"Helm add-ons are the most common case that generally combines provisioning of a helm chart as well as supporting infrastructure such as wiring of proper IAM policies for the Kubernetes service account, provisioning or configuring other AWS resources (VPC, subnets, node groups). In order to provide consistency across all Helm add-ons supplied by the SSP framework all Helm add-ons are implemented as derivatives of the HelmAddOn base class and support properties based on HelmAddOnUserProps . See the example extension section below for more details. Use cases that are enabled by leveraging the base HelmAddOn class: Consistency across all helm based add-on will reduce effort to understand how to apply and configure standard add-on options. Ability to override helm chart repository can enable leveraging private helm chart repository by the customer and facilitate add-on usage for private EKS clusters. Extensibility mechanisms available in the SSP framework can allow to intercept helm deployments and leverage GitOps driven add-on configuration.","title":"Helm Add-ons"},{"location":"extensibility/#non-helm-add-ons","text":"Add-ons that don't leverage helm but require to install arbitrary Kubernetes manifests will not be able to leverage the benefits provided by the HelmAddOn however, they are still relatively easy to implement. Deployment of arbitrary kubernetes manifests can leverage the following construct: import { KubernetesManifest } from \"@aws-cdk/aws-eks\" ; import { ClusterAddOn , ClusterInfo } from \"../../spi\" ; import { loadYaml , readYamlDocument } from \"../../utils/yaml-utils\" ; export class MyNonHelmAddOn implements ClusterAddOn { deploy ( clusterInfo : ClusterInfo ) : void { const cluster = clusterInfo . cluster ; // Apply manifest const doc = readYamlDocument ( __dirname + '/my-product.yaml' ); // ... apply any substitutions for dynamic values const manifest = docArray . split ( \"---\" ). map ( e => loadYaml ( e )); new KubernetesManifest ( cluster . stack , \"myproduct-manifest\" , { cluster , manifest , overwrite : true }); } } Note: When leveraging this approach consider how customers can apply the add-on for fully private clusters. It may be reasonable to bundle the manifest with the add-on in the npm package.","title":"Non-helm Add-ons"},{"location":"extensibility/#add-on-dependencies","text":"Add-ons can depend on other add-ons and that dependency may be soft or hard. Hard dependency implies that add-on provisioning must fail if the dependency is not available. For example, if an add-on requires access to AWS Secrets Manager for a secret containing a license key, credentials or other sensitive information, it can declare dependency on the CSI Secret Store Driver. Dependency management for direct hard dependency are implemented using a decorator @dependable . Example: import { Construct } from \"@aws-cdk/core\" ; import { ClusterInfo } from \"../../spi\" ; import { dependable } from \"../../utils\" ; import { HelmAddOn , HelmAddOnUserProps } from \"../helm-addon\" ; export class MyProductAddOn extends HelmAddOn { readonly options : MyProductAddOnProps ; // extends HelmAddOnUserProps ... @dependable ( 'AwsLoadBalancerControllerAddOn' ) // depends on AwsLoadBalancerController deploy ( clusterInfo : ClusterInfo ) : Promise < Construct > { ... }","title":"Add-on Dependencies"},{"location":"extensibility/#passing-secrets-to-add-ons","text":"Secrets from the AWS Secrets Manager or AWS Systems Manager Parameter Store can be made available as files mounted in Amazon EKS pods. It can be achieved with the help of AWS Secrets and Configuration Provider (ASCP) for the Kubernetes Secrets Store CSI Driver . The ASCP works with Amazon Elastic Kubernetes Service (Amazon EKS) 1.17+. More information on general concepts for leveraging ASCP can be found here . SSP Framework provides support for both Secrets Store CSI Driver as well as ASCP with the Secrets Store Add-on . Add-ons requiring support for secrets can declare dependency on the secret store add-on: export class MyAddOn extends ssp . addons . HelmAddOn { ... // Declares dependency on secret store add-on if secrets are needed. // Customers will have to explicitly add this add-on to the blueprint. @ssp . utils . dependable ( ssp . SecretsStoreAddOn . name ) deploy ( clusterInfo : ssp.ClusterInfo ) : Promise < Construct > { ... } In order to propagate the secret from the Secrets Manager to the Kubernetes cluster, the add-on should create a SecretProviderClass Kubernetes object. leveraging the ssp.addons.SecretProviderClass . The framework will take care of wiring the Kubernetes service account with the correct IAM permissions to pull the secret: const sa = clusterInfo . cluster . addServiceAccount (...); const csiSecret : ssp.addons.CsiSecretProps = { secretProvider : new ssp . LookupSecretsManagerSecretByName ( licenseKeySecret ), // the secret must be defined upfront and available in the region with the name specified in the licenseKeySecret kubernetesSecret : { secretName : 'my-addon-license-secret' , data : [ { key : 'licenseKey' } ] } }; const secretProviderClass = new ssp . addons . SecretProviderClass ( clusterInfo , sa , \"my-addon-license-secret-class\" , csiSecret ); Note: you can also leverage LookupSecretsManagerSecretByArn , LookupSsmSecretByAttrs or a custom implementation of the secret provider interface ssp.addons.SecretProvider . After the secret provider class is created, it should be mounted on any pod in the namespace to make the secret accessible. Mounting the secret volume also creates a regular Kubernetes Secret object based on the supplied description ( my-addon-license-secret ). This capability is controlled by the configuration of the SSP Secret Store add-on and is enabled by default. Many Helm charts provide options to mount additional volumes and mounts to the provisioned product. For example, a Helm chart (ArgoCD, FluentBit) allows specifying volumes and volumeMounts as the helm chart values. Mounting the secret in such cases is simple and does not require an additional pod for secrets. Here is an example of a secret volume and volume mount passed as values to a Helm chart: const chart = this . addHelmChart ( clusterInfo , { ... // standard values , volumes : [ { name : \"secrets-store-inline\" , csi : { driver : \"secrets-store.csi.k8s.io\" , readOnly : true , volumeAttributes : { secretProviderClass : \"my-addon-license-secret-class\" } } } ], volumeMounts : [ { name : \"secrets-store-inline\" , mountPath : \"/mnt/secret-store\" } ] }); After the secret volume is mounted (on any pod), you will see that a Kubernetes secret (for example my-addon-license-secret ) is also created in the target namespace. See the supplied code example for more details.","title":"Passing Secrets to Add-ons"},{"location":"extensibility/#private-extensions","text":"Extensions specific to a customer instance of SSPs can be implemented inline with the blueprint in the same codebase. Such extensions are scoped to the customer base and cannot be reused. Example of a private extension: class MyAddOn extends HelmAddOn { constructor () { super ({ chart : 'mychart' , ... }); } deploy ( clusterInfo : ssp.ClusterInfo ) : Promise < Construct > { return Promise . resolve ( this . addHelmChart ( clusterInfo , {})); } } ssp . EksBlueprint . builder () . addOns ( new MyAddOn ()) . build ( app , 'my-extension-test-blueprint' );","title":"Private Extensions"},{"location":"extensibility/#public-extensions","text":"The life-cycle of a public extension should be decoupled from the life-cycle of the SSP Quickstart main repository . When decoupled, extensions can be released at any arbitrary cadence specific to the extension, enabling better agility when it comes to new features or bug fixes. In order to enable this model the following workflow outline steps required to create and release a public extension: Public extensions are created in a separate repository. Public GitHub repository is preferred as it aligns with the open-source spirit of the framework and enables external reviews/feedback. Extensions are released and consumed as distinct public NPM packages. Public Extensions are expected to have sufficient documentation to allow customers to consume them independently. Documentation can reside in GitHub or external resources referenced in the documentation bundled with the extension. Public extensions are expected to be tested and validated against released SSP versions, e.g. with a CICD pipeline. Pipeline can be created with the pipelines support from the SSP framework or leveraging customer/partner specific tools.","title":"Public Extensions"},{"location":"extensibility/#partner-extensions","text":"Partner extensions (APN Partner) are expected to comply with the public extension workflow and additional items required to ensure proper validation and documentation support for a partner extension. Documentation PR should be created to the main SSP Quickstart repository to update the AddOns section. Example of add-on documentation can be found here along with the list of other add-ons. An example that shows a ready to use pattern leveraging the add-on should be submitted to the SSP Patterns Repository . This step will enable AWS PSAs to validate the add-on as well as provide a ready to use pattern to the customers, that could be copied/cloned in their SSP implementation.","title":"Partner Extensions"},{"location":"extensibility/#example-extension","text":"Example extension contains a sample implementation of a FluentBit log forwarder add-on and covers the following aspects of an extension workflow: Pre-requisite configuration related to nodejs, npm, typescript. Project template with support to build, test and run the extension. Example blueprint (can be found in ./bin/main.ts) that references the add-on. Example of configuring a Kubernetes service account with IRSA (IAM roles for service accounts) and required IAM policies. Example of the helm chart provisioning. Example of passing secret values to the add-on (such as credentials and/or licenseKeys) by leveraging CSI Secret Store Driver. Outlines support to build, package and publish the add-on in an NPM repository.","title":"Example Extension"},{"location":"getting-started/","text":"Getting Started \u00b6 This getting started guide will walk you through setting up a new CDK project which leverages the ssp-amazon-eks NPM module to deploy a simple SSP. Project setup \u00b6 To use the ssp-amazon-eks module, you must have the AWS Cloud Development Kit (CDK) installed. Install CDK via the following. npm install -g aws-cdk@1.143.0 Verify the installation. cdk --version Create a new typescript CDK project in an empty directory. cdk init app --language typescript Deploy a Blueprint EKS Cluster \u00b6 Install the ssp-amazon-eks NPM package via the following. npm i @aws-quickstart/ssp-amazon-eks Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new cdk . App (); const addOns : Array < ssp . ClusterAddOn > = [ new ssp . addons . ArgoCDAddOn , new ssp . addons . CalicoAddOn , new ssp . addons . MetricsServerAddOn , new ssp . addons . ClusterAutoScalerAddOn , new ssp . addons . ContainerInsightsAddOn , new ssp . addons . AwsLoadBalancerControllerAddOn (), new ssp . addons . VpcCniAddOn (), new ssp . addons . CoreDnsAddOn (), new ssp . addons . KubeProxyAddOn (), new ssp . addons . XrayAddOn () ]; const account = 'XXXXXXXXXXXXX' ; const region = 'us-east-2' ; const props = { env : { account , region } }; new ssp . EksBlueprint ( app , { id : 'east-test-1' , addOns }, props ); Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is an process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. Bootstrap your environment with the following command. cdk bootstrap Note: if the account/region combination used in the code example above is different from the initial combination used with cdk bootstrap , you will need to perform cdk bootstrap again to avoid error. Please reference CDK usage doc for detail. Deploy the stack using the following command. This command will take roughly 20 minutes to complete. cdk deploy Congratulations! You have deployed your first EKS cluster with ssp-amazon-eks . The above code will provision the following: A new Well-Architected VPC with both Public and Private subnets. A new Well-Architected EKS cluster in the region and account you specify. Nginx into your cluster to serve as a reverse proxy for your workloads. ArgoCD into your cluster to support GitOps deployments. Calico into your cluster to support Network policies. Metrics Server into your cluster to support metrics collection. AWS and Kubernetes resources needed to support Cluster Autoscaler . AWS and Kubernetes resources needed to forward logs and metrics to Container Insights . AWS and Kubernetes resources needed to support AWS Load Balancer Controller . Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS. CoreDNS Amazon EKS add-on (CoreDns) into your cluster. CoreDns is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS kube-proxy Amazon EKS add-on (KubeProxy) into your cluster to maintains network rules on each Amazon EC2 node AWS and Kubernetes resources needed to support AWS X-Ray . Cluster Access \u00b6 Once the deploy completes, you will see output in your terminal window similar to the following: Outputs: east-test-1.easttest1ClusterName8D8E5E5E = east-test-1 east-test-1.easttest1ConfigCommand25ABB520 = aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> east-test-1.easttest1GetTokenCommand337FE3DD = aws eks get-token --cluster-name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Stack ARN: arn:aws:cloudformation:us-east-1:115717706081:stack/east-test-1/e1b9e6a0-d5f6-11eb-8498-0a374cd00e27 To update your Kubernetes config for you new cluster, copy and run the east-test-1.easttest1ConfigCommand25ABB520 command (the second command) in your terminal. aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Validate that you now have kubectl access to your cluster via the following: kubectl get namespace You should see output that lists all namespaces in your cluster. Deploy workloads with ArgoCD \u00b6 Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads across multiple namespaces. The sample app of apps repository that we use in this getting started guide can be found here . You can leverage Automatic Bootstrapping for automatic onboarding of workloads. This feature may be leveraged even when workload repositories are not ready yet, as it creates a placeholder for future workloads and decouples workload onboarding for the infrastructure provisioning pipeline. The next steps, described in this guide apply for cases when customer prefer to bootstrap their workloads manually through ArgoCD UI console. Install ArgoCD CLI \u00b6 These steps are needed for manual workload onboarding. For automatic bootstrapping please refer to the Automatic Bootstrapping . Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd: v2.0.1+33eaf11.dirty Exposing ArgoCD \u00b6 To access ArgoCD running in your Kubernetes cluster, we can leverage Kubernetes Port Forwarding . To do so, first capture the ArgoCD service name in an environment variable. export ARGO_SERVER=$(kubectl get svc -n argocd -l app.kubernetes.io/name=argocd-server -o name) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the ArgoCD login screen. Logging Into ArgoCD \u00b6 ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD You can also login to the ArgoCD UI with generated password and the username admin . echo $ARGO_PASSWORD Deploy workloads to your cluster \u00b6 Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc,argocd \\ -s https://github.com/aws-samples/ssp-eks-workloads.git Create the application within Argo by running the following command argocd app create dev-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/aws-samples/ssp-eks-workloads.git \\ --path \"envs/dev\" Sync the apps by running the following command argocd app sync dev-apps Validate deployments. \u00b6 To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-riker . kubectl port-forward svc/guestbook-ui -n team-riker 4040:80 Open up localhost:4040 in your browser and you should see the application. Next Steps \u00b6 For information on onboarding teams to your clusters, see Team documentation . For information on deploying Continuous Delivery pipelines for your infrastructure, see Pipelines documentation . For information on supported add-ons, see Add-on documentation For information on Onboarding and managing workloads in your clusters, see Workload documentation .","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This getting started guide will walk you through setting up a new CDK project which leverages the ssp-amazon-eks NPM module to deploy a simple SSP.","title":"Getting Started"},{"location":"getting-started/#project-setup","text":"To use the ssp-amazon-eks module, you must have the AWS Cloud Development Kit (CDK) installed. Install CDK via the following. npm install -g aws-cdk@1.143.0 Verify the installation. cdk --version Create a new typescript CDK project in an empty directory. cdk init app --language typescript","title":"Project setup"},{"location":"getting-started/#deploy-a-blueprint-eks-cluster","text":"Install the ssp-amazon-eks NPM package via the following. npm i @aws-quickstart/ssp-amazon-eks Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from '@aws-cdk/core' ; import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new cdk . App (); const addOns : Array < ssp . ClusterAddOn > = [ new ssp . addons . ArgoCDAddOn , new ssp . addons . CalicoAddOn , new ssp . addons . MetricsServerAddOn , new ssp . addons . ClusterAutoScalerAddOn , new ssp . addons . ContainerInsightsAddOn , new ssp . addons . AwsLoadBalancerControllerAddOn (), new ssp . addons . VpcCniAddOn (), new ssp . addons . CoreDnsAddOn (), new ssp . addons . KubeProxyAddOn (), new ssp . addons . XrayAddOn () ]; const account = 'XXXXXXXXXXXXX' ; const region = 'us-east-2' ; const props = { env : { account , region } }; new ssp . EksBlueprint ( app , { id : 'east-test-1' , addOns }, props ); Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is an process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. Bootstrap your environment with the following command. cdk bootstrap Note: if the account/region combination used in the code example above is different from the initial combination used with cdk bootstrap , you will need to perform cdk bootstrap again to avoid error. Please reference CDK usage doc for detail. Deploy the stack using the following command. This command will take roughly 20 minutes to complete. cdk deploy Congratulations! You have deployed your first EKS cluster with ssp-amazon-eks . The above code will provision the following: A new Well-Architected VPC with both Public and Private subnets. A new Well-Architected EKS cluster in the region and account you specify. Nginx into your cluster to serve as a reverse proxy for your workloads. ArgoCD into your cluster to support GitOps deployments. Calico into your cluster to support Network policies. Metrics Server into your cluster to support metrics collection. AWS and Kubernetes resources needed to support Cluster Autoscaler . AWS and Kubernetes resources needed to forward logs and metrics to Container Insights . AWS and Kubernetes resources needed to support AWS Load Balancer Controller . Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS. CoreDNS Amazon EKS add-on (CoreDns) into your cluster. CoreDns is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS kube-proxy Amazon EKS add-on (KubeProxy) into your cluster to maintains network rules on each Amazon EC2 node AWS and Kubernetes resources needed to support AWS X-Ray .","title":"Deploy a Blueprint EKS Cluster"},{"location":"getting-started/#cluster-access","text":"Once the deploy completes, you will see output in your terminal window similar to the following: Outputs: east-test-1.easttest1ClusterName8D8E5E5E = east-test-1 east-test-1.easttest1ConfigCommand25ABB520 = aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> east-test-1.easttest1GetTokenCommand337FE3DD = aws eks get-token --cluster-name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Stack ARN: arn:aws:cloudformation:us-east-1:115717706081:stack/east-test-1/e1b9e6a0-d5f6-11eb-8498-0a374cd00e27 To update your Kubernetes config for you new cluster, copy and run the east-test-1.easttest1ConfigCommand25ABB520 command (the second command) in your terminal. aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Validate that you now have kubectl access to your cluster via the following: kubectl get namespace You should see output that lists all namespaces in your cluster.","title":"Cluster Access"},{"location":"getting-started/#deploy-workloads-with-argocd","text":"Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads across multiple namespaces. The sample app of apps repository that we use in this getting started guide can be found here . You can leverage Automatic Bootstrapping for automatic onboarding of workloads. This feature may be leveraged even when workload repositories are not ready yet, as it creates a placeholder for future workloads and decouples workload onboarding for the infrastructure provisioning pipeline. The next steps, described in this guide apply for cases when customer prefer to bootstrap their workloads manually through ArgoCD UI console.","title":"Deploy workloads with ArgoCD"},{"location":"getting-started/#install-argocd-cli","text":"These steps are needed for manual workload onboarding. For automatic bootstrapping please refer to the Automatic Bootstrapping . Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd: v2.0.1+33eaf11.dirty","title":"Install ArgoCD CLI"},{"location":"getting-started/#exposing-argocd","text":"To access ArgoCD running in your Kubernetes cluster, we can leverage Kubernetes Port Forwarding . To do so, first capture the ArgoCD service name in an environment variable. export ARGO_SERVER=$(kubectl get svc -n argocd -l app.kubernetes.io/name=argocd-server -o name) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the ArgoCD login screen.","title":"Exposing ArgoCD"},{"location":"getting-started/#logging-into-argocd","text":"ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD You can also login to the ArgoCD UI with generated password and the username admin . echo $ARGO_PASSWORD","title":"Logging Into ArgoCD"},{"location":"getting-started/#deploy-workloads-to-your-cluster","text":"Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc,argocd \\ -s https://github.com/aws-samples/ssp-eks-workloads.git Create the application within Argo by running the following command argocd app create dev-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/aws-samples/ssp-eks-workloads.git \\ --path \"envs/dev\" Sync the apps by running the following command argocd app sync dev-apps","title":"Deploy workloads to your cluster"},{"location":"getting-started/#validate-deployments","text":"To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-riker . kubectl port-forward svc/guestbook-ui -n team-riker 4040:80 Open up localhost:4040 in your browser and you should see the application.","title":"Validate deployments."},{"location":"getting-started/#next-steps","text":"For information on onboarding teams to your clusters, see Team documentation . For information on deploying Continuous Delivery pipelines for your infrastructure, see Pipelines documentation . For information on supported add-ons, see Add-on documentation For information on Onboarding and managing workloads in your clusters, see Workload documentation .","title":"Next Steps"},{"location":"pipelines/","text":"Pipelines \u00b6 While it is convenient to leverage the CDK command line tool to deploy your first cluster, we recommend setting up automated pipelines that will be responsible for deploying and updating your EKS infrastructure. To accomplish this, the EKS SSP - Reference Solution leverages the Pipelines CDK module. This module makes it trivial to create Continuous Delivery (CD) pipelines via CodePipeline that are responsible for deploying and updating your infrastructure. Additionally, the EKS SSP - Reference Solution leverages the GitHub integration that the Pipelines CDK module provides in order to integrate our pipelines with GitHub. The end result is that any new configuration pushed to a GitHub repository containing our CDK will be automatically deployed. Defining your blueprint to use with pipeline \u00b6 Creation of a pipeline starts with defining the blueprint that will be deployed across the pipeline stages. The framework allows defining a blueprint builder without instantiating the stack. import * as ssp from '@aws-quickstart/ssp-amazon-eks' import * as team from 'path/to/teams' const blueprint = ssp . EksBlueprint . builder () . account ( account ) // the supplied default will fail, but build and synth will pass . region ( 'us-west-1' ) . addOns ( new ssp . AwsLoadBalancerControllerAddOn , new ssp . ExternalDnsAddOn , new ssp . NginxAddOn , new ssp . CalicoAddOn , new ssp . MetricsServerAddOn , new ssp . ClusterAutoScalerAddOn , new ssp . ContainerInsightsAddOn ) . teams ( new team . TeamRikerSetup ); The difference between the above code and a normal way of instantiating the stack is lack of .build() at the end of the blueprint definition. This code will produce a blueprint builder that can be instantiated inside the pipeline stages. Creating a pipeline \u00b6 We can create a new CodePipeline resource via the following. import * as ssp from '@aws-quickstart/ssp-amazon-eks' const blueprint = ssp . EksBlueprint . builder () ...; // configure your blueprint builder ssp . CodePipelineStack . builder () . name ( \"ssp-eks-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ repoUrl : 'ssp-eks-patterns' , credentialsSecretName : 'github-token' , branch : 'main' }) Note: the above code depends on the AWS secret github-token defined in the target account/region. The secret may be fined in one main region, and replicated to all target regions. Creating stages \u00b6 Once our pipeline is created, we need to define stages for the pipeline. To do so, we can leverage ssp.StackStage convenience class and builder support for it. Let's continue leveraging the pipeline builder defined in the previous step. const blueprint = ssp . EksBlueprint . builder () ...; // configure your blueprint builder ssp . CodePipelineStack . builder () . name ( \"ssp-eks-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ // your repo info }) . stage ({ id : 'us-west-1-managed-ssp-test' , stackBuilder : blueprint.clone ( 'us-west-1' ) // clone the blueprint to customize for the stage. You can add more add-ons, teams here. }) . stage ({ id : 'us-east-2-managed-ssp-prod' , stackBuilder : blueprint.clone ( 'us-east-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. stageProps : { manualApprovals : true } }) Consider adding ArgoCDAddOn with your specific workload bootstrap repository to automatically bootstrap workloads in the provisioned clusters. See Bootstrapping for more details. Adding Waves \u00b6 In many case, when enterprises configure their SDLC environments, such as dev/test/staging/prod, each environment may contain more than a single cluster. It is convenient to provision and maintain (update/upgrade) such clusters in parallel within the limits of the environment. Such environments may be represented as waves of the pipeline. The wave concept is not limited to just a logical environment. It may represent any grouping of clusters that should be executed in parallel. An important advantage of running stages in parallel is the time gain associated with it. Each stage may potentially take tens of minutes (e.g. initial provisioning, upgrade, etc.) and as the number of clusters increase, the overall pipeline run may become very lengthy and won't provide enough agility for the enterprise. Running parallel stages within a wave provides roughly the time performance equivalent to a single stage. Pipeline functionality provides wave support to express waves with blueprints. You can mix individual stages and waves together. An individual stage can be viewed as a wave with a single stage. ssp . CodePipelineStack . builder () . name ( \"ssp-eks-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ //... }) . stage ({ id : 'us-west-1-managed-ssp' , stackBuilder : blueprint.clone ( 'us-west-1' ) }) . wave ( { // adding two clusters for dev env id : \"dev\" , stages : [ { id : \"dev-west-1\" , stackBuilder : blueprint.clone ( 'us-west-1' ). account ( DEV_ACCOUNT )}, // requires trust relationship with the code pipeline role { id : \"dev-east-2\" , stackBuilder : blueprint.clone ( 'us-east-2' ). account ( DEV_ACCOUNT )}, // See https://docs.aws.amazon.com/cdk/api/v1/docs/pipelines-readme.html#cdk-environment-bootstrapping ] }) . wave ( { id : \"prod\" , stages : [ { id : \"prod-west-1\" , stackBuilder : blueprint.clone ( 'us-west-1' )}, { id : \"prod-east-2\" , stackBuilder : blueprint.clone ( 'us-east-2' )}, ] }) Build the pipeline stack \u00b6 Now that we have defined the blueprint builder, the pipeline with repository and stages we just need to invoke the build() step to create the stack. const blueprint = ssp . EksBlueprint . builder () ...; // configure your blueprint builder ssp . CodePipelineStack . builder () . name ( \"ssp-eks-pipeline\" ) . owner ( \"aws-samples\" ) // owner of your repo . repository ({ // your repo info }) . stage ({ id : 'dev' , stackBuilder : blueprint.clone ( 'us-west-1' ) // clone the blueprint to customize for the stage. You can add more add-ons, teams here. }) . stage ({ id : 'test' , stackBuilder : blueprint.clone ( 'us-east-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. }) . stage ({ id : 'prod' , stackBuilder : blueprint.clone ( 'us-west-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. }) . build ( scope , \"ssp-pipeline-stack\" , props ); // will produce the self-mutating pipeline in the target region and start provisioning the defined blueprints. Deploying Pipelines \u00b6 In order to deploy pipelines, each environment (account and region) where pipeline will either be running and each environment to which it will be deploying should be bootstrapped based on CodePipeline documentation . Examples of bootstrapping (from the original documentation): To bootstrap an environment for provisioning the pipeline: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-1 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws://111111111111/us-east-1 To bootstrap a different environment for deploying CDK applications into using a pipeline in account 111111111111: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-2 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ --trust 11111111111 \\ aws://222222222222/us-east-2 If you only want to trust an account to do lookups (e.g, when your CDK application has a Vpc.fromLookup() call), use the option --trust-for-lookup: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-2 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ --trust-for-lookup 11111111111 \\ aws://222222222222/us-east-2 Troubleshooting \u00b6 SSP Build can fail with AccessDenied exception during build phase. Typical error messages: Error: AccessDeniedException: User: arn:aws:sts::<account>:assumed-role/ssp-pipeline-stack-sspekspipelinePipelineBuildSynt-1NPFJRH6H7TB1/AWSCodeBuild-e95830ee-07f6-46f5-aaee-90e269c7eb5f is not authorized to perform: current credentials could not be used to assume 'arn:aws:iam::<account>:role/cdk-hnb659fds-lookup-role-,account>-eu-west-3', but are for the right account. Proceeding anyway. Error: you are not authorized to perform this operation. This can happen for a few reasons, but most typical is related to the stack requiring elevated permissions at build time. Such permissions may be required to perform lookups, such as look up fo VPC, Hosted Zone, Certificate (if imported) and those are handled during stack synthesis. Resolution To address this issue, you can locate the role leveraged for Code Build and provide required permissions. Depending on the scope of the build role, the easiest resolution is to add AdministratorAccess permission to the build role which typically looks similar to this ssp-pipeline-stack-sspekspipelinePipelineBuildSynt-1NPFJRH6H7TB1 provided your pipeline stack was named ssp-pipeline-stack . If adding administrative access to the role solves the issue, you can the consider tightening the role scope to just the required permissions, such as access to specific resources needed for the build.","title":"Pipelines"},{"location":"pipelines/#pipelines","text":"While it is convenient to leverage the CDK command line tool to deploy your first cluster, we recommend setting up automated pipelines that will be responsible for deploying and updating your EKS infrastructure. To accomplish this, the EKS SSP - Reference Solution leverages the Pipelines CDK module. This module makes it trivial to create Continuous Delivery (CD) pipelines via CodePipeline that are responsible for deploying and updating your infrastructure. Additionally, the EKS SSP - Reference Solution leverages the GitHub integration that the Pipelines CDK module provides in order to integrate our pipelines with GitHub. The end result is that any new configuration pushed to a GitHub repository containing our CDK will be automatically deployed.","title":"Pipelines"},{"location":"pipelines/#defining-your-blueprint-to-use-with-pipeline","text":"Creation of a pipeline starts with defining the blueprint that will be deployed across the pipeline stages. The framework allows defining a blueprint builder without instantiating the stack. import * as ssp from '@aws-quickstart/ssp-amazon-eks' import * as team from 'path/to/teams' const blueprint = ssp . EksBlueprint . builder () . account ( account ) // the supplied default will fail, but build and synth will pass . region ( 'us-west-1' ) . addOns ( new ssp . AwsLoadBalancerControllerAddOn , new ssp . ExternalDnsAddOn , new ssp . NginxAddOn , new ssp . CalicoAddOn , new ssp . MetricsServerAddOn , new ssp . ClusterAutoScalerAddOn , new ssp . ContainerInsightsAddOn ) . teams ( new team . TeamRikerSetup ); The difference between the above code and a normal way of instantiating the stack is lack of .build() at the end of the blueprint definition. This code will produce a blueprint builder that can be instantiated inside the pipeline stages.","title":"Defining your blueprint to use with pipeline"},{"location":"pipelines/#creating-a-pipeline","text":"We can create a new CodePipeline resource via the following. import * as ssp from '@aws-quickstart/ssp-amazon-eks' const blueprint = ssp . EksBlueprint . builder () ...; // configure your blueprint builder ssp . CodePipelineStack . builder () . name ( \"ssp-eks-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ repoUrl : 'ssp-eks-patterns' , credentialsSecretName : 'github-token' , branch : 'main' }) Note: the above code depends on the AWS secret github-token defined in the target account/region. The secret may be fined in one main region, and replicated to all target regions.","title":"Creating a pipeline"},{"location":"pipelines/#creating-stages","text":"Once our pipeline is created, we need to define stages for the pipeline. To do so, we can leverage ssp.StackStage convenience class and builder support for it. Let's continue leveraging the pipeline builder defined in the previous step. const blueprint = ssp . EksBlueprint . builder () ...; // configure your blueprint builder ssp . CodePipelineStack . builder () . name ( \"ssp-eks-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ // your repo info }) . stage ({ id : 'us-west-1-managed-ssp-test' , stackBuilder : blueprint.clone ( 'us-west-1' ) // clone the blueprint to customize for the stage. You can add more add-ons, teams here. }) . stage ({ id : 'us-east-2-managed-ssp-prod' , stackBuilder : blueprint.clone ( 'us-east-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. stageProps : { manualApprovals : true } }) Consider adding ArgoCDAddOn with your specific workload bootstrap repository to automatically bootstrap workloads in the provisioned clusters. See Bootstrapping for more details.","title":"Creating stages"},{"location":"pipelines/#adding-waves","text":"In many case, when enterprises configure their SDLC environments, such as dev/test/staging/prod, each environment may contain more than a single cluster. It is convenient to provision and maintain (update/upgrade) such clusters in parallel within the limits of the environment. Such environments may be represented as waves of the pipeline. The wave concept is not limited to just a logical environment. It may represent any grouping of clusters that should be executed in parallel. An important advantage of running stages in parallel is the time gain associated with it. Each stage may potentially take tens of minutes (e.g. initial provisioning, upgrade, etc.) and as the number of clusters increase, the overall pipeline run may become very lengthy and won't provide enough agility for the enterprise. Running parallel stages within a wave provides roughly the time performance equivalent to a single stage. Pipeline functionality provides wave support to express waves with blueprints. You can mix individual stages and waves together. An individual stage can be viewed as a wave with a single stage. ssp . CodePipelineStack . builder () . name ( \"ssp-eks-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ //... }) . stage ({ id : 'us-west-1-managed-ssp' , stackBuilder : blueprint.clone ( 'us-west-1' ) }) . wave ( { // adding two clusters for dev env id : \"dev\" , stages : [ { id : \"dev-west-1\" , stackBuilder : blueprint.clone ( 'us-west-1' ). account ( DEV_ACCOUNT )}, // requires trust relationship with the code pipeline role { id : \"dev-east-2\" , stackBuilder : blueprint.clone ( 'us-east-2' ). account ( DEV_ACCOUNT )}, // See https://docs.aws.amazon.com/cdk/api/v1/docs/pipelines-readme.html#cdk-environment-bootstrapping ] }) . wave ( { id : \"prod\" , stages : [ { id : \"prod-west-1\" , stackBuilder : blueprint.clone ( 'us-west-1' )}, { id : \"prod-east-2\" , stackBuilder : blueprint.clone ( 'us-east-2' )}, ] })","title":"Adding Waves"},{"location":"pipelines/#build-the-pipeline-stack","text":"Now that we have defined the blueprint builder, the pipeline with repository and stages we just need to invoke the build() step to create the stack. const blueprint = ssp . EksBlueprint . builder () ...; // configure your blueprint builder ssp . CodePipelineStack . builder () . name ( \"ssp-eks-pipeline\" ) . owner ( \"aws-samples\" ) // owner of your repo . repository ({ // your repo info }) . stage ({ id : 'dev' , stackBuilder : blueprint.clone ( 'us-west-1' ) // clone the blueprint to customize for the stage. You can add more add-ons, teams here. }) . stage ({ id : 'test' , stackBuilder : blueprint.clone ( 'us-east-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. }) . stage ({ id : 'prod' , stackBuilder : blueprint.clone ( 'us-west-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. }) . build ( scope , \"ssp-pipeline-stack\" , props ); // will produce the self-mutating pipeline in the target region and start provisioning the defined blueprints.","title":"Build the pipeline stack"},{"location":"pipelines/#deploying-pipelines","text":"In order to deploy pipelines, each environment (account and region) where pipeline will either be running and each environment to which it will be deploying should be bootstrapped based on CodePipeline documentation . Examples of bootstrapping (from the original documentation): To bootstrap an environment for provisioning the pipeline: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-1 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws://111111111111/us-east-1 To bootstrap a different environment for deploying CDK applications into using a pipeline in account 111111111111: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-2 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ --trust 11111111111 \\ aws://222222222222/us-east-2 If you only want to trust an account to do lookups (e.g, when your CDK application has a Vpc.fromLookup() call), use the option --trust-for-lookup: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-2 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ --trust-for-lookup 11111111111 \\ aws://222222222222/us-east-2","title":"Deploying Pipelines"},{"location":"pipelines/#troubleshooting","text":"SSP Build can fail with AccessDenied exception during build phase. Typical error messages: Error: AccessDeniedException: User: arn:aws:sts::<account>:assumed-role/ssp-pipeline-stack-sspekspipelinePipelineBuildSynt-1NPFJRH6H7TB1/AWSCodeBuild-e95830ee-07f6-46f5-aaee-90e269c7eb5f is not authorized to perform: current credentials could not be used to assume 'arn:aws:iam::<account>:role/cdk-hnb659fds-lookup-role-,account>-eu-west-3', but are for the right account. Proceeding anyway. Error: you are not authorized to perform this operation. This can happen for a few reasons, but most typical is related to the stack requiring elevated permissions at build time. Such permissions may be required to perform lookups, such as look up fo VPC, Hosted Zone, Certificate (if imported) and those are handled during stack synthesis. Resolution To address this issue, you can locate the role leveraged for Code Build and provide required permissions. Depending on the scope of the build role, the easiest resolution is to add AdministratorAccess permission to the build role which typically looks similar to this ssp-pipeline-stack-sspekspipelinePipelineBuildSynt-1NPFJRH6H7TB1 provided your pipeline stack was named ssp-pipeline-stack . If adding administrative access to the role solves the issue, you can the consider tightening the role scope to just the required permissions, such as access to specific resources needed for the build.","title":"Troubleshooting"},{"location":"teams/","text":"Teams \u00b6 The ssp-amazon-eks framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team . ApplicationTeam \u00b6 To create an ApplicationTeam for your cluster, simply implement a class that extends ApplicationTeam . You will need to supply a team name, an array of users, and (optionally) a directory where you may optionally place any policy definitions and generic manifests for the team. These manifests will be applied by the platform and will be outside of the team control NOTE: When the manifests are applied, namespaces are not checked. Therefore, you are responsible for namespace settings in the yaml files. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] teamManifestDir : './examples/teams/team-awesome/' }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace. (Optionally) read all additional manifests (e.g., network policies, OPA policies, others) stored in a provided directory, and applies them. PlatformTeam \u00b6 To create an PlatformTeam for your cluster, simply implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRoleArn : ` ${ YOUR_ROLE_ARN } ` ; }) DefaultTeamRoles \u00b6 The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services Team Benefits \u00b6 By managing teams via infrastructure as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access. Cluster Access ( kubectl ) \u00b6 The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${teamname}teamrole arn:aws:iam::${account}:role/west-dev-${teamname}AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam::${account}:role/west-dev-${platform-team-name}AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam::${account}:role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam::${account}:role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example. Console Access \u00b6 Provided that each team has received the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team. Examples \u00b6 There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Teams"},{"location":"teams/#teams","text":"The ssp-amazon-eks framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team .","title":"Teams"},{"location":"teams/#applicationteam","text":"To create an ApplicationTeam for your cluster, simply implement a class that extends ApplicationTeam . You will need to supply a team name, an array of users, and (optionally) a directory where you may optionally place any policy definitions and generic manifests for the team. These manifests will be applied by the platform and will be outside of the team control NOTE: When the manifests are applied, namespaces are not checked. Therefore, you are responsible for namespace settings in the yaml files. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] teamManifestDir : './examples/teams/team-awesome/' }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace. (Optionally) read all additional manifests (e.g., network policies, OPA policies, others) stored in a provided directory, and applies them.","title":"ApplicationTeam"},{"location":"teams/#platformteam","text":"To create an PlatformTeam for your cluster, simply implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRoleArn : ` ${ YOUR_ROLE_ARN } ` ; })","title":"PlatformTeam"},{"location":"teams/#defaultteamroles","text":"The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services","title":"DefaultTeamRoles"},{"location":"teams/#team-benefits","text":"By managing teams via infrastructure as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access.","title":"Team Benefits"},{"location":"teams/#cluster-access-kubectl","text":"The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${teamname}teamrole arn:aws:iam::${account}:role/west-dev-${teamname}AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam::${account}:role/west-dev-${platform-team-name}AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam::${account}:role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam::${account}:role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example.","title":"Cluster Access (kubectl)"},{"location":"teams/#console-access","text":"Provided that each team has received the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team.","title":"Console Access"},{"location":"teams/#examples","text":"There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Examples"},{"location":"addons/","text":"Add-ons \u00b6 The ssp-amazon-eks framework leverages a modular approach to managing Add-ons that run within the context of a Kubernetes cluster. Customers are free to select the add-ons that run in each of their blueprint clusters. Within the context of the ssp-amazon-eks framework, an add-on is abstracted as ClusterAddOn interface, and the implementation of the add-on interface can do whatever is necessary to support the desired add-on functionality. This can include applying manifests to a Kubernetes cluster or calling AWS APIs to provision new resources. Supported Add-ons \u00b6 The framework currently supports the following add-ons. Add-on Description AppMeshAddOn Adds an AppMesh controller and CRDs (pending validation on the latest version of CDK) ArgoCDAddOn Provisions Argo CD into your cluster. AWS Load Balancer Controller Provisions the AWS Load Balancer Controller into your cluster. CalicoAddOn Adds the Calico 1.7.1 CNI/Network policy engine ClusterAutoscalerAddOn Adds the standard cluster autoscaler ContainerInsightsAddOn Adds Container Insights support integrating monitoring with CloudWatch CoreDnsAddOn Adds CoreDNS Amazon EKS add-on. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS EbsCsiDriverAddOn Adds EBS CSI Driver Amazon EKS add-on. This driver manages the lifecycle of Amazon EBS volumes for persistent storage ExternalDnsAddOn Adds External DNS support for AWS to the cluster, integrating with Amazon Route 53 Keptn Keptn Control Plane and Execution Plane AddOn KubecostAddOn Adds Kubecost cost analyzer to the EKS cluster KubeviousAddOn Adds Kubevious open source Kubernetes dashboard to an EKS cluster KarpenterAddOn Adds Karpenter support for Amazon EKS. KubeProxyAddOn Adds kube-proxy Amazon EKS add-on. Kube-proxy maintains network rules on each Amazon EC2 node MetricsServerAddOn Adds metrics server (pre-req for HPA and other monitoring tools) NginxAddOn Adds NGINX ingress controller SecretsStoreAddOn Adds AWS Secrets Manager and Config Provider for Secret Store CSI Driver to the EKS Cluster SSMAgentAddOn Adds Amazon SSM Agent to worker nodes VpcCniAddOn Adds the Amazon VPC CNI Amazon EKS addon to support native VPC networking for Amazon EKS Weave GitOps Weave GitOps Core AddOn VeleroAddOn Adds Velero to the EKS Cluster XrayAddOn Adds XRay Daemon to the EKS Cluster Standard Helm Add-On Configuration Options \u00b6 Many add-ons leverage helm to provision and maintain deployments. All provided add-ons that leverage helm allow specifying the following add-on attributes: /** * Name of the helm chart (add-on) */ name? : string , /** * Namespace where helm release will be installed */ namespace? : string , /** * Chart name */ chart? : string , /** * Helm chart version. */ version? : string , /** * Helm release */ release? : string , /** * Helm repository */ repository? : string , /** * Optional values for the helm chart. */ values? : Values Ability to set repository url may be leveraged for private repositories. Version field can be modified from the default chart version, e.g. if the add-on should be upgraded to the desired version, however, since the helm chart version supplied by the customer may not have been tested as part of the SSP release process, SSP community may not be able to reproduce/fix issues related to the helm chart version upgrade.","title":"Overview"},{"location":"addons/#add-ons","text":"The ssp-amazon-eks framework leverages a modular approach to managing Add-ons that run within the context of a Kubernetes cluster. Customers are free to select the add-ons that run in each of their blueprint clusters. Within the context of the ssp-amazon-eks framework, an add-on is abstracted as ClusterAddOn interface, and the implementation of the add-on interface can do whatever is necessary to support the desired add-on functionality. This can include applying manifests to a Kubernetes cluster or calling AWS APIs to provision new resources.","title":"Add-ons"},{"location":"addons/#supported-add-ons","text":"The framework currently supports the following add-ons. Add-on Description AppMeshAddOn Adds an AppMesh controller and CRDs (pending validation on the latest version of CDK) ArgoCDAddOn Provisions Argo CD into your cluster. AWS Load Balancer Controller Provisions the AWS Load Balancer Controller into your cluster. CalicoAddOn Adds the Calico 1.7.1 CNI/Network policy engine ClusterAutoscalerAddOn Adds the standard cluster autoscaler ContainerInsightsAddOn Adds Container Insights support integrating monitoring with CloudWatch CoreDnsAddOn Adds CoreDNS Amazon EKS add-on. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS EbsCsiDriverAddOn Adds EBS CSI Driver Amazon EKS add-on. This driver manages the lifecycle of Amazon EBS volumes for persistent storage ExternalDnsAddOn Adds External DNS support for AWS to the cluster, integrating with Amazon Route 53 Keptn Keptn Control Plane and Execution Plane AddOn KubecostAddOn Adds Kubecost cost analyzer to the EKS cluster KubeviousAddOn Adds Kubevious open source Kubernetes dashboard to an EKS cluster KarpenterAddOn Adds Karpenter support for Amazon EKS. KubeProxyAddOn Adds kube-proxy Amazon EKS add-on. Kube-proxy maintains network rules on each Amazon EC2 node MetricsServerAddOn Adds metrics server (pre-req for HPA and other monitoring tools) NginxAddOn Adds NGINX ingress controller SecretsStoreAddOn Adds AWS Secrets Manager and Config Provider for Secret Store CSI Driver to the EKS Cluster SSMAgentAddOn Adds Amazon SSM Agent to worker nodes VpcCniAddOn Adds the Amazon VPC CNI Amazon EKS addon to support native VPC networking for Amazon EKS Weave GitOps Weave GitOps Core AddOn VeleroAddOn Adds Velero to the EKS Cluster XrayAddOn Adds XRay Daemon to the EKS Cluster","title":"Supported Add-ons"},{"location":"addons/#standard-helm-add-on-configuration-options","text":"Many add-ons leverage helm to provision and maintain deployments. All provided add-ons that leverage helm allow specifying the following add-on attributes: /** * Name of the helm chart (add-on) */ name? : string , /** * Namespace where helm release will be installed */ namespace? : string , /** * Chart name */ chart? : string , /** * Helm chart version. */ version? : string , /** * Helm release */ release? : string , /** * Helm repository */ repository? : string , /** * Optional values for the helm chart. */ values? : Values Ability to set repository url may be leveraged for private repositories. Version field can be modified from the default chart version, e.g. if the add-on should be upgraded to the desired version, however, since the helm chart version supplied by the customer may not have been tested as part of the SSP release process, SSP community may not be able to reproduce/fix issues related to the helm chart version upgrade.","title":"Standard Helm Add-On Configuration Options"},{"location":"addons/app-mesh/","text":"AWS App Mesh Add-on \u00b6 AWS App Mesh is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. The App Mesh add-on provisions the necessary AWS resources and Helm charts into an EKS cluster that are needed to support App Mesh for EKS workloads. Full documentation on using App Mesh with EKS can be found here . Usage \u00b6 import { AppMeshAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new AppMeshAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Functionality \u00b6 Creates an App Mesh IAM service account. Adds both AWSCloudMapFullAccess and AWSAppMeshFullAccess roles to the service account. Adds AWSXRayDaemonWriteAccess to the instance role if XRay integration is enabled. Creates the appmesh-system namespace. Deploys the appmesh-controller Helm chart into the cluster. Supports standard helm configuration options . App Mesh Sidecar Injection \u00b6 You can configure certain namespaces for automatic injection of App Mesh sidecar (Envoy) proxy. This will enable handling cross-cutting aspects such as service to service communication, resiliency patterns (circuit breaker/retries) as well handle ingress and egress for the workloads running in the namespace. Here is an example of a team with a namespace configured for automatic sidecar injection: export class TeamBurnham extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), namespaceAnnotations : { \"appmesh.k8s.aws/sidecarInjectorWebhook\" : \"enabled\" } }); } } Tracing Integration \u00b6 App Mesh integrates with a number of tracing providers for distributed tracing support. At the moment it supports AWS X-Ray, Jaeger, and Datadog providers. The X-Ray integration at present requires either a managed node group or a self-managed auto-scaling group backed by EC2. Fargate is not supported. Enabling integration: const appMeshAddOn = new ssp . AppMeshAddOn ({ enableTracing : true , tracingProvider : \"x-ray\" }), When configured, App Mesh will automatically inject an XRay sidecar to handle tracing which enables troubleshooting latency issues. App Mesh and XRay Integration Example \u00b6 team-burnham sample workload repository is configured with an example workload that demonstrates a \"meshified\" workload. After the workload is deployed with ArgoCD or applied directly to the cluster, a DJ application will be created in the team-burnham namespace, similar to the one used for the EKS Workshop . It was adapted for GitOps integration with SSP and relies on automatic sidecar injection as well as tracing integration with App Mesh. After the workload is deployed you can generate some traffic to populated traces: $ export DJ_POD_NAME = $( kubectl get pods -n team-burnham -l app = dj -o jsonpath = '{.items[].metadata.name}' ) $ kubectl -n team-burnham exec -it ${ DJ_POD_NAME } -c dj bash $ while true ; do curl http://jazz.team-burnham.svc.cluster.local:9080/ echo curl http://metal.team-burnham.svc.cluster.local:9080/ echo done The above script will start producing load which will generate traces with XRay. Once traces are produced (for a minute or more) you can navigate to the AWS XRay console and click on Service Map. You will see a screenshot similar to this:","title":"AWS App Mesh"},{"location":"addons/app-mesh/#aws-app-mesh-add-on","text":"AWS App Mesh is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. The App Mesh add-on provisions the necessary AWS resources and Helm charts into an EKS cluster that are needed to support App Mesh for EKS workloads. Full documentation on using App Mesh with EKS can be found here .","title":"AWS App Mesh Add-on"},{"location":"addons/app-mesh/#usage","text":"import { AppMeshAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new AppMeshAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/app-mesh/#functionality","text":"Creates an App Mesh IAM service account. Adds both AWSCloudMapFullAccess and AWSAppMeshFullAccess roles to the service account. Adds AWSXRayDaemonWriteAccess to the instance role if XRay integration is enabled. Creates the appmesh-system namespace. Deploys the appmesh-controller Helm chart into the cluster. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/app-mesh/#app-mesh-sidecar-injection","text":"You can configure certain namespaces for automatic injection of App Mesh sidecar (Envoy) proxy. This will enable handling cross-cutting aspects such as service to service communication, resiliency patterns (circuit breaker/retries) as well handle ingress and egress for the workloads running in the namespace. Here is an example of a team with a namespace configured for automatic sidecar injection: export class TeamBurnham extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), namespaceAnnotations : { \"appmesh.k8s.aws/sidecarInjectorWebhook\" : \"enabled\" } }); } }","title":"App Mesh Sidecar Injection"},{"location":"addons/app-mesh/#tracing-integration","text":"App Mesh integrates with a number of tracing providers for distributed tracing support. At the moment it supports AWS X-Ray, Jaeger, and Datadog providers. The X-Ray integration at present requires either a managed node group or a self-managed auto-scaling group backed by EC2. Fargate is not supported. Enabling integration: const appMeshAddOn = new ssp . AppMeshAddOn ({ enableTracing : true , tracingProvider : \"x-ray\" }), When configured, App Mesh will automatically inject an XRay sidecar to handle tracing which enables troubleshooting latency issues.","title":"Tracing Integration"},{"location":"addons/app-mesh/#app-mesh-and-xray-integration-example","text":"team-burnham sample workload repository is configured with an example workload that demonstrates a \"meshified\" workload. After the workload is deployed with ArgoCD or applied directly to the cluster, a DJ application will be created in the team-burnham namespace, similar to the one used for the EKS Workshop . It was adapted for GitOps integration with SSP and relies on automatic sidecar injection as well as tracing integration with App Mesh. After the workload is deployed you can generate some traffic to populated traces: $ export DJ_POD_NAME = $( kubectl get pods -n team-burnham -l app = dj -o jsonpath = '{.items[].metadata.name}' ) $ kubectl -n team-burnham exec -it ${ DJ_POD_NAME } -c dj bash $ while true ; do curl http://jazz.team-burnham.svc.cluster.local:9080/ echo curl http://metal.team-burnham.svc.cluster.local:9080/ echo done The above script will start producing load which will generate traces with XRay. Once traces are produced (for a minute or more) you can navigate to the AWS XRay console and click on Service Map. You will see a screenshot similar to this:","title":"App Mesh and XRay Integration Example"},{"location":"addons/argo-cd/","text":"Argo CD Add-on \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and can optionally bootstrap your workloads from public and private Git repositories. The Argo CD add-on allows platform administrators to combine cluster provisioning and workload bootstrapping in a single step and enables use cases such as replicating an existing running production cluster in a different region in a matter of minutes. This is important for business continuity and disaster recovery cases as well as for cross-regional availability and geographical expansion. Please see the documentation below for details on automatic boostrapping with ArgoCD add-on. If you prefer manual bootstrapping (once your cluster is deployed with this add-on included), you can find instructions on getting started with Argo CD in our Getting Started guide. Full Argo CD project documentation can be found here . Usage \u00b6 To provision and maintain ArgoCD components without any bootstrapping, the add-on provides a no-argument constructor to get started. import { ArgoCDAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new ArgoCDAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); The above will create an argocd namespace and install all Argo CD components. In order to bootstrap workloads you will need to change the default ArgoCD admin password and add repositories as specified in the Getting Started documentation. Functionality \u00b6 Creates the namespace specified in the construction parameter ( argocd by default). Deploys the argo-cd Helm chart into the cluster. Allows to specify ApplicationRepository selecting the required authentication method as SSH Key, username/password or username/token. Credentials are expected to be set in AWS Secrets Manager and replicated to the desired region. If bootstrap repository is specified, creates the initial bootstrap application which may be leveraged to bootstrap workloads and/or other add-ons through GitOps. Allows setting the initial admin password through AWS Secrets Manager, replicating to the desired region. Supports standard helm configuration options . Setting an Admin Password \u00b6 By default, the Argo CD add-on will create a new admin password for you. To specify your own, you can leverage the AWS Secrets Manager. const argoCDAddOn = new ArgoCDAddOn ({ adminPasswordSecretName : `your-secret-name` }); const addOns : Array < ClusterAddOn > = [ argoCDAddOn ]; The attribute adminPasswordSecretName is the logical name of the secret in AWS Secret Manager . Note, that when deploying to multiple regions, the secret is expected to be replicated to each region. Inside ArgoCD, the admin password is stored as a bcrypt hash. This step will be performed by the framework and stored in the ArgoCD admin secret . You can change the admin password through the Secrets Manager, but it will require rerunning the provisioning pipeline to apply the change. Bootstrapping \u00b6 The SSP framework provides an approach to bootstrap workloads and/or additional add-ons from a customer GitOps repository. In a general case, the bootstrap GitOps repository may contains an App of Apps that points to all workloads and add-ons. In order to enable bootstrapping, the add-on allows passing an ApplicationRepository at construction time. The following repository types are supported at present: Public HTTP/HTTPS repositories (e.g., GitHub) Private HTTPS accessible git repositories requiring username/password authentication. Private git repositories with SSH access requiring an SSH key for authentication. Private HTTPS accessible GitHub repositories accessible with GitHub token. An example is provided below, along with an approach that could use a separate app of apps to bootstrap workloads in different stages, which is important for a software delivery platform as it allows segregating workloads specific to each stage of the SDLC and defines clear promotion processes through GitOps. import { ArgoCDAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOns = ...; const repoUrl = 'https://github.com/aws-samples/ssp-eks-workloads.git' const bootstrapRepo = { repoUrl , credentialsSecretName : 'github-ssh-test' , credentialsType : 'SSH' } const devBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/dev' } }); const testBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/test' , }, }); const prodBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/prod' , }, adminPasswordSecretName : 'argo-admin-secret' , }); const east1 = 'us-east-1' ; new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ east1 } ` , addOns : addOns.concat ( devBootstrapArgo ), teams }, { env : { region : east1 } }); const east2 = 'us-east-2' ; new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ east2 } ` , addOns : addOns.concat ( testBootstrapArgo ), teams }, { env : { region : east2 } }); const west2 = 'us-west-2' new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ west2 } ` , addOns : addOns.concat ( prodBootstrapArgo ), teams }, { env : { region : west2 } }); The application promotion process in the above example is handled entirely through GitOps. Each stage specific App of Apps contains references to respective application GitOps repository for each stage (e.g referencing the release vs work branches or path-based within individual app GitOps repository). Secrets Support \u00b6 The framework provides support to supply repository and administrator secrets in AWS Secrets Manager. This support is evolving and will be improved over time as ArgoCD itself matures. Private Repositories \u00b6 SSH Key Authentication Set credentialsType to SSH when defining bootstrap repository in the ArgoCD add-on configuration. . addOns ( new ssp . addons . ArgoCDAddOn ({ bootstrapRepo : { repoUrl : 'git@github.com:aws-samples/ssp-eks-workloads.git' , path : 'envs/dev' , credentialsSecretName : 'github-ssh-json' , credentialsType : 'SSH' } })) Note: In this case the configuration assumes that there is a secret github-ssh-json define in the target account and all the regions where the blueprint will be deployed. Define the secret in AWS Secret Manager as \"Plain Text\" that contains a JSON structure (as of ArgoCD 2.x) with the fields sshPrivateKey and url defined. Note, that JSON does not allow line break characters, so all new line characters must be escaped with \\n . Example Structure: { \"sshPrivateKey\" : \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\" , \"url\" : \"git@github\" } Note: You can notice explicit \\n characters in the sshPrivateKey . url attribute is required and must specify full or partial URL for credentials template. For example git@github will set the credentials for all GitHub repositories when SSH authentication is used. For more information see Repository Credentials and SSH Repositories from official ArgoCD documentation. To escape your SSH private key for storing it as a secret you can use the following command on Mac/Linux: awk 'NF {sub(/\\r/, \"\"); printf \"%s\\\\n\",$0;}' <path-to-your-cert> A convenience script to create the JSON structure for SSH private key can be found here . You will need to set the PEM_FILE (full path to the ssh private key file) and URL_TEMPLATE (part of the URL for credentials template) variables inside the script. ( important ) Replicate the secret to all the desired regions. Please see instructions for GitHub oou n details on setting up SSH access. Username Password and Token Authentication Set credentialsType to USERNAME or TOKEN when defining ApplicationRepository in the ArgoCD add-on configuration. Define the secret in the AWS Secret Manager as \"Key Value\" and set fields username and password to the desired values (clear text). For TOKEN username could be set to any username and password field set to the GitHub token. Replicate to the desired regions. Make sure that for this type of authentication your repository URL is set as https , e.g. https://github.com/aws-samples/ssp-eks-workloads.git. Admin Secret Create a secret in the AWS Secrets Manager as \"Plain Text\" and set the value to the desired ArgoCD admin password. Replicate the secret to all the desired regions. Set the secret name in adminPasswordSecretName in ArgoCD add-on configuration. You can change the secret value through AWS Secrets Manager, however, it will require to rerun cdk deploy with the minimal changeset to apply the change. Alternatively to get started, the admin password hash can be set bypassing the AWS Secret by setting the following structure in the values properties of the add-on parameters: import * as bcrypt from \"bcrypt\" ; . addOns ( new ssp . addons . ArgoCDAddOn ({ ... // other settings values : { \"configs\" : { \"secret\" : { \"argocdServerAdminPassword\" : bcrypt . hash ( < your password plain text > , 10 ) // or just supply <your bcrypt hash> directly } } } })) For more information, please refer to the ArgoCD official documentation . Known Issues \u00b6 Destruction of the cluster with provisioned applications may cause cloud formation to get stuck on deleting ArgoCD namespace. This happens because the server component that handles Application CRD resource is destroyed before it has a chance to clean up applications that were provisioned through GitOps (of which CFN is unaware). To address this issue at the moment, App of Apps application should be destroyed manually before destroying the stack. Changing the administrator password in the AWS Secrets Manager and rerunning the stack causes login error on ArgoCD UI. This happens due to the fact that Argo Helm rewrites the secret containing the Dex server API Key (OIDC component of ArgoCD). The workaround at present is to restart the argocd-server pod, which repopulates the token. Secret management aspect of ArgoCD will be improved in the future to not require this step after password change.","title":"ArgoCD"},{"location":"addons/argo-cd/#argo-cd-add-on","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and can optionally bootstrap your workloads from public and private Git repositories. The Argo CD add-on allows platform administrators to combine cluster provisioning and workload bootstrapping in a single step and enables use cases such as replicating an existing running production cluster in a different region in a matter of minutes. This is important for business continuity and disaster recovery cases as well as for cross-regional availability and geographical expansion. Please see the documentation below for details on automatic boostrapping with ArgoCD add-on. If you prefer manual bootstrapping (once your cluster is deployed with this add-on included), you can find instructions on getting started with Argo CD in our Getting Started guide. Full Argo CD project documentation can be found here .","title":"Argo CD Add-on"},{"location":"addons/argo-cd/#usage","text":"To provision and maintain ArgoCD components without any bootstrapping, the add-on provides a no-argument constructor to get started. import { ArgoCDAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new ArgoCDAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); The above will create an argocd namespace and install all Argo CD components. In order to bootstrap workloads you will need to change the default ArgoCD admin password and add repositories as specified in the Getting Started documentation.","title":"Usage"},{"location":"addons/argo-cd/#functionality","text":"Creates the namespace specified in the construction parameter ( argocd by default). Deploys the argo-cd Helm chart into the cluster. Allows to specify ApplicationRepository selecting the required authentication method as SSH Key, username/password or username/token. Credentials are expected to be set in AWS Secrets Manager and replicated to the desired region. If bootstrap repository is specified, creates the initial bootstrap application which may be leveraged to bootstrap workloads and/or other add-ons through GitOps. Allows setting the initial admin password through AWS Secrets Manager, replicating to the desired region. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/argo-cd/#setting-an-admin-password","text":"By default, the Argo CD add-on will create a new admin password for you. To specify your own, you can leverage the AWS Secrets Manager. const argoCDAddOn = new ArgoCDAddOn ({ adminPasswordSecretName : `your-secret-name` }); const addOns : Array < ClusterAddOn > = [ argoCDAddOn ]; The attribute adminPasswordSecretName is the logical name of the secret in AWS Secret Manager . Note, that when deploying to multiple regions, the secret is expected to be replicated to each region. Inside ArgoCD, the admin password is stored as a bcrypt hash. This step will be performed by the framework and stored in the ArgoCD admin secret . You can change the admin password through the Secrets Manager, but it will require rerunning the provisioning pipeline to apply the change.","title":"Setting an Admin Password"},{"location":"addons/argo-cd/#bootstrapping","text":"The SSP framework provides an approach to bootstrap workloads and/or additional add-ons from a customer GitOps repository. In a general case, the bootstrap GitOps repository may contains an App of Apps that points to all workloads and add-ons. In order to enable bootstrapping, the add-on allows passing an ApplicationRepository at construction time. The following repository types are supported at present: Public HTTP/HTTPS repositories (e.g., GitHub) Private HTTPS accessible git repositories requiring username/password authentication. Private git repositories with SSH access requiring an SSH key for authentication. Private HTTPS accessible GitHub repositories accessible with GitHub token. An example is provided below, along with an approach that could use a separate app of apps to bootstrap workloads in different stages, which is important for a software delivery platform as it allows segregating workloads specific to each stage of the SDLC and defines clear promotion processes through GitOps. import { ArgoCDAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOns = ...; const repoUrl = 'https://github.com/aws-samples/ssp-eks-workloads.git' const bootstrapRepo = { repoUrl , credentialsSecretName : 'github-ssh-test' , credentialsType : 'SSH' } const devBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/dev' } }); const testBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/test' , }, }); const prodBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/prod' , }, adminPasswordSecretName : 'argo-admin-secret' , }); const east1 = 'us-east-1' ; new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ east1 } ` , addOns : addOns.concat ( devBootstrapArgo ), teams }, { env : { region : east1 } }); const east2 = 'us-east-2' ; new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ east2 } ` , addOns : addOns.concat ( testBootstrapArgo ), teams }, { env : { region : east2 } }); const west2 = 'us-west-2' new ssp . EksBlueprint ( scope , { id : ` ${ id } - ${ west2 } ` , addOns : addOns.concat ( prodBootstrapArgo ), teams }, { env : { region : west2 } }); The application promotion process in the above example is handled entirely through GitOps. Each stage specific App of Apps contains references to respective application GitOps repository for each stage (e.g referencing the release vs work branches or path-based within individual app GitOps repository).","title":"Bootstrapping"},{"location":"addons/argo-cd/#secrets-support","text":"The framework provides support to supply repository and administrator secrets in AWS Secrets Manager. This support is evolving and will be improved over time as ArgoCD itself matures.","title":"Secrets Support"},{"location":"addons/argo-cd/#private-repositories","text":"SSH Key Authentication Set credentialsType to SSH when defining bootstrap repository in the ArgoCD add-on configuration. . addOns ( new ssp . addons . ArgoCDAddOn ({ bootstrapRepo : { repoUrl : 'git@github.com:aws-samples/ssp-eks-workloads.git' , path : 'envs/dev' , credentialsSecretName : 'github-ssh-json' , credentialsType : 'SSH' } })) Note: In this case the configuration assumes that there is a secret github-ssh-json define in the target account and all the regions where the blueprint will be deployed. Define the secret in AWS Secret Manager as \"Plain Text\" that contains a JSON structure (as of ArgoCD 2.x) with the fields sshPrivateKey and url defined. Note, that JSON does not allow line break characters, so all new line characters must be escaped with \\n . Example Structure: { \"sshPrivateKey\" : \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\" , \"url\" : \"git@github\" } Note: You can notice explicit \\n characters in the sshPrivateKey . url attribute is required and must specify full or partial URL for credentials template. For example git@github will set the credentials for all GitHub repositories when SSH authentication is used. For more information see Repository Credentials and SSH Repositories from official ArgoCD documentation. To escape your SSH private key for storing it as a secret you can use the following command on Mac/Linux: awk 'NF {sub(/\\r/, \"\"); printf \"%s\\\\n\",$0;}' <path-to-your-cert> A convenience script to create the JSON structure for SSH private key can be found here . You will need to set the PEM_FILE (full path to the ssh private key file) and URL_TEMPLATE (part of the URL for credentials template) variables inside the script. ( important ) Replicate the secret to all the desired regions. Please see instructions for GitHub oou n details on setting up SSH access. Username Password and Token Authentication Set credentialsType to USERNAME or TOKEN when defining ApplicationRepository in the ArgoCD add-on configuration. Define the secret in the AWS Secret Manager as \"Key Value\" and set fields username and password to the desired values (clear text). For TOKEN username could be set to any username and password field set to the GitHub token. Replicate to the desired regions. Make sure that for this type of authentication your repository URL is set as https , e.g. https://github.com/aws-samples/ssp-eks-workloads.git. Admin Secret Create a secret in the AWS Secrets Manager as \"Plain Text\" and set the value to the desired ArgoCD admin password. Replicate the secret to all the desired regions. Set the secret name in adminPasswordSecretName in ArgoCD add-on configuration. You can change the secret value through AWS Secrets Manager, however, it will require to rerun cdk deploy with the minimal changeset to apply the change. Alternatively to get started, the admin password hash can be set bypassing the AWS Secret by setting the following structure in the values properties of the add-on parameters: import * as bcrypt from \"bcrypt\" ; . addOns ( new ssp . addons . ArgoCDAddOn ({ ... // other settings values : { \"configs\" : { \"secret\" : { \"argocdServerAdminPassword\" : bcrypt . hash ( < your password plain text > , 10 ) // or just supply <your bcrypt hash> directly } } } })) For more information, please refer to the ArgoCD official documentation .","title":"Private Repositories"},{"location":"addons/argo-cd/#known-issues","text":"Destruction of the cluster with provisioned applications may cause cloud formation to get stuck on deleting ArgoCD namespace. This happens because the server component that handles Application CRD resource is destroyed before it has a chance to clean up applications that were provisioned through GitOps (of which CFN is unaware). To address this issue at the moment, App of Apps application should be destroyed manually before destroying the stack. Changing the administrator password in the AWS Secrets Manager and rerunning the stack causes login error on ArgoCD UI. This happens due to the fact that Argo Helm rewrites the secret containing the Dex server API Key (OIDC component of ArgoCD). The workaround at present is to restart the argocd-server pod, which repopulates the token. Secret management aspect of ArgoCD will be improved in the future to not require this step after password change.","title":"Known Issues"},{"location":"addons/aws-for-fluent-bit/","text":"AWS for Fluent Bit \u00b6 Fluent Bit is an open source log processor and forwarder which allows you to collect data like metrics and logs from different sources, enrich them with filters and send them to multiple destinations. AWS provides a Fluent Bit image with plugins for both CloudWatch Logs and Kinesis Data Firehose. The AWS for Fluent Bit image is available on the Amazon ECR Public Gallery. For more details, see AWS for Fluent Bit GitHub repository . Usage \u00b6 import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const awsForFluentBit = new ssp . addons . AwsForFluentBitAddOn (); const addOns : Array < ClusterAddOn > = [ awsForFluentBit ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Configuration \u00b6 AWS for FluentBit can be configured to forward logs to multiple AWS destinations including CloudWatch, Kinesis, and Elasticsearch (now AWS OpenSearch Search). Sample configuration can be found below: import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const awsForFluentBit = new ssp . addons . AwsForFluentBitAddOn ({ values : { cloudWatch : { enabled : true , region : \"<aws_region\" , logGroupName : \"<log_groups_name>\" }, kinesis : { enabled : true , region : \"<aws_region\" , deliveryStream : \"<delivery_stream>\" }, elasticSearch : { enabled : true , region : \"<aws_region\" , host : \"<elastic_search_host>\" } } }); IAM Policies \u00b6 When leveraging AWS for FluentBit to forward logs to various AWS destinations, you will need to supply an IAM role that grants privileges to the namespace in which FluentBit runs. For example, in order to forward logs to Amazon Elasticsearch Service, you would supply the following IAMPolicyStatement. import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const domain = es . Domain () const domainWritePolicy = new iam . PolicyStatement ({ actions : [ 'es:ESHttpDelete' , 'es:ESHttpPost' , 'es:ESHttpPut' , 'es:ESHttpPatch' ], resources : [ domain . arn ], }) const awsForFluentBit = new ssp . addons . AwsForFluentBitAddOn ({ iamPolicies : [ domainWritePolicy ] values : { elasticSearch : { enabled : true , region : \"<aws_region\" , host : \"<elastic_search_host>\" } } });","title":"AWS for Fluent Bit"},{"location":"addons/aws-for-fluent-bit/#aws-for-fluent-bit","text":"Fluent Bit is an open source log processor and forwarder which allows you to collect data like metrics and logs from different sources, enrich them with filters and send them to multiple destinations. AWS provides a Fluent Bit image with plugins for both CloudWatch Logs and Kinesis Data Firehose. The AWS for Fluent Bit image is available on the Amazon ECR Public Gallery. For more details, see AWS for Fluent Bit GitHub repository .","title":"AWS for Fluent Bit"},{"location":"addons/aws-for-fluent-bit/#usage","text":"import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const awsForFluentBit = new ssp . addons . AwsForFluentBitAddOn (); const addOns : Array < ClusterAddOn > = [ awsForFluentBit ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/aws-for-fluent-bit/#configuration","text":"AWS for FluentBit can be configured to forward logs to multiple AWS destinations including CloudWatch, Kinesis, and Elasticsearch (now AWS OpenSearch Search). Sample configuration can be found below: import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const awsForFluentBit = new ssp . addons . AwsForFluentBitAddOn ({ values : { cloudWatch : { enabled : true , region : \"<aws_region\" , logGroupName : \"<log_groups_name>\" }, kinesis : { enabled : true , region : \"<aws_region\" , deliveryStream : \"<delivery_stream>\" }, elasticSearch : { enabled : true , region : \"<aws_region\" , host : \"<elastic_search_host>\" } } });","title":"Configuration"},{"location":"addons/aws-for-fluent-bit/#iam-policies","text":"When leveraging AWS for FluentBit to forward logs to various AWS destinations, you will need to supply an IAM role that grants privileges to the namespace in which FluentBit runs. For example, in order to forward logs to Amazon Elasticsearch Service, you would supply the following IAMPolicyStatement. import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const domain = es . Domain () const domainWritePolicy = new iam . PolicyStatement ({ actions : [ 'es:ESHttpDelete' , 'es:ESHttpPost' , 'es:ESHttpPut' , 'es:ESHttpPatch' ], resources : [ domain . arn ], }) const awsForFluentBit = new ssp . addons . AwsForFluentBitAddOn ({ iamPolicies : [ domainWritePolicy ] values : { elasticSearch : { enabled : true , region : \"<aws_region\" , host : \"<elastic_search_host>\" } } });","title":"IAM Policies"},{"location":"addons/aws-load-balancer-controller/","text":"AWS Load Balancer Controller Add-on \u00b6 The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. In the past, you used the Kubernetes in-tree load balancer for instance targets, but used the AWS Load balancer Controller for IP targets. With the AWS Load Balancer Controller version 2.2.0 or later, you can create Network Load Balancers using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. For more information about AWS Load Balancer Controller please see the official documentation . This controller is a required for proper configuration of other ingress controllers such as NGINX. Usage \u00b6 import { AwsLoadBalancerControllerAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new AwsLoadBalancerControllerAddon (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > }, }); To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2 /2 2 2 3m58s Functionality \u00b6 Adds proper IAM permissions and creates a Kubernetes service account with IRSA integration. Allows configuration options such as enabling WAF and Shield. Allows to replace the helm chart version if a specific version of the controller is needed. Creates an IngressClass associated with the AWS Load Balance Controller when the createIngressClassResource prop is set to true Supports standard helm configuration options . Note : An ingressClass must be created in the cluster, either using the createIngressClassResource prop or externally, to be able to create Ingresses associated with the AWS ALB. Creating a Load Balanced Service \u00b6 Once the AWS Load Balancer Controller add-on is installed in your cluster, it is able to provision both Network Load Balancers and Application Load Balancers on your behalf. For example, when the following manifest is applied to your cluster, it will create an NLB. apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb name : udp-test1 spec : type : LoadBalancer ports : - port : 5005 protocol : UDP targetPort : 5005 selector : name : your-app","title":"AWS Load Balancer Controller"},{"location":"addons/aws-load-balancer-controller/#aws-load-balancer-controller-add-on","text":"The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. In the past, you used the Kubernetes in-tree load balancer for instance targets, but used the AWS Load balancer Controller for IP targets. With the AWS Load Balancer Controller version 2.2.0 or later, you can create Network Load Balancers using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. For more information about AWS Load Balancer Controller please see the official documentation . This controller is a required for proper configuration of other ingress controllers such as NGINX.","title":"AWS Load Balancer Controller Add-on"},{"location":"addons/aws-load-balancer-controller/#usage","text":"import { AwsLoadBalancerControllerAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new AwsLoadBalancerControllerAddon (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > }, }); To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2 /2 2 2 3m58s","title":"Usage"},{"location":"addons/aws-load-balancer-controller/#functionality","text":"Adds proper IAM permissions and creates a Kubernetes service account with IRSA integration. Allows configuration options such as enabling WAF and Shield. Allows to replace the helm chart version if a specific version of the controller is needed. Creates an IngressClass associated with the AWS Load Balance Controller when the createIngressClassResource prop is set to true Supports standard helm configuration options . Note : An ingressClass must be created in the cluster, either using the createIngressClassResource prop or externally, to be able to create Ingresses associated with the AWS ALB.","title":"Functionality"},{"location":"addons/aws-load-balancer-controller/#creating-a-load-balanced-service","text":"Once the AWS Load Balancer Controller add-on is installed in your cluster, it is able to provision both Network Load Balancers and Application Load Balancers on your behalf. For example, when the following manifest is applied to your cluster, it will create an NLB. apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb name : udp-test1 spec : type : LoadBalancer ports : - port : 5005 protocol : UDP targetPort : 5005 selector : name : your-app","title":"Creating a Load Balanced Service"},{"location":"addons/aws-node-termination-handler/","text":"AWS Node Termination Handler \u00b6 The AWS Node Termination Handler (NTH) project ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as EC2 maintenance events, EC2 Spot interruptions, ASG Scale-In, ASG AZ Rebalance, and EC2 Instance Termination via the API or Console. If not handled, your application code may not stop gracefully, take longer to recover full availability, or accidentally schedule work to nodes that are going down. For more information see [README.md][https://github.com/aws/aws-node-termination-handler#readme]. NTH can operate in two different modes: Instance Metadata Service (IMDS) or the Queue Processor. To choose the operating mode refer to this table . Best Practice NTH should only be used when you are using self-managed node groups and self-managed node groups with Spot instances. For more information on why you do not need NTH on managed node groups see this issue and EKS Workshop for detailed explanation. Best Practice Use NTH in Queue Processor option to add every AWS Node Termination Handler feature to the self-managed node group. Note With AWS Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to use AWS Node Termination Handler. Usage \u00b6 import * as cdk from '@aws-cdk/core' ; // SSP Lib import * as ssp from '@aws-quickstart/ssp-amazon-eks' import * as eks from '@aws-cdk/aws-eks' ; export default class BottlerocketConstruct extends cdk . Construct { constructor ( scope : cdk.Construct , id : string ) { super ( scope , id ); // AddOns for the cluster. const addOns : Array < ssp . ClusterAddOn > = [ new ssp . AwsNodeTerminationHandlerAddOn , ]; const stackID = ` ${ id } -blueprint` ; const clusterProvider = new ssp . AsgClusterProvider ({ version : eks.KubernetesVersion.V1_20 , machineImageType : eks.MachineImageType.BOTTLEROCKET }); new ssp . EksBlueprint ( scope , { id : stackID , addOns , clusterProvider }, { env : { region : 'us-east-1' } } ); } } To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming handler is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-node-termination-handler 1 /1 1 1 23m Functionality \u00b6 IMDS Mode (default) \u00b6 Node group ASG tagged with key=aws-node-termination-handler/managed Deploy the AWS Node Termination Handler helm chart Queue Mode \u00b6 Node group ASG tagged with key=aws-node-termination-handler/managed AutoScaling Group Termination Lifecycle Hook Amazon Simple Queue Service (SQS) Queue Amazon EventBridge Rule IAM Role for the aws-node-termination-handler Queue Processing Pods Deploy the AWS Node Termination Handler helm chart","title":"AWS Node Termination Handler"},{"location":"addons/aws-node-termination-handler/#aws-node-termination-handler","text":"The AWS Node Termination Handler (NTH) project ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as EC2 maintenance events, EC2 Spot interruptions, ASG Scale-In, ASG AZ Rebalance, and EC2 Instance Termination via the API or Console. If not handled, your application code may not stop gracefully, take longer to recover full availability, or accidentally schedule work to nodes that are going down. For more information see [README.md][https://github.com/aws/aws-node-termination-handler#readme]. NTH can operate in two different modes: Instance Metadata Service (IMDS) or the Queue Processor. To choose the operating mode refer to this table . Best Practice NTH should only be used when you are using self-managed node groups and self-managed node groups with Spot instances. For more information on why you do not need NTH on managed node groups see this issue and EKS Workshop for detailed explanation. Best Practice Use NTH in Queue Processor option to add every AWS Node Termination Handler feature to the self-managed node group. Note With AWS Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to use AWS Node Termination Handler.","title":"AWS Node Termination Handler"},{"location":"addons/aws-node-termination-handler/#usage","text":"import * as cdk from '@aws-cdk/core' ; // SSP Lib import * as ssp from '@aws-quickstart/ssp-amazon-eks' import * as eks from '@aws-cdk/aws-eks' ; export default class BottlerocketConstruct extends cdk . Construct { constructor ( scope : cdk.Construct , id : string ) { super ( scope , id ); // AddOns for the cluster. const addOns : Array < ssp . ClusterAddOn > = [ new ssp . AwsNodeTerminationHandlerAddOn , ]; const stackID = ` ${ id } -blueprint` ; const clusterProvider = new ssp . AsgClusterProvider ({ version : eks.KubernetesVersion.V1_20 , machineImageType : eks.MachineImageType.BOTTLEROCKET }); new ssp . EksBlueprint ( scope , { id : stackID , addOns , clusterProvider }, { env : { region : 'us-east-1' } } ); } } To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming handler is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-node-termination-handler 1 /1 1 1 23m","title":"Usage"},{"location":"addons/aws-node-termination-handler/#functionality","text":"","title":"Functionality"},{"location":"addons/aws-node-termination-handler/#imds-mode-default","text":"Node group ASG tagged with key=aws-node-termination-handler/managed Deploy the AWS Node Termination Handler helm chart","title":"IMDS Mode (default)"},{"location":"addons/aws-node-termination-handler/#queue-mode","text":"Node group ASG tagged with key=aws-node-termination-handler/managed AutoScaling Group Termination Lifecycle Hook Amazon Simple Queue Service (SQS) Queue Amazon EventBridge Rule IAM Role for the aws-node-termination-handler Queue Processing Pods Deploy the AWS Node Termination Handler helm chart","title":"Queue Mode"},{"location":"addons/calico/","text":"Calico Add-on \u00b6 Project Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. To secure workloads in Kubernetes, Calico utilizes Network Policies. The Calico add-on adds support for Calico to an EKS cluster. By default, the native VPC-CNI plugin for Kubernetes on EKS does not support Kubernetes Network Policies. Installing Calico (or alternate CNI provider) will enable customers to define and apply standard Kubernetes Network Policies to their EKS cluster. Calico add-on supports standard helm configuration options . Usage \u00b6 import { CalicoAddon , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new CalicoAddon (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Applying Network Policies \u00b6 In the Getting Started guide, we bootstrapped an EKS cluster with the workloads contained in the ssp-workloads repository. Below, we will demonstrate how we can apply network policies to govern traffic between the workloads once Calico is installed. To start, we can verify that there are no network policies in place in your EKS cluster. kubectl get networkpolicy -A This means that all resources within the cluster should be able to make ingress and egress connections with other resources within and outside the cluster. You can verify, for example, that you are able to ping a team-burnham pod from a team-riker pod. To do so, first retrieve the podIP from the team-burnham namespace. BURNHAM_POD = $( kubectl get pod -n team-burnham -o jsonpath = '{.items[0].metadata.name}' ) BURNHAM_POD_IP = $( kubectl get pod -n team-burnham $BURNHAM_POD -o jsonpath = '{.status.podIP}' ) Now you can start a shell from the pod in the team-riker namespace and ping the pod from team-burnham namespace: RIKER_POD = $( kubectl -n team-riker get pod -o jsonpath = '{.items[0].metadata.name}' ) kubectl exec -ti -n team-riker $RIKER_POD -- sh Note: since this opens a shell inside the pod, it will not have the environment variables saved above. You should retrieve the actual podIP from the environment variable BURNHAM_POD_IP . With those actual values, curl the IP and port 80 of the pod from team-burnham : # curl -s <Team Burnham Pod IP>:80>/dev/null && echo Success. || echo Fail. You should see Success. Applying Kubernetes Network Policy to block traffic \u00b6 Let's apply the following Network Policy: kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : default-deny spec : podSelector : matchLabels : {} Save it as deny-all.yaml . Run the following commands to apply the policy to both team-riker and team-burnham namespaces: kubectl -n team-riker apply -f deny-all.yaml kubectl -n team-burnham apply -f deny-all.yaml This will prevent access to all resources within both namespaces. Try curl commands from above to verify that it fails. Applying additional policy to re-open pod to pod communications \u00b6 You can apply a new Kubernetes Network Policy on top of the previous to \u201cpoke holes\u201d for egress and ingress needs. For example, if you want to be able to curl from the team-riker pod to the team-burnham pod, the following Kubernetes NetworkPolicy should be applied. kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : namespace : team-burnham name : allow-riker-to-burnham spec : podSelector : matchLabels : app : guestbook-ui policyTypes : - Ingress - Egress ingress : - from : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 egress : - to : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 Save as allow-burnham-riker.yaml and apply the new NetworkPolicy: kubectl apply -f allow-burnham-riker.yaml Once the policy is applied, once again try the curl command from above. You should now see Success. once again. Securing your environment with Kubernetes Network Policies \u00b6 Calico also allows Custom Resource Definitions (CRD) which provides the ability to add features not in the standard Kubernetes Network Policies, such as: Explicit Deny rules Layer 7 rule support (i.e. Http Request types) Endpoint support other than standard pods: OpenShift, VMs, interfaces, etc. In order to use CRDs (in particular defined within the projectcalico.org/v3 Calico API), you must install the Calico CLI ( calicoctl ). You can find more information about Calico Network Policy and using calicoctl here .","title":"Calico"},{"location":"addons/calico/#calico-add-on","text":"Project Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. To secure workloads in Kubernetes, Calico utilizes Network Policies. The Calico add-on adds support for Calico to an EKS cluster. By default, the native VPC-CNI plugin for Kubernetes on EKS does not support Kubernetes Network Policies. Installing Calico (or alternate CNI provider) will enable customers to define and apply standard Kubernetes Network Policies to their EKS cluster. Calico add-on supports standard helm configuration options .","title":"Calico Add-on"},{"location":"addons/calico/#usage","text":"import { CalicoAddon , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new CalicoAddon (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/calico/#applying-network-policies","text":"In the Getting Started guide, we bootstrapped an EKS cluster with the workloads contained in the ssp-workloads repository. Below, we will demonstrate how we can apply network policies to govern traffic between the workloads once Calico is installed. To start, we can verify that there are no network policies in place in your EKS cluster. kubectl get networkpolicy -A This means that all resources within the cluster should be able to make ingress and egress connections with other resources within and outside the cluster. You can verify, for example, that you are able to ping a team-burnham pod from a team-riker pod. To do so, first retrieve the podIP from the team-burnham namespace. BURNHAM_POD = $( kubectl get pod -n team-burnham -o jsonpath = '{.items[0].metadata.name}' ) BURNHAM_POD_IP = $( kubectl get pod -n team-burnham $BURNHAM_POD -o jsonpath = '{.status.podIP}' ) Now you can start a shell from the pod in the team-riker namespace and ping the pod from team-burnham namespace: RIKER_POD = $( kubectl -n team-riker get pod -o jsonpath = '{.items[0].metadata.name}' ) kubectl exec -ti -n team-riker $RIKER_POD -- sh Note: since this opens a shell inside the pod, it will not have the environment variables saved above. You should retrieve the actual podIP from the environment variable BURNHAM_POD_IP . With those actual values, curl the IP and port 80 of the pod from team-burnham : # curl -s <Team Burnham Pod IP>:80>/dev/null && echo Success. || echo Fail. You should see Success.","title":"Applying Network Policies"},{"location":"addons/calico/#applying-kubernetes-network-policy-to-block-traffic","text":"Let's apply the following Network Policy: kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : default-deny spec : podSelector : matchLabels : {} Save it as deny-all.yaml . Run the following commands to apply the policy to both team-riker and team-burnham namespaces: kubectl -n team-riker apply -f deny-all.yaml kubectl -n team-burnham apply -f deny-all.yaml This will prevent access to all resources within both namespaces. Try curl commands from above to verify that it fails.","title":"Applying Kubernetes Network Policy to block traffic"},{"location":"addons/calico/#applying-additional-policy-to-re-open-pod-to-pod-communications","text":"You can apply a new Kubernetes Network Policy on top of the previous to \u201cpoke holes\u201d for egress and ingress needs. For example, if you want to be able to curl from the team-riker pod to the team-burnham pod, the following Kubernetes NetworkPolicy should be applied. kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : namespace : team-burnham name : allow-riker-to-burnham spec : podSelector : matchLabels : app : guestbook-ui policyTypes : - Ingress - Egress ingress : - from : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 egress : - to : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 Save as allow-burnham-riker.yaml and apply the new NetworkPolicy: kubectl apply -f allow-burnham-riker.yaml Once the policy is applied, once again try the curl command from above. You should now see Success. once again.","title":"Applying additional policy to re-open pod to pod communications"},{"location":"addons/calico/#securing-your-environment-with-kubernetes-network-policies","text":"Calico also allows Custom Resource Definitions (CRD) which provides the ability to add features not in the standard Kubernetes Network Policies, such as: Explicit Deny rules Layer 7 rule support (i.e. Http Request types) Endpoint support other than standard pods: OpenShift, VMs, interfaces, etc. In order to use CRDs (in particular defined within the projectcalico.org/v3 Calico API), you must install the Calico CLI ( calicoctl ). You can find more information about Calico Network Policy and using calicoctl here .","title":"Securing your environment with Kubernetes Network Policies"},{"location":"addons/cluster-autoscaler/","text":"Cluster Autoscaler Add-on \u00b6 The Cluster Autoscaler add-on adds support for Cluster Autoscaler to an EKS cluster. Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when: pods fail due to insufficient resources, or pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time. Usage \u00b6 import { ClusterAutoScalerAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new ClusterAutoscalerAddOn () const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Functionality \u00b6 Adds proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) to the NodeGroup IAM role. Configures service account, cluster roles, roles, role bindings and deployment. Resolves proper CA image to pull based on the Kubernetes version. Configuration allows passing a specific version of the image to pull. Applies proper tags for discoverability to the EC2 instances. Supports standard helm configuration options . Testing the scaling functionality \u00b6 The following steps will help test and validate Cluster Autoscaler functionality in your cluster. Deploy a sample app as a deployment. Create a Horizontal Pod Autoscaler (HPA) resource. Generate load to trigger scaling. Deploy a sample app \u00b6 Take a note of the number of nodes available: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-189-107.us-west-2.compute.internal Ready <none> 80m v1.19.6-eks-49a6c0 The first step is to create a sample application via deployment and request 20m of CPU: kubectl create deployment php-apache --image = us.gcr.io/k8s-artifacts-prod/hpa-example kubectl set resources deploy php-apache --requests = cpu = 20m kubectl expose php-apache --port 80 You can see that there's 1 pod currently running: kubectl get pod -l app = php-apache NAME READY STATUS RESTARTS AGE php-apache-55c4584468-vsbl7 1/1 Running 0 63s Create HPA resource \u00b6 Now we can create Horizontal Pod Autoscaler resource with 50% CPU target utilization, and the minimum number of pods at 1 and max at 20: kubectl autoscale deployment php-apache \\ --cpu-percent = 50 \\ --min = 1 \\ --max = 20 You can verify by looking at the hpa resource: kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 10%/50% 1 20 2 52s Generate load \u00b6 With the resources created, you can generate load on the apache server with a busybox container: kubectl --generator = run-pod/v1 run -i --tty load-generator --image = busybox /bin/sh You can generate the actual load on the shell by running a while loop: while true ; do wget -q -O - http://php-apache ; done Verify that Cluster Autoscaler works \u00b6 While the load is being generated, access another terminal to verify that HPA is working. The following command should return a list of many nods created (as many as 10): kubectl get pods -l app = php-apache -o wide --watch With more pods being created, you would expect more nodes to be created; you can access the Cluster Autoscaler logs to confirm: kubectl -n kube-system logs -f deployment/cluster-autoscaler Lastly, you can list all the nodes and see that there are now multiple nodes: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-187-70.us-west-2.compute.internal Ready <none> 73s v1.19.6-eks-49a6c0 ip-10-0-189-107.us-west-2.compute.internal Ready <none> 84m v1.19.6-eks-49a6c0 ip-10-0-224-226.us-west-2.compute.internal Ready <none> 46s v1.19.6-eks-49a6c0 ip-10-0-233-105.us-west-2.compute.internal Ready <none> 90s v1.19.6-eks-49a6c0","title":"Cluster Autoscaler"},{"location":"addons/cluster-autoscaler/#cluster-autoscaler-add-on","text":"The Cluster Autoscaler add-on adds support for Cluster Autoscaler to an EKS cluster. Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when: pods fail due to insufficient resources, or pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time.","title":"Cluster Autoscaler Add-on"},{"location":"addons/cluster-autoscaler/#usage","text":"import { ClusterAutoScalerAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new ClusterAutoscalerAddOn () const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/cluster-autoscaler/#functionality","text":"Adds proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) to the NodeGroup IAM role. Configures service account, cluster roles, roles, role bindings and deployment. Resolves proper CA image to pull based on the Kubernetes version. Configuration allows passing a specific version of the image to pull. Applies proper tags for discoverability to the EC2 instances. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/cluster-autoscaler/#testing-the-scaling-functionality","text":"The following steps will help test and validate Cluster Autoscaler functionality in your cluster. Deploy a sample app as a deployment. Create a Horizontal Pod Autoscaler (HPA) resource. Generate load to trigger scaling.","title":"Testing the scaling functionality"},{"location":"addons/cluster-autoscaler/#deploy-a-sample-app","text":"Take a note of the number of nodes available: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-189-107.us-west-2.compute.internal Ready <none> 80m v1.19.6-eks-49a6c0 The first step is to create a sample application via deployment and request 20m of CPU: kubectl create deployment php-apache --image = us.gcr.io/k8s-artifacts-prod/hpa-example kubectl set resources deploy php-apache --requests = cpu = 20m kubectl expose php-apache --port 80 You can see that there's 1 pod currently running: kubectl get pod -l app = php-apache NAME READY STATUS RESTARTS AGE php-apache-55c4584468-vsbl7 1/1 Running 0 63s","title":"Deploy a sample app"},{"location":"addons/cluster-autoscaler/#create-hpa-resource","text":"Now we can create Horizontal Pod Autoscaler resource with 50% CPU target utilization, and the minimum number of pods at 1 and max at 20: kubectl autoscale deployment php-apache \\ --cpu-percent = 50 \\ --min = 1 \\ --max = 20 You can verify by looking at the hpa resource: kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 10%/50% 1 20 2 52s","title":"Create HPA resource"},{"location":"addons/cluster-autoscaler/#generate-load","text":"With the resources created, you can generate load on the apache server with a busybox container: kubectl --generator = run-pod/v1 run -i --tty load-generator --image = busybox /bin/sh You can generate the actual load on the shell by running a while loop: while true ; do wget -q -O - http://php-apache ; done","title":"Generate load"},{"location":"addons/cluster-autoscaler/#verify-that-cluster-autoscaler-works","text":"While the load is being generated, access another terminal to verify that HPA is working. The following command should return a list of many nods created (as many as 10): kubectl get pods -l app = php-apache -o wide --watch With more pods being created, you would expect more nodes to be created; you can access the Cluster Autoscaler logs to confirm: kubectl -n kube-system logs -f deployment/cluster-autoscaler Lastly, you can list all the nodes and see that there are now multiple nodes: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-187-70.us-west-2.compute.internal Ready <none> 73s v1.19.6-eks-49a6c0 ip-10-0-189-107.us-west-2.compute.internal Ready <none> 84m v1.19.6-eks-49a6c0 ip-10-0-224-226.us-west-2.compute.internal Ready <none> 46s v1.19.6-eks-49a6c0 ip-10-0-233-105.us-west-2.compute.internal Ready <none> 90s v1.19.6-eks-49a6c0","title":"Verify that Cluster Autoscaler works"},{"location":"addons/container-insights/","text":"Container Insights Add-on \u00b6 The Container Insights add-on adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using an embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. IMPORTANT CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing , see Amazon CloudWatch Pricing. Usage \u00b6 Add the following as an add-on to your main.ts file to add Containers Insights to your cluster import { ContainerInsightsAddOn , , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new ContainerInsightsAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Prerequisites \u00b6 Once the Container Insights add-on has been installed in your cluster, validate that the CloudWatch Agent and the FluentD daemons are running. ` kubectl get all -n amazon-cloudwatch ` You should see output similar to the following: NAME READY STATUS RESTARTS AGE pod/cloudwatch-agent-k8wxl 1/1 Running 0 105s pod/fluentd-cloudwatch-78zv4 1/1 Running 0 105s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/cloudwatch-agent 1 1 1 1 1 <none> 107s daemonset.apps/fluentd-cloudwatch 1 1 1 1 1 <none> 106s To enable or disable control plane logs with the console, run the following command in your terminal. aws eks update-cluster-config \\ --region us-east-2 \\ --name east-dev \\ --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' You should see a similar output as the following. { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"InProgress\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } You can also monitor the status of your log configuration update to your cluster by running the following command. aws eks describe-update \\ --region <region-code> \\ --name <prod> \\ --update-id <883405c8-65c6-4758-8cee-2a7c1340a6d9> Once the update is complete, you should see a similar output. { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"Successful\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } View metrics for cluster and workloads \u00b6 Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance. View cluster level logs \u00b6 After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console. To view these logs on the CloudWatch console follow these steps: Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks/ /cluster. Choose the log stream to view. The following list describes the log stream name format for each log type. Kubernetes API server component logs (api) \u2013 kube-apiserver- Audit (audit) \u2013 kube-apiserver-audit- Authenticator (authenticator) \u2013 authenticator- Controller manager (controllerManager) \u2013 kube-controller-manager- Scheduler (scheduler) \u2013 kube-scheduler- Next in the console, click on Log groups under Logs. You will see under log streams all the log streams from your Amazon EKS control plane. View workload level logs \u00b6 In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console In the navigation pane, choose Insights. Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events. In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at /aws/containerinsights/east-dev/performance When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table. In the query editor, replace the default query with the following query and choose Run query. STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName SORT avg_node_cpu_utilization DESC This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like. To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page. STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName SORT avg_number_of_container_restarts DESC This query displays a list of your pods, sorted by average number of container restarts as shown below If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax. View containers via that container map in container insights. \u00b6 In order to view a map of all of your containers running inside your cluster, click on View your container map in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.","title":"Container Insights"},{"location":"addons/container-insights/#container-insights-add-on","text":"The Container Insights add-on adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using an embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. IMPORTANT CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing , see Amazon CloudWatch Pricing.","title":"Container Insights Add-on"},{"location":"addons/container-insights/#usage","text":"Add the following as an add-on to your main.ts file to add Containers Insights to your cluster import { ContainerInsightsAddOn , , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new ContainerInsightsAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/container-insights/#prerequisites","text":"Once the Container Insights add-on has been installed in your cluster, validate that the CloudWatch Agent and the FluentD daemons are running. ` kubectl get all -n amazon-cloudwatch ` You should see output similar to the following: NAME READY STATUS RESTARTS AGE pod/cloudwatch-agent-k8wxl 1/1 Running 0 105s pod/fluentd-cloudwatch-78zv4 1/1 Running 0 105s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/cloudwatch-agent 1 1 1 1 1 <none> 107s daemonset.apps/fluentd-cloudwatch 1 1 1 1 1 <none> 106s To enable or disable control plane logs with the console, run the following command in your terminal. aws eks update-cluster-config \\ --region us-east-2 \\ --name east-dev \\ --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' You should see a similar output as the following. { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"InProgress\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } You can also monitor the status of your log configuration update to your cluster by running the following command. aws eks describe-update \\ --region <region-code> \\ --name <prod> \\ --update-id <883405c8-65c6-4758-8cee-2a7c1340a6d9> Once the update is complete, you should see a similar output. { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"Successful\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } }","title":"Prerequisites"},{"location":"addons/container-insights/#view-metrics-for-cluster-and-workloads","text":"Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance.","title":"View metrics for cluster and workloads"},{"location":"addons/container-insights/#view-cluster-level-logs","text":"After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console. To view these logs on the CloudWatch console follow these steps: Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks/ /cluster. Choose the log stream to view. The following list describes the log stream name format for each log type. Kubernetes API server component logs (api) \u2013 kube-apiserver- Audit (audit) \u2013 kube-apiserver-audit- Authenticator (authenticator) \u2013 authenticator- Controller manager (controllerManager) \u2013 kube-controller-manager- Scheduler (scheduler) \u2013 kube-scheduler- Next in the console, click on Log groups under Logs. You will see under log streams all the log streams from your Amazon EKS control plane.","title":"View cluster level logs"},{"location":"addons/container-insights/#view-workload-level-logs","text":"In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console In the navigation pane, choose Insights. Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events. In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at /aws/containerinsights/east-dev/performance When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table. In the query editor, replace the default query with the following query and choose Run query. STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName SORT avg_node_cpu_utilization DESC This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like. To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page. STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName SORT avg_number_of_container_restarts DESC This query displays a list of your pods, sorted by average number of container restarts as shown below If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax.","title":"View workload level logs"},{"location":"addons/container-insights/#view-containers-via-that-container-map-in-container-insights","text":"In order to view a map of all of your containers running inside your cluster, click on View your container map in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.","title":"View containers via that container map in container insights."},{"location":"addons/coredns/","text":"CoreDNS Amazon EKS Add-on \u00b6 The CoreDNS Amazon EKS Add-on adds support for CoreDNS . CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. The CoreDNS Pods provide name resolution for all Pods in the cluster. For more information about CoreDNS, see Using CoreDNS for Service Discovery in the Kubernetes documentation. Installing CoreDNS as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update CoreDNS. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs CoreDNS as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons. Prerequisite \u00b6 Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later. Usage \u00b6 import * as ssp from '@shapirov/cdk-eks-blueprint' ; readonly coreDNS = new ssp . addons . CoreDnsAddOn ( \"v1.8.0-eksbuild.1\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ coreDNS ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Configuration Options \u00b6 version : Pass in the core-dns plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.19, below command shows versions of the CoreDNS add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name coredns \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text # Output v1.8.3-eksbuild.1 False v1.8.0-eksbuild.1 True v1.7.0-eksbuild.1 False Validation \u00b6 To validate that coredns add-on is running, ensure that both the coredns pods are in Running state. $ kubectl get pods -n kube-system | grep coredns NAME READY STATUS RESTARTS AGE coredns-644944ff4-2hjkj 1 /1 Running 0 34d coredns-644944ff4-fz6p5 1 /1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name coredns \\ --query \"addon.addonVersion\" \\ --output text # Output v1.8.0-eksbuild.1 Functionality \u00b6 Applies CoreDNS add-on to an Amazon EKS cluster.","title":"CoreDns"},{"location":"addons/coredns/#coredns-amazon-eks-add-on","text":"The CoreDNS Amazon EKS Add-on adds support for CoreDNS . CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. The CoreDNS Pods provide name resolution for all Pods in the cluster. For more information about CoreDNS, see Using CoreDNS for Service Discovery in the Kubernetes documentation. Installing CoreDNS as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update CoreDNS. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs CoreDNS as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons.","title":"CoreDNS Amazon EKS Add-on"},{"location":"addons/coredns/#prerequisite","text":"Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.","title":"Prerequisite"},{"location":"addons/coredns/#usage","text":"import * as ssp from '@shapirov/cdk-eks-blueprint' ; readonly coreDNS = new ssp . addons . CoreDnsAddOn ( \"v1.8.0-eksbuild.1\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ coreDNS ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/coredns/#configuration-options","text":"version : Pass in the core-dns plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.19, below command shows versions of the CoreDNS add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name coredns \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text # Output v1.8.3-eksbuild.1 False v1.8.0-eksbuild.1 True v1.7.0-eksbuild.1 False","title":"Configuration Options"},{"location":"addons/coredns/#validation","text":"To validate that coredns add-on is running, ensure that both the coredns pods are in Running state. $ kubectl get pods -n kube-system | grep coredns NAME READY STATUS RESTARTS AGE coredns-644944ff4-2hjkj 1 /1 Running 0 34d coredns-644944ff4-fz6p5 1 /1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name coredns \\ --query \"addon.addonVersion\" \\ --output text # Output v1.8.0-eksbuild.1","title":"Validation"},{"location":"addons/coredns/#functionality","text":"Applies CoreDNS add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/ebs-csi-driver/","text":"EBS CSI Driver Amazon EKS Add-on \u00b6 The EBS CSI Driver Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. This driver is not automatically installed when you first create a cluster, it must be added to the cluster in order to manage EBS volumes. For more information on the driver, please review the user guide . Prerequisites \u00b6 Amazon EKS EBS CSI Driver add-on is only available on Amazon EKS clusters running Kubernetes version 1.20 and later. Note that the version of the driver that can be used on an EKS cluster depends on the version of Kubernetes running in the cluster. See the configuration options section below for more details Usage \u00b6 import { App } from '@aws-cdk/core' ; import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new App (); ssp . EksBlueprint . builder () . addOns ( new ssp . EbsCsiDriverAddOn () ) . build ( app , 'my-cluster' ); Configuration Options \u00b6 version : Version of the EBS CSI Driver add-on to be installed. The version must be compatible with kubernetes cluster version. # Command to show versions of the EBS CSI Driver add-on available for cluster version is 1.20 aws eks describe-addon-versions \\ --addon-name aws-ebs-csi-driver \\ --kubernetes-version 1 .20 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text # Output v1.4.0-eksbuild.preview Validation \u00b6 To validate that EBS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster kubectl get pods -n kube-system | grep ebs # Output ebs-csi-controller-95848f4d9-hlrzs 4 /4 Running 0 5m8s ebs-csi-controller-95848f4d9-m4f54 4 /4 Running 0 4m38s ebs-csi-node-c9xdf 3 /3 Running 0 5m8s Additionally, the aws cli can be used to determine which version of the add-on is installed in the cluster # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name aws-ebs-csi-driver \\ --query \"addon.addonVersion\" \\ --output text # Output v1.4.0-eksbuild.preview Functionality \u00b6 Applies the EBS CSI Driver add-on to an Amazon EKS cluster.","title":"EBS CSI Driver Amazon EKS Add-on"},{"location":"addons/ebs-csi-driver/#ebs-csi-driver-amazon-eks-add-on","text":"The EBS CSI Driver Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. This driver is not automatically installed when you first create a cluster, it must be added to the cluster in order to manage EBS volumes. For more information on the driver, please review the user guide .","title":"EBS CSI Driver Amazon EKS Add-on"},{"location":"addons/ebs-csi-driver/#prerequisites","text":"Amazon EKS EBS CSI Driver add-on is only available on Amazon EKS clusters running Kubernetes version 1.20 and later. Note that the version of the driver that can be used on an EKS cluster depends on the version of Kubernetes running in the cluster. See the configuration options section below for more details","title":"Prerequisites"},{"location":"addons/ebs-csi-driver/#usage","text":"import { App } from '@aws-cdk/core' ; import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new App (); ssp . EksBlueprint . builder () . addOns ( new ssp . EbsCsiDriverAddOn () ) . build ( app , 'my-cluster' );","title":"Usage"},{"location":"addons/ebs-csi-driver/#configuration-options","text":"version : Version of the EBS CSI Driver add-on to be installed. The version must be compatible with kubernetes cluster version. # Command to show versions of the EBS CSI Driver add-on available for cluster version is 1.20 aws eks describe-addon-versions \\ --addon-name aws-ebs-csi-driver \\ --kubernetes-version 1 .20 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text # Output v1.4.0-eksbuild.preview","title":"Configuration Options"},{"location":"addons/ebs-csi-driver/#validation","text":"To validate that EBS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster kubectl get pods -n kube-system | grep ebs # Output ebs-csi-controller-95848f4d9-hlrzs 4 /4 Running 0 5m8s ebs-csi-controller-95848f4d9-m4f54 4 /4 Running 0 4m38s ebs-csi-node-c9xdf 3 /3 Running 0 5m8s Additionally, the aws cli can be used to determine which version of the add-on is installed in the cluster # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name aws-ebs-csi-driver \\ --query \"addon.addonVersion\" \\ --output text # Output v1.4.0-eksbuild.preview","title":"Validation"},{"location":"addons/ebs-csi-driver/#functionality","text":"Applies the EBS CSI Driver add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/external-dns/","text":"External DNS Add-on \u00b6 External DNS add-on is based on the ExternalDNS open source project and allows integration of exposed Kubernetes services and Ingresses with DNS providers, in particular Amazon Route 53 . The add-on provides functionality to configure IAM policies and Kubernetes service accounts for Route 53 integration support based on AWS Tutorial for External DNS . Usage \u00b6 import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const hostedZoneName = ... const hostedZone = new ssp . addons . LookupHostedZoneProvider ( hostedZoneName ) const addOn = new ssp . addons . ExternalDnsAddon ({ hostedZone }); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); To validate that external DNS add-on is running ensure that the add-on deployment is in RUNNING state: # Assuming add-on is installed in the external-dns namespace. $ kubectl get po -n external-dns NAME READY STATUS RESTARTS AGE external-dns-fcf6c9c66-xd8f4 1 /1 Running 0 3d3h Using External DNS \u00b6 You can now provision external services and ingresses integrating with Route 53. For example to provision an NLB you can use the following service manifest: apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb external-dns.alpha.kubernetes.io/hostname : <MyDomainName> name : udp-test1 spec : type : LoadBalancer ports : - port : 80 protocol : TCP targetPort : 80 selector : name : your-app Note the external-dns.alpha.kubernetes.io/hostname annotation for the service name that allows specifying its domain name. Hosted Zone Providers \u00b6 In order for external DNS to work, you need to supply one or more hosted zones. Hosted zones are expected to be supplied leveraging resource providers . To help customers handle common use cases for Route 53 provisioning the framework provides a few convenience providers that can be registered with the EKS Blueprint Stack. Name look-up and direct import provider: This provider will allow to bind to an existing hosted zone based on its name. const myDomainName = \"\" ; ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ssp . LookupHostedZoneProvider ( myDomainName )) . addOns ( new ssp . addons . ExternalDnsAddon ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . build (...); If the hosted zone ID is known, then the recommended approach is to use a ImportHostedZoneProvider which bypasses any lookup calls. const myHostedZoneId = \"\" ; ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ssp . addons . ImportHostedZoneProvider ( myHostedZoneId )) . addOns ( new ssp . addons . ExternalDnsAddon ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . build (...); Delegating Hosted Zone Provider In many cases, enterprises choose to decouple provisioning of root domains and subdomains. A common pattern is to have a specific DNS account (AWS account) hosting the root domain, while subdomains may be created within individual workload accounts. This implies cross-account access from child account to the parent DNS account for subdomain delegation. Prerequisites: Parent account defines an IAM role with the following managed policies: AmazonRoute53DomainsFullAccess AmazonRoute53ReadOnlyAccess AmazonRoute53AutoNamingFullAccess Create trust relationship between this role and the child account that is expected to add subdomains. For info see IAM tutorial . Example: Let's assume that parent DNS account parentAccountId has domain named myglobal-domain.com . Now when provisioned an SSP EKS cluster you would like to use a stage specific name like dev.myglobal-domain.com or test.myglobal-domain.com . In addition to these requirements, you would like to enable tenant specific access to those domains such as my-tenant1.dev.myglobal-domain.com or team specific access team-riker.dev.myglobal-domain.com . The setup will look the following way: In the parentAccountId account you create a role for delegation ( DomainOperatorRole in this example) and a trust relationship to the child account in which SSP EKS blueprint will be provisioned. In a general case, the number of child accounts can be large, so each of them will have to be listed in the trust relationship. In the parentAccountId you create a public hosted zone for myglobal-domain.com . With that the setup that may require separate automation (or a manual process) is complete. Use the following configuration of the add-on: ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ssp . DelegatingHostedZoneProvider ({ parentDomain : 'myglobal-domain.com' , subdomain : 'dev.myglobal-domain.com' , parentAccountId : parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true }) . addOns ( new ssp . addons . ExternalDnsAddon ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) The parameter wildcardSubdomain above when set to true will also create a CNAME for *.dev.myglobal-domain.com . This is at the moment used for host based routing within EKS (e.g. with NGINX ingress). Configuration Options \u00b6 namespace : Optional target namespace where add-on will be installed. Changing this value on the operating cluster is not recommended. Set to external-dns by default. version : The add-on is leveraging a Bitnami helm chart. This parameter allows overriding the helm chart version used. hostedZone : Hosted zone provider is a interface that provides one or more hosted zones that the add-on will leverage for the service and ingress configuration. Functionality \u00b6 Applies External-DNS configuration for AWS DNS provider. See AWS Tutorial for External DNS for more information. Supports standard helm configuration options .","title":"External DNS"},{"location":"addons/external-dns/#external-dns-add-on","text":"External DNS add-on is based on the ExternalDNS open source project and allows integration of exposed Kubernetes services and Ingresses with DNS providers, in particular Amazon Route 53 . The add-on provides functionality to configure IAM policies and Kubernetes service accounts for Route 53 integration support based on AWS Tutorial for External DNS .","title":"External DNS Add-on"},{"location":"addons/external-dns/#usage","text":"import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const hostedZoneName = ... const hostedZone = new ssp . addons . LookupHostedZoneProvider ( hostedZoneName ) const addOn = new ssp . addons . ExternalDnsAddon ({ hostedZone }); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); To validate that external DNS add-on is running ensure that the add-on deployment is in RUNNING state: # Assuming add-on is installed in the external-dns namespace. $ kubectl get po -n external-dns NAME READY STATUS RESTARTS AGE external-dns-fcf6c9c66-xd8f4 1 /1 Running 0 3d3h","title":"Usage"},{"location":"addons/external-dns/#using-external-dns","text":"You can now provision external services and ingresses integrating with Route 53. For example to provision an NLB you can use the following service manifest: apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb external-dns.alpha.kubernetes.io/hostname : <MyDomainName> name : udp-test1 spec : type : LoadBalancer ports : - port : 80 protocol : TCP targetPort : 80 selector : name : your-app Note the external-dns.alpha.kubernetes.io/hostname annotation for the service name that allows specifying its domain name.","title":"Using External DNS"},{"location":"addons/external-dns/#hosted-zone-providers","text":"In order for external DNS to work, you need to supply one or more hosted zones. Hosted zones are expected to be supplied leveraging resource providers . To help customers handle common use cases for Route 53 provisioning the framework provides a few convenience providers that can be registered with the EKS Blueprint Stack. Name look-up and direct import provider: This provider will allow to bind to an existing hosted zone based on its name. const myDomainName = \"\" ; ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ssp . LookupHostedZoneProvider ( myDomainName )) . addOns ( new ssp . addons . ExternalDnsAddon ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . build (...); If the hosted zone ID is known, then the recommended approach is to use a ImportHostedZoneProvider which bypasses any lookup calls. const myHostedZoneId = \"\" ; ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ssp . addons . ImportHostedZoneProvider ( myHostedZoneId )) . addOns ( new ssp . addons . ExternalDnsAddon ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . build (...); Delegating Hosted Zone Provider In many cases, enterprises choose to decouple provisioning of root domains and subdomains. A common pattern is to have a specific DNS account (AWS account) hosting the root domain, while subdomains may be created within individual workload accounts. This implies cross-account access from child account to the parent DNS account for subdomain delegation. Prerequisites: Parent account defines an IAM role with the following managed policies: AmazonRoute53DomainsFullAccess AmazonRoute53ReadOnlyAccess AmazonRoute53AutoNamingFullAccess Create trust relationship between this role and the child account that is expected to add subdomains. For info see IAM tutorial . Example: Let's assume that parent DNS account parentAccountId has domain named myglobal-domain.com . Now when provisioned an SSP EKS cluster you would like to use a stage specific name like dev.myglobal-domain.com or test.myglobal-domain.com . In addition to these requirements, you would like to enable tenant specific access to those domains such as my-tenant1.dev.myglobal-domain.com or team specific access team-riker.dev.myglobal-domain.com . The setup will look the following way: In the parentAccountId account you create a role for delegation ( DomainOperatorRole in this example) and a trust relationship to the child account in which SSP EKS blueprint will be provisioned. In a general case, the number of child accounts can be large, so each of them will have to be listed in the trust relationship. In the parentAccountId you create a public hosted zone for myglobal-domain.com . With that the setup that may require separate automation (or a manual process) is complete. Use the following configuration of the add-on: ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ssp . DelegatingHostedZoneProvider ({ parentDomain : 'myglobal-domain.com' , subdomain : 'dev.myglobal-domain.com' , parentAccountId : parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true }) . addOns ( new ssp . addons . ExternalDnsAddon ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) The parameter wildcardSubdomain above when set to true will also create a CNAME for *.dev.myglobal-domain.com . This is at the moment used for host based routing within EKS (e.g. with NGINX ingress).","title":"Hosted Zone Providers"},{"location":"addons/external-dns/#configuration-options","text":"namespace : Optional target namespace where add-on will be installed. Changing this value on the operating cluster is not recommended. Set to external-dns by default. version : The add-on is leveraging a Bitnami helm chart. This parameter allows overriding the helm chart version used. hostedZone : Hosted zone provider is a interface that provides one or more hosted zones that the add-on will leverage for the service and ingress configuration.","title":"Configuration Options"},{"location":"addons/external-dns/#functionality","text":"Applies External-DNS configuration for AWS DNS provider. See AWS Tutorial for External DNS for more information. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/karpenter/","text":"Karpenter Add-on \u00b6 Karpenter add-on is based on the Karpenter open source node provisioning project. It provides a more efficient and cost-effective way to manage workloads by launching just the right compute resources to handle a cluster's application. Karpenter works by: Watching for pods that the Kubernetes scheduler has marked as unschedulable, Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods, Provisioning nodes that meet the requirements of the pods, Scheduling the pods to run on the new nodes, and Removing the nodes when the nodes are no longer needed Prerequisite \u00b6 (If using Spot), EC2 Spot Service Linked Role should be created. See here for more details. Usage \u00b6 import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new cdk . App (); const account = < AWS_ACCOUNT_ID > ; const region = < AWS_REGION > ; const env : { account , region }, const blueprint = ssp . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns ( new ssp . addons . KarpenterAddOn () ) . teams (). build ( app , 'my-stack-name' , { env }); To validate that Karpenter add-on is running ensure that the add-on deployments for the controller and the webhook are in RUNNING state: # Assuming add-on is installed in the karpenter namespace. $ kubectl get po -n karpenter NAME READY STATUS RESTARTS AGE karpenter-controller-79dc99d7cd-zkkf7 1 /1 Running 0 62m karpenter-webhook-7bf684c676-52chv 1 /1 Running 0 62m Functionality \u00b6 EKS VPC subnets are tagged with the following (as required by Karpenter): kubernetes.io/cluster/$CLUSTER_NAME . Creates Karpenter Node Role, Karpenter Instance Profile, and Karpenter Controller Policy (Please see Karpenter documentation here for more details on what is required and why) Creates karpenter namespace. Creates Kubernetes Service Account, and associate AWS IAM Role with Karpenter Controller Policy attached using IRSA . Deploys Karpenter helm chart in the karpenter namespace, configuring cluster name and cluster endpoint on the controller by default. (Optionally) provision a default Karpenter Provisioner CRD based on user-provided spec.requirements Using Karpenter \u00b6 To use Karpenter, you need to provision a Karpenter provisioner CRD . A single provisioner is capable of handling many different pod shapes. This can be done in 2 ways (either will yield the same provisioner): Provide a set of spec.requirements during add-on deployment. const provisionerSpecs = { 'node.kubernetes.io/instance-type' : [ 'm5.2xlarge' ], 'topology.kubernetes.io/zone' : [ 'us-east-1c' ], 'kubernetes.io/arch' : [ 'amd64' , 'arm64' ], 'karpenter.sh/capacity-type' : [ 'spot' , 'on-demand' ], } const karpenterAddOn = new ssp . addons . KarpenterAddOn ({ provisionerSpecs : provisionerSpecs }) If the provisionerSpecs is not provided at deploy time, the add-on will be installed without a Provisioner. Use kubectl to apply a provisioner manifest: cat <<EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: \"node.kubernetes.io/instance-type\" operator: In values: [\"m5.2xlarge\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"us-east-1c\"] - key: \"kubernetes.io/arch\" operator: In values: [\"arm64\", \"amd64\"] - key: \"karpenter.sh/capacity-type\" operator: In values: [\"spot\", \"on-demand\"] provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF Testing with a sample deployment \u00b6 Now that the provisioner is deployed, Karpenter is active and ready to provision nodes. Create some pods using a deployment: cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF Now scale the deployment: kubectl scale deployment inflate --replicas 10 The provisioner will then start deploying more nodes to deploy the scaled replicas. You can verify by either looking at the karpenter controller logs, kubectl logs -f -n karpenter $( kubectl get pods -n karpenter -l karpenter = controller -o name ) or, by looking at the nodes being created: kubectl get nodes","title":"Karpenter Add-on"},{"location":"addons/karpenter/#karpenter-add-on","text":"Karpenter add-on is based on the Karpenter open source node provisioning project. It provides a more efficient and cost-effective way to manage workloads by launching just the right compute resources to handle a cluster's application. Karpenter works by: Watching for pods that the Kubernetes scheduler has marked as unschedulable, Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods, Provisioning nodes that meet the requirements of the pods, Scheduling the pods to run on the new nodes, and Removing the nodes when the nodes are no longer needed","title":"Karpenter Add-on"},{"location":"addons/karpenter/#prerequisite","text":"(If using Spot), EC2 Spot Service Linked Role should be created. See here for more details.","title":"Prerequisite"},{"location":"addons/karpenter/#usage","text":"import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new cdk . App (); const account = < AWS_ACCOUNT_ID > ; const region = < AWS_REGION > ; const env : { account , region }, const blueprint = ssp . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns ( new ssp . addons . KarpenterAddOn () ) . teams (). build ( app , 'my-stack-name' , { env }); To validate that Karpenter add-on is running ensure that the add-on deployments for the controller and the webhook are in RUNNING state: # Assuming add-on is installed in the karpenter namespace. $ kubectl get po -n karpenter NAME READY STATUS RESTARTS AGE karpenter-controller-79dc99d7cd-zkkf7 1 /1 Running 0 62m karpenter-webhook-7bf684c676-52chv 1 /1 Running 0 62m","title":"Usage"},{"location":"addons/karpenter/#functionality","text":"EKS VPC subnets are tagged with the following (as required by Karpenter): kubernetes.io/cluster/$CLUSTER_NAME . Creates Karpenter Node Role, Karpenter Instance Profile, and Karpenter Controller Policy (Please see Karpenter documentation here for more details on what is required and why) Creates karpenter namespace. Creates Kubernetes Service Account, and associate AWS IAM Role with Karpenter Controller Policy attached using IRSA . Deploys Karpenter helm chart in the karpenter namespace, configuring cluster name and cluster endpoint on the controller by default. (Optionally) provision a default Karpenter Provisioner CRD based on user-provided spec.requirements","title":"Functionality"},{"location":"addons/karpenter/#using-karpenter","text":"To use Karpenter, you need to provision a Karpenter provisioner CRD . A single provisioner is capable of handling many different pod shapes. This can be done in 2 ways (either will yield the same provisioner): Provide a set of spec.requirements during add-on deployment. const provisionerSpecs = { 'node.kubernetes.io/instance-type' : [ 'm5.2xlarge' ], 'topology.kubernetes.io/zone' : [ 'us-east-1c' ], 'kubernetes.io/arch' : [ 'amd64' , 'arm64' ], 'karpenter.sh/capacity-type' : [ 'spot' , 'on-demand' ], } const karpenterAddOn = new ssp . addons . KarpenterAddOn ({ provisionerSpecs : provisionerSpecs }) If the provisionerSpecs is not provided at deploy time, the add-on will be installed without a Provisioner. Use kubectl to apply a provisioner manifest: cat <<EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: \"node.kubernetes.io/instance-type\" operator: In values: [\"m5.2xlarge\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"us-east-1c\"] - key: \"kubernetes.io/arch\" operator: In values: [\"arm64\", \"amd64\"] - key: \"karpenter.sh/capacity-type\" operator: In values: [\"spot\", \"on-demand\"] provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF","title":"Using Karpenter"},{"location":"addons/karpenter/#testing-with-a-sample-deployment","text":"Now that the provisioner is deployed, Karpenter is active and ready to provision nodes. Create some pods using a deployment: cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF Now scale the deployment: kubectl scale deployment inflate --replicas 10 The provisioner will then start deploying more nodes to deploy the scaled replicas. You can verify by either looking at the karpenter controller logs, kubectl logs -f -n karpenter $( kubectl get pods -n karpenter -l karpenter = controller -o name ) or, by looking at the nodes being created: kubectl get nodes","title":"Testing with a sample deployment"},{"location":"addons/kube-proxy/","text":"Kube-proxy Amazon EKS Add-on \u00b6 The Kube-proxy Amazon EKS Add-on adds support for kube-proxy . Kube-proxy maintains network rules on each Amazon EC2 node. It enables network communication to your pods. Kube-proxy is not deployed to Fargate nodes. For more information, see kube-proxy in the Kubernetes documentation. Installing Kube-proxy as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs Kube-proxy as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons. Prerequisite \u00b6 Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later. Usage \u00b6 import * as ssp from '@shapirov/cdk-eks-blueprint' ; readonly kubeProxy = new ssp . addons . KubeProxyAddOn ( \"v1.19.6-eksbuild.2\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ kubeProxy ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Configuration Options \u00b6 version : Pass in the kube-proxy plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.20, below command shows versions of the Kube-proxy add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name kube-proxy \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.19.6-eksbuild.2 True v1.18.8-eksbuild.1 False Validation \u00b6 To validate that kube-proxy add-on is running, ensure that the pod is in Running state $ kubectl get pods -n kube-system | grep kube-proxy NAME READY STATUS RESTARTS AGE kube-proxy-6lrjm 1 /1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of Kube-proxy installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name kube-proxy \\ --query \"addon.addonVersion\" \\ --output text # Output v1.19.6-eksbuild.2 Functionality \u00b6 Applies Kube-proxy add-on to Amazon EKS cluster.","title":"Kube Proxy"},{"location":"addons/kube-proxy/#kube-proxy-amazon-eks-add-on","text":"The Kube-proxy Amazon EKS Add-on adds support for kube-proxy . Kube-proxy maintains network rules on each Amazon EC2 node. It enables network communication to your pods. Kube-proxy is not deployed to Fargate nodes. For more information, see kube-proxy in the Kubernetes documentation. Installing Kube-proxy as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs Kube-proxy as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons.","title":"Kube-proxy Amazon EKS Add-on"},{"location":"addons/kube-proxy/#prerequisite","text":"Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.","title":"Prerequisite"},{"location":"addons/kube-proxy/#usage","text":"import * as ssp from '@shapirov/cdk-eks-blueprint' ; readonly kubeProxy = new ssp . addons . KubeProxyAddOn ( \"v1.19.6-eksbuild.2\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ kubeProxy ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/kube-proxy/#configuration-options","text":"version : Pass in the kube-proxy plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.20, below command shows versions of the Kube-proxy add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name kube-proxy \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.19.6-eksbuild.2 True v1.18.8-eksbuild.1 False","title":"Configuration Options"},{"location":"addons/kube-proxy/#validation","text":"To validate that kube-proxy add-on is running, ensure that the pod is in Running state $ kubectl get pods -n kube-system | grep kube-proxy NAME READY STATUS RESTARTS AGE kube-proxy-6lrjm 1 /1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of Kube-proxy installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name kube-proxy \\ --query \"addon.addonVersion\" \\ --output text # Output v1.19.6-eksbuild.2","title":"Validation"},{"location":"addons/kube-proxy/#functionality","text":"Applies Kube-proxy add-on to Amazon EKS cluster.","title":"Functionality"},{"location":"addons/kubecost/","text":"Kubecost AddOn \u00b6 Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts. By identifying root causes for negative patterns, customers using Kubecost save 30-50% or more of their Kubernetes cloud infrastructure costs. To read more about Kubecost and how to use it, see the product and technical docs . Installation \u00b6 Using npm : $ npm install @kubecost/kubecost-ssp-addon Usage \u00b6 import * as cdk from '@aws-cdk/core' ; import { EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; import { KubecostAddOn } from '@kubecost/kubecost-ssp-addon' ; export default class KubecostConstruct extends cdk . Construct { constructor ( scope : cdk.Construct , id : string ) { super ( scope , id ); // AddOns for the cluster const stackId = ` ${ id } -blueprint` ; EksBlueprint . builder () . account ( process . env . CDK_DEFAULT_ACCOUNT ! ) . region ( process . env . CDK_DEFAULT_REGION ) . addOns ( new KubecostAddOn ()) . build ( scope , stackId ); } } KubecostAddOn Options (props) \u00b6 namespace: string (optional) \u00b6 The namespace where Kubecost will be installed. Defaults to kubecost . kubecostToken: string (optional) \u00b6 You may get one here . version: string (optional) \u00b6 The cost-analyzer helm chart version. Defaults to the latest stable version specified in this repo ( 1.88.1 at the time of writing). values?: { [key: string]: any } (optional) \u00b6 Custom values to pass to the chart. Config options: https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options customPrometheus: string (optional) \u00b6 Kubecost comes bundled with a Prometheus installation. However, if you wish to integrate with an external Prometheus deployment, provide your local Prometheus service address with this format http://..svc . Note: integrating with an existing Prometheus is only officially supported under Kubecost paid plans and requires some extra configurations on your Prometheus: https://docs.kubecost.com/custom-prom.html installPrometheusNodeExporter: boolean (optional) \u00b6 Set to false to use an existing Node Exporter DaemonSet. Note: this requires your existing Node Exporter endpoint to be visible from the namespace where Kubecost is installed. https://github.com/kubecost/docs/blob/main/getting-started.md#using-an-existing-node-exporter repository: string , release: string , chart: string (optional) \u00b6 Additional options for customers who may need to supply their own private Helm repository. Support \u00b6 If you have any questions about Kubecost, get in touch with the team on Slack . License \u00b6 The Kubecost SSP AddOn is licensed under the Apache 2.0 license. Project repository","title":"Kubecost AddOn"},{"location":"addons/kubecost/#kubecost-addon","text":"Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts. By identifying root causes for negative patterns, customers using Kubecost save 30-50% or more of their Kubernetes cloud infrastructure costs. To read more about Kubecost and how to use it, see the product and technical docs .","title":"Kubecost AddOn"},{"location":"addons/kubecost/#installation","text":"Using npm : $ npm install @kubecost/kubecost-ssp-addon","title":"Installation"},{"location":"addons/kubecost/#usage","text":"import * as cdk from '@aws-cdk/core' ; import { EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; import { KubecostAddOn } from '@kubecost/kubecost-ssp-addon' ; export default class KubecostConstruct extends cdk . Construct { constructor ( scope : cdk.Construct , id : string ) { super ( scope , id ); // AddOns for the cluster const stackId = ` ${ id } -blueprint` ; EksBlueprint . builder () . account ( process . env . CDK_DEFAULT_ACCOUNT ! ) . region ( process . env . CDK_DEFAULT_REGION ) . addOns ( new KubecostAddOn ()) . build ( scope , stackId ); } }","title":"Usage"},{"location":"addons/kubecost/#kubecostaddon-options-props","text":"","title":"KubecostAddOn Options (props)"},{"location":"addons/kubecost/#namespace-string-optional","text":"The namespace where Kubecost will be installed. Defaults to kubecost .","title":"namespace: string (optional)"},{"location":"addons/kubecost/#kubecosttoken-string-optional","text":"You may get one here .","title":"kubecostToken: string (optional)"},{"location":"addons/kubecost/#version-string-optional","text":"The cost-analyzer helm chart version. Defaults to the latest stable version specified in this repo ( 1.88.1 at the time of writing).","title":"version: string (optional)"},{"location":"addons/kubecost/#values-key-string-any-optional","text":"Custom values to pass to the chart. Config options: https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options","title":"values?: { [key: string]: any } (optional)"},{"location":"addons/kubecost/#customprometheus-string-optional","text":"Kubecost comes bundled with a Prometheus installation. However, if you wish to integrate with an external Prometheus deployment, provide your local Prometheus service address with this format http://..svc . Note: integrating with an existing Prometheus is only officially supported under Kubecost paid plans and requires some extra configurations on your Prometheus: https://docs.kubecost.com/custom-prom.html","title":"customPrometheus: string (optional)"},{"location":"addons/kubecost/#installprometheusnodeexporter-boolean-optional","text":"Set to false to use an existing Node Exporter DaemonSet. Note: this requires your existing Node Exporter endpoint to be visible from the namespace where Kubecost is installed. https://github.com/kubecost/docs/blob/main/getting-started.md#using-an-existing-node-exporter","title":"installPrometheusNodeExporter: boolean (optional)"},{"location":"addons/kubecost/#repository-string-release-string-chart-string-optional","text":"Additional options for customers who may need to supply their own private Helm repository.","title":"repository: string, release: string, chart: string (optional)"},{"location":"addons/kubecost/#support","text":"If you have any questions about Kubecost, get in touch with the team on Slack .","title":"Support"},{"location":"addons/kubecost/#license","text":"The Kubecost SSP AddOn is licensed under the Apache 2.0 license. Project repository","title":"License"},{"location":"addons/kubevious/","text":"Kubevious Add-on \u00b6 This add-on installs Kubevious open source Kubernetes dashboard on Amazon EKS. Kubevious provides logical grouping of application resources eliminating the need to dig through selectors and labels. It also provides the ability identify potential misconfigurations using both standard and user created rules that monitor the cluster Usage \u00b6 import { App } from '@aws-cdk/core' ; import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new App (); ssp . EksBlueprint . builder () . addOns ( new ssp . KubeviousAddOn () ) . build ( app , 'my-cluster' ); Configuration Options \u00b6 version : Version fo the Helm Chart to be used to install Kubevious ingressEnabled : Indicates whether to expose Kubevious using an ingress gateway. Set to false by default kubeviousServiceType : Type of service used to expose Kubevious backend. Set to 'ClusterIP' by default values : Arbitrary values to pass to the chart. Refer to the Kubevious Helm Chart documentation for additional details Validation \u00b6 To validate that Kubevious is installed properly in the cluster, check that the Kubevious deployments, services and stateful sets are running. kubectl get all -n kubevious Note that Kubevious is installed in its own kubevious namespace Accessing the Kubevious dashboard \u00b6 To access the application, set up port-forwarding as follows: kubectl port-forward $( kubectl get pods -n kubevious -l \"app.kubernetes.io/component=kubevious-ui\" -o jsonpath = \"{.items[0].metadata.name}\" ) 8080 :80 -n kubevious After the port-forwarding has started, the application can be accessed by navigating to http://localhost:8080 Alternatively, Kubevious can be exposed by enabling the ingress by setting the ingressEnabled configuration option to true. MySQL root password \u00b6 Kubevious internally deploys and uses MySQL to persist data. The Kubevious add-on secures access to the database by generating a random password for the MySQL root user. While it is not usually necessary to access the Kubevious MySQL database externally, it is possible to retrieve the generated value by executing the command below: echo $( kubectl get secret kubevious-mysql-secret-root -o jsonpath = '{.data.MYSQL_ROOT_PASSWORD}' -n kubevious ) | base64 --decode Functionality \u00b6 Installs Kubevious in the cluster Sets up all AIM necessary roles to integrate Kubevious in AWS EKS Supports standard helm configuration options .","title":"Kubevious Add-on"},{"location":"addons/kubevious/#kubevious-add-on","text":"This add-on installs Kubevious open source Kubernetes dashboard on Amazon EKS. Kubevious provides logical grouping of application resources eliminating the need to dig through selectors and labels. It also provides the ability identify potential misconfigurations using both standard and user created rules that monitor the cluster","title":"Kubevious Add-on"},{"location":"addons/kubevious/#usage","text":"import { App } from '@aws-cdk/core' ; import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new App (); ssp . EksBlueprint . builder () . addOns ( new ssp . KubeviousAddOn () ) . build ( app , 'my-cluster' );","title":"Usage"},{"location":"addons/kubevious/#configuration-options","text":"version : Version fo the Helm Chart to be used to install Kubevious ingressEnabled : Indicates whether to expose Kubevious using an ingress gateway. Set to false by default kubeviousServiceType : Type of service used to expose Kubevious backend. Set to 'ClusterIP' by default values : Arbitrary values to pass to the chart. Refer to the Kubevious Helm Chart documentation for additional details","title":"Configuration Options"},{"location":"addons/kubevious/#validation","text":"To validate that Kubevious is installed properly in the cluster, check that the Kubevious deployments, services and stateful sets are running. kubectl get all -n kubevious Note that Kubevious is installed in its own kubevious namespace","title":"Validation"},{"location":"addons/kubevious/#accessing-the-kubevious-dashboard","text":"To access the application, set up port-forwarding as follows: kubectl port-forward $( kubectl get pods -n kubevious -l \"app.kubernetes.io/component=kubevious-ui\" -o jsonpath = \"{.items[0].metadata.name}\" ) 8080 :80 -n kubevious After the port-forwarding has started, the application can be accessed by navigating to http://localhost:8080 Alternatively, Kubevious can be exposed by enabling the ingress by setting the ingressEnabled configuration option to true.","title":"Accessing the Kubevious dashboard"},{"location":"addons/kubevious/#mysql-root-password","text":"Kubevious internally deploys and uses MySQL to persist data. The Kubevious add-on secures access to the database by generating a random password for the MySQL root user. While it is not usually necessary to access the Kubevious MySQL database externally, it is possible to retrieve the generated value by executing the command below: echo $( kubectl get secret kubevious-mysql-secret-root -o jsonpath = '{.data.MYSQL_ROOT_PASSWORD}' -n kubevious ) | base64 --decode","title":"MySQL root password"},{"location":"addons/kubevious/#functionality","text":"Installs Kubevious in the cluster Sets up all AIM necessary roles to integrate Kubevious in AWS EKS Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/metrics-server/","text":"Metrics Server AddOn \u00b6 Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It is not deployed by default in Amazon EKS clusters. The Metrics Server is commonly used by other Kubernetes add ons, such as the Horizontal Pod Autoscaler , Vertical Autoscaling or the Kubernetes Dashboard . Important : Don't use Metrics Server when you need an accurate source of resource usage metrics or as a monitoring solution. Usage \u00b6 index.ts \u00b6 import { MetricsServerAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; # Deploy Metrics Server v0 .5.0 const addOn = new MetricsServerAddOn ( 'v0.5.0' ); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Once deployed, you can see metrics-server pod in the kube-system namespace. $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1 /1 1 1 20m Functionality \u00b6 Deploys the metrics-server helm chart in kube-system namespace by default. Supports standard helm configuration options . Testing with Kubernetes Dashboard \u00b6 For testing, we will use the Kubernetes Dashboard to view CPU and memory metrics of our cluster. Apply the kubernetes dashboard manifest. $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created Create a file called eks-admin-service-account.yaml with the text below. This manifest defines a service account and cluster role binding called eks-admin. $ cat << 'EOF' >> eks-admin-service-account.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: eks-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: eks-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: eks-admin namespace: kube-system EOF Apply the service account and cluster role binding to your cluster. $ kubectl apply -f eks-admin-service-account.yaml serviceaccount/eks-admin created clusterrolebinding.rbac.authorization.k8s.io/eks-admin created Retrieve an authentication token for the eks-admin service account. Copy the value from the output. You use this token to connect to the dashboard. $ kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}' ) Name: eks-admin-token-dwzb2 Namespace: kube-system Labels: <none> Annotations: kubernetes.io/service-account.name: eks-admin kubernetes.io/service-account.uid: 6fb4eb46-553e-44bf-b0e7-9ae8f5f500d6 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1066 bytes namespace: 11 bytes token: XXXXXXXXXXXXXXXXXXXXXX Start the kubectl proxy. $ kubectl proxy Open the dashboard in your browser and login using the value for token above. Note : It may take a few minutes before CPU and memory metrics appear in the dashboard","title":"Metrics Server"},{"location":"addons/metrics-server/#metrics-server-addon","text":"Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It is not deployed by default in Amazon EKS clusters. The Metrics Server is commonly used by other Kubernetes add ons, such as the Horizontal Pod Autoscaler , Vertical Autoscaling or the Kubernetes Dashboard . Important : Don't use Metrics Server when you need an accurate source of resource usage metrics or as a monitoring solution.","title":"Metrics Server AddOn"},{"location":"addons/metrics-server/#usage","text":"","title":"Usage"},{"location":"addons/metrics-server/#indexts","text":"import { MetricsServerAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; # Deploy Metrics Server v0 .5.0 const addOn = new MetricsServerAddOn ( 'v0.5.0' ); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Once deployed, you can see metrics-server pod in the kube-system namespace. $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1 /1 1 1 20m","title":"index.ts"},{"location":"addons/metrics-server/#functionality","text":"Deploys the metrics-server helm chart in kube-system namespace by default. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/metrics-server/#testing-with-kubernetes-dashboard","text":"For testing, we will use the Kubernetes Dashboard to view CPU and memory metrics of our cluster. Apply the kubernetes dashboard manifest. $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created Create a file called eks-admin-service-account.yaml with the text below. This manifest defines a service account and cluster role binding called eks-admin. $ cat << 'EOF' >> eks-admin-service-account.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: eks-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: eks-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: eks-admin namespace: kube-system EOF Apply the service account and cluster role binding to your cluster. $ kubectl apply -f eks-admin-service-account.yaml serviceaccount/eks-admin created clusterrolebinding.rbac.authorization.k8s.io/eks-admin created Retrieve an authentication token for the eks-admin service account. Copy the value from the output. You use this token to connect to the dashboard. $ kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}' ) Name: eks-admin-token-dwzb2 Namespace: kube-system Labels: <none> Annotations: kubernetes.io/service-account.name: eks-admin kubernetes.io/service-account.uid: 6fb4eb46-553e-44bf-b0e7-9ae8f5f500d6 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1066 bytes namespace: 11 bytes token: XXXXXXXXXXXXXXXXXXXXXX Start the kubectl proxy. $ kubectl proxy Open the dashboard in your browser and login using the value for token above. Note : It may take a few minutes before CPU and memory metrics appear in the dashboard","title":"Testing with Kubernetes Dashboard"},{"location":"addons/nginx/","text":"NGINX Add-on \u00b6 This add-on installs NGINX Ingress Controller on Amazon EKS. NGINX ingress controller is using NGINX as a reverse proxy and load balancer. Other than handling Kubernetes ingress objects, this ingress controller can facilitate multi-tenancy and segregation of workload ingresses based on host name (host-based routing) and/or URL Path (path based routing). IMPORTANT : This add-on depends on AWS Load Balancer Controller Add-on in order to enable NLB support. AWS Load Balancer Controller add-on must be present in add-on array and must be in add-on array before the NGINX ingress controller add-on for it to work, as shown in below example. Otherwise will run into error Assertion failed: Missing a dependency for AwsLoadBalancerControllerAddOn . Usage \u00b6 import { AwsLoadBalancerControllerAddOn , NginxAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const externalDnsHostname = ...; const awsLbControllerAddOn = new AwsLoadBalancerControllerAddon (); const nginxAddOn = new NginxAddOn ({ externalDnsHostname }); const addOns : Array < ClusterAddOn > = [ awsLbControllerAddOn , nginxAddOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > }, }); To validate that installation is successful run the following command: $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE ssp-addon-nginx-ingress-78b8567p4q6 1 /1 Running 0 4d10h Note that the ingress controller is deployed in the kube-system namespace. Once deployed, it allows applications to create ingress objects and use host based routing with external DNS support, if External DNS Add-on is installed. Configuration \u00b6 backendProtocol : indication for AWS Load Balancer controller with respect to the protocol supported on the load balancer. TCP by default. crossZoneEnabled : whether to create a cross-zone load balancer with the service that backs NGINX. internetFacing : whether the created load balancer is internet facing. Defaults to true if not specified. Internal load balancer is provisioned if set to false targetType : IP or instance mode. Defaults to IP which requires VPC-CNI and has better performance eliminating a hop through kubeproxy. Instance mode leverages traditional NodePort mode on the instances. externaDnsHostname : Used in conjunction with the external DNS add-on to handle automatic registration of the service with Route53. values : Arbitrary values to pass to the chart as per https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/# DNS Integration and Routing \u00b6 If External DNS Add-on is installed, it is possible to configure NGINX ingress with an external NLB load balancer and leverage wild-card DNS domains (and public certificate) to route external traffic to individual workloads. The following example provides support for AWS Load Balancer controller, External DNS and NGINX add-ons to enable such routing: ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ssp . DelegatingHostedZoneProvider ({ parentDomain : 'myglobal-domain.com' , subdomain : 'dev.myglobal-domain.com' , parentAccountId : parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true }) . addOns ( new ssp . addons . ExternalDnsAddon ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . addOns ( new ssp . NginxAddOn ({ internetFacing : true , backendProtocol : \"tcp\" , externaDnsHostname : subdomain , crossZoneEnabled : false }) . build (...); Assuming the subdomain in the above example is dev.my-domain.com and wildcard is enabled for the external DNS add-on customers can now create ingress objects for host-based routing. Let's define an ingress object for team-riker that is currently deploying guestbook application with no ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : ingress-riker namespace : team-riker spec : rules : - host : riker.dev.my-domain.com http : paths : - backend : serviceName : guestbook-ui servicePort : 80 path : / pathType : Prefix A similar ingress may be defined for team-troi routing to the workloads deployed by that team: apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : ingress-troi namespace : team-troi spec : rules : - host : troi.dev.my-domain.com http : paths : - backend : serviceName : guestbook-ui servicePort : 80 path : / pathType : Prefix After the above ingresses applied (ideally through a GitOps engine) you can now navigate to the specified hosts respectively: http://riker.dev.my-domain.com http://troi.dev.my-domain.com TLS Termination and Certificates \u00b6 You can configure the NGINX add-on to terminate TLS at the load balancer and supply an ACM certificate through the platform blueprint. A certificate can be registered using a named resource provider . For convenience the framework provides a couple of common certificate providers: Import Certificate This case is used when certificate is already created and you just need to reference it with the blueprint stack: const myCertArn = \"\" ; ssp . EksBlueprint . builder () . resourceProvider ( GlobalResources . Certificate , new ImportCertificateProvider ( myCertArn , \"cert1-id\" )) . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-cert-provider' ); Create Certificate This approach is used when certificate should be created with the blueprint stack. In this case, the new certificate requires DNS validation which can be accomplished automatically if the corresponding Route53 hosted zone is provisioned (either along with the stack or separately) and registered as a resource provider. ssp . EksBlueprint . builder () . resourceProvider ( GlobalResources . HostedZone , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) . resourceProvider ( GlobalResources . Certificate , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , GlobalResources . HostedZone )) // referencing hosted zone for automatic DNS validation . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zone for External DNS . addOns ( new ExternalDnsAddon ({ hostedZoneResources : [ GlobalResources . HostedZone ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' ); Functionality \u00b6 Installs NGINX ingress controller Provides convenience options to integrate with AWS Load Balancer controller to leverage NLB for the load balancer Provides convenience options to integrate with External DNS add-on for integration with Amazon Route 53. Allows configuring TLS termination at the load balancer provisioned with the add-on. Supports standard helm configuration options .","title":"Nginx"},{"location":"addons/nginx/#nginx-add-on","text":"This add-on installs NGINX Ingress Controller on Amazon EKS. NGINX ingress controller is using NGINX as a reverse proxy and load balancer. Other than handling Kubernetes ingress objects, this ingress controller can facilitate multi-tenancy and segregation of workload ingresses based on host name (host-based routing) and/or URL Path (path based routing). IMPORTANT : This add-on depends on AWS Load Balancer Controller Add-on in order to enable NLB support. AWS Load Balancer Controller add-on must be present in add-on array and must be in add-on array before the NGINX ingress controller add-on for it to work, as shown in below example. Otherwise will run into error Assertion failed: Missing a dependency for AwsLoadBalancerControllerAddOn .","title":"NGINX Add-on"},{"location":"addons/nginx/#usage","text":"import { AwsLoadBalancerControllerAddOn , NginxAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const externalDnsHostname = ...; const awsLbControllerAddOn = new AwsLoadBalancerControllerAddon (); const nginxAddOn = new NginxAddOn ({ externalDnsHostname }); const addOns : Array < ClusterAddOn > = [ awsLbControllerAddOn , nginxAddOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > }, }); To validate that installation is successful run the following command: $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE ssp-addon-nginx-ingress-78b8567p4q6 1 /1 Running 0 4d10h Note that the ingress controller is deployed in the kube-system namespace. Once deployed, it allows applications to create ingress objects and use host based routing with external DNS support, if External DNS Add-on is installed.","title":"Usage"},{"location":"addons/nginx/#configuration","text":"backendProtocol : indication for AWS Load Balancer controller with respect to the protocol supported on the load balancer. TCP by default. crossZoneEnabled : whether to create a cross-zone load balancer with the service that backs NGINX. internetFacing : whether the created load balancer is internet facing. Defaults to true if not specified. Internal load balancer is provisioned if set to false targetType : IP or instance mode. Defaults to IP which requires VPC-CNI and has better performance eliminating a hop through kubeproxy. Instance mode leverages traditional NodePort mode on the instances. externaDnsHostname : Used in conjunction with the external DNS add-on to handle automatic registration of the service with Route53. values : Arbitrary values to pass to the chart as per https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/#","title":"Configuration"},{"location":"addons/nginx/#dns-integration-and-routing","text":"If External DNS Add-on is installed, it is possible to configure NGINX ingress with an external NLB load balancer and leverage wild-card DNS domains (and public certificate) to route external traffic to individual workloads. The following example provides support for AWS Load Balancer controller, External DNS and NGINX add-ons to enable such routing: ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ssp . DelegatingHostedZoneProvider ({ parentDomain : 'myglobal-domain.com' , subdomain : 'dev.myglobal-domain.com' , parentAccountId : parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true }) . addOns ( new ssp . addons . ExternalDnsAddon ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . addOns ( new ssp . NginxAddOn ({ internetFacing : true , backendProtocol : \"tcp\" , externaDnsHostname : subdomain , crossZoneEnabled : false }) . build (...); Assuming the subdomain in the above example is dev.my-domain.com and wildcard is enabled for the external DNS add-on customers can now create ingress objects for host-based routing. Let's define an ingress object for team-riker that is currently deploying guestbook application with no ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : ingress-riker namespace : team-riker spec : rules : - host : riker.dev.my-domain.com http : paths : - backend : serviceName : guestbook-ui servicePort : 80 path : / pathType : Prefix A similar ingress may be defined for team-troi routing to the workloads deployed by that team: apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : ingress-troi namespace : team-troi spec : rules : - host : troi.dev.my-domain.com http : paths : - backend : serviceName : guestbook-ui servicePort : 80 path : / pathType : Prefix After the above ingresses applied (ideally through a GitOps engine) you can now navigate to the specified hosts respectively: http://riker.dev.my-domain.com http://troi.dev.my-domain.com","title":"DNS Integration and Routing"},{"location":"addons/nginx/#tls-termination-and-certificates","text":"You can configure the NGINX add-on to terminate TLS at the load balancer and supply an ACM certificate through the platform blueprint. A certificate can be registered using a named resource provider . For convenience the framework provides a couple of common certificate providers: Import Certificate This case is used when certificate is already created and you just need to reference it with the blueprint stack: const myCertArn = \"\" ; ssp . EksBlueprint . builder () . resourceProvider ( GlobalResources . Certificate , new ImportCertificateProvider ( myCertArn , \"cert1-id\" )) . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-cert-provider' ); Create Certificate This approach is used when certificate should be created with the blueprint stack. In this case, the new certificate requires DNS validation which can be accomplished automatically if the corresponding Route53 hosted zone is provisioned (either along with the stack or separately) and registered as a resource provider. ssp . EksBlueprint . builder () . resourceProvider ( GlobalResources . HostedZone , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) . resourceProvider ( GlobalResources . Certificate , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , GlobalResources . HostedZone )) // referencing hosted zone for automatic DNS validation . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zone for External DNS . addOns ( new ExternalDnsAddon ({ hostedZoneResources : [ GlobalResources . HostedZone ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' );","title":"TLS Termination and Certificates"},{"location":"addons/nginx/#functionality","text":"Installs NGINX ingress controller Provides convenience options to integrate with AWS Load Balancer controller to leverage NLB for the load balancer Provides convenience options to integrate with External DNS add-on for integration with Amazon Route 53. Allows configuring TLS termination at the load balancer provisioned with the add-on. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/opa-gatekeeper/","text":"What is OPA Gatekeeper? \u00b6 The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. OPA uses a policy language known as Rego which is a query language which was purpose built to support structured document models such as JSON. To learn more about Rego check out this link . OPA Gatekeeper is an open-source project that provides a first-class integration between OPA and Kubernetes. What Gatekeeper adds is an extensible parameterized policy library that includes native Kubernetes CRD's for instantiating and extending the OPA policy library. The Kubernetes API Server is configured to query OPA for admission control decisions when objects (e.g., Pods, Services, etc.) are created, updated, or deleted. The API Server sends the entire Kubernetes object in the webhook request to OPA. OPA evaluates the policies it has loaded using the admission review as input. Gatekeeper also provides audit functionality as well. The diagram below shows the flow between a user making a request to the Kube-API server and how AdmissionReview and AdmissionRequests are made through OPA Gatekeeper. ) In the context of a Shared Services Platform running on Amazon EKS, platform teams and administrators need a way of being able to set policies to adhere to governance and security requirements for all workloads and teams working on the same cluster. Examples of standard use cases for using policies via OPA Gatekeeper are listed below: Which users can access which resources. Which subnets egress traffic is allowed to. Which clusters a workload must be deployed to. Which registries binaries can be downloaded from. Which OS capabilities a container can execute with. Which times of day the system can be accessed at. RBAC (role-based access control) can help with some of the scenarios above but roles are nothing but a group of permissions that you then assign to users leveraging rolebindings. If for example, a user tries to perform an operation (get, list, watch, create, etc...) that particular user may do so if they have the appropriate role. Please note that RBAC should be used in conjunction with OPA Gatekeeper policies to fully secure your cluster. Key Terminology \u00b6 OPA Constraint Framework - Framework that enforces CRD-based policies and allow declaratively configured policies to be reliably shareable Constraint - A Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected. Enforcement Point - Places where constraints can be enforced. Examples are Git hooks, Kubernetes admission controllers, and audit systems. Constraint Template - Templates that allows users to declare new constraints Target - Represents a coherent set of objects sharing a common identification and/or selection scheme, generic purpose, and can be analyzed in the same validation context Usage \u00b6 import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new cdk . App (); const account = < AWS_ACCOUNT_ID > ; const region = < AWS_REGION > ; const blueprint = ssp . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns ( new ssp . addons . OpaGatekeeperAddOn () ) . teams (). build ( app , 'my-stack-name' ); To validate that OPA Gatekeeper is running within your cluster run the following command: k get po -n gatekeeper-system You should see the following output: NAME READY STATUS RESTARTS AGE gatekeeper-audit-7c5998d4c-b5n7j 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-b86zm 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-bntdt 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-tb7fz 1 /1 Running 0 1d You will notice the gatekeeper-audit-7c5998d4c-b5n7j pod that is created when we deploy the OpaGatekeeperAddOn . The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint. The gatekeeper-controller-manager is simply there to manage the OpaGatekeeperAddOn . Example with OPA Gatekeeper \u00b6 For the purposes of operating within a Shared Services Platform, we will be focusing on how to use a policy driven approach to secure our cluster using OPA Gatekeeper. The OPA Gatekeeper community has created a library of example policies and constraint templates which can be found here . In this example we will create a policy that enforces including labels for newly created namespaces and pods. The ConstraintTemplate can be found here . Run the following command to create the ConstraintTemplate: kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/template.yaml To verify that the ConstraintTemplate was created run the following command: kubectl get constrainttemplate You should see the following output: NAME AGE k8srequiredlabels 45s You will notice that if you create a new namespace without any labels that the request will go through and that is because we now need to create the individual Constraint CRD as defined by the Constraint Template that we created above. Let's create the individal Constraint CRD using the command below: k apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/samples/all-must-have-owner/constraint.yaml If we then try and create a namespace by running kubectl create ns test (notice that we are not adding any labels) you will get the following error message: Error from server ([ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username ) : admission webhook \"validation.gatekeeper.sh\" denied the request: [ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username For more information on OPA Gatekeeper please refer to the links below: https://github.com/open-policy-agent https://open-policy-agent.github.io/gatekeeper/website/docs/ https://github.com/open-policy-agent/gatekeeper-library","title":"What is OPA Gatekeeper?"},{"location":"addons/opa-gatekeeper/#what-is-opa-gatekeeper","text":"The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. OPA uses a policy language known as Rego which is a query language which was purpose built to support structured document models such as JSON. To learn more about Rego check out this link . OPA Gatekeeper is an open-source project that provides a first-class integration between OPA and Kubernetes. What Gatekeeper adds is an extensible parameterized policy library that includes native Kubernetes CRD's for instantiating and extending the OPA policy library. The Kubernetes API Server is configured to query OPA for admission control decisions when objects (e.g., Pods, Services, etc.) are created, updated, or deleted. The API Server sends the entire Kubernetes object in the webhook request to OPA. OPA evaluates the policies it has loaded using the admission review as input. Gatekeeper also provides audit functionality as well. The diagram below shows the flow between a user making a request to the Kube-API server and how AdmissionReview and AdmissionRequests are made through OPA Gatekeeper. ) In the context of a Shared Services Platform running on Amazon EKS, platform teams and administrators need a way of being able to set policies to adhere to governance and security requirements for all workloads and teams working on the same cluster. Examples of standard use cases for using policies via OPA Gatekeeper are listed below: Which users can access which resources. Which subnets egress traffic is allowed to. Which clusters a workload must be deployed to. Which registries binaries can be downloaded from. Which OS capabilities a container can execute with. Which times of day the system can be accessed at. RBAC (role-based access control) can help with some of the scenarios above but roles are nothing but a group of permissions that you then assign to users leveraging rolebindings. If for example, a user tries to perform an operation (get, list, watch, create, etc...) that particular user may do so if they have the appropriate role. Please note that RBAC should be used in conjunction with OPA Gatekeeper policies to fully secure your cluster.","title":"What is OPA Gatekeeper?"},{"location":"addons/opa-gatekeeper/#key-terminology","text":"OPA Constraint Framework - Framework that enforces CRD-based policies and allow declaratively configured policies to be reliably shareable Constraint - A Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected. Enforcement Point - Places where constraints can be enforced. Examples are Git hooks, Kubernetes admission controllers, and audit systems. Constraint Template - Templates that allows users to declare new constraints Target - Represents a coherent set of objects sharing a common identification and/or selection scheme, generic purpose, and can be analyzed in the same validation context","title":"Key Terminology"},{"location":"addons/opa-gatekeeper/#usage","text":"import * as ssp from '@aws-quickstart/ssp-amazon-eks' ; const app = new cdk . App (); const account = < AWS_ACCOUNT_ID > ; const region = < AWS_REGION > ; const blueprint = ssp . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns ( new ssp . addons . OpaGatekeeperAddOn () ) . teams (). build ( app , 'my-stack-name' ); To validate that OPA Gatekeeper is running within your cluster run the following command: k get po -n gatekeeper-system You should see the following output: NAME READY STATUS RESTARTS AGE gatekeeper-audit-7c5998d4c-b5n7j 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-b86zm 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-bntdt 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-tb7fz 1 /1 Running 0 1d You will notice the gatekeeper-audit-7c5998d4c-b5n7j pod that is created when we deploy the OpaGatekeeperAddOn . The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint. The gatekeeper-controller-manager is simply there to manage the OpaGatekeeperAddOn .","title":"Usage"},{"location":"addons/opa-gatekeeper/#example-with-opa-gatekeeper","text":"For the purposes of operating within a Shared Services Platform, we will be focusing on how to use a policy driven approach to secure our cluster using OPA Gatekeeper. The OPA Gatekeeper community has created a library of example policies and constraint templates which can be found here . In this example we will create a policy that enforces including labels for newly created namespaces and pods. The ConstraintTemplate can be found here . Run the following command to create the ConstraintTemplate: kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/template.yaml To verify that the ConstraintTemplate was created run the following command: kubectl get constrainttemplate You should see the following output: NAME AGE k8srequiredlabels 45s You will notice that if you create a new namespace without any labels that the request will go through and that is because we now need to create the individual Constraint CRD as defined by the Constraint Template that we created above. Let's create the individal Constraint CRD using the command below: k apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/samples/all-must-have-owner/constraint.yaml If we then try and create a namespace by running kubectl create ns test (notice that we are not adding any labels) you will get the following error message: Error from server ([ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username ) : admission webhook \"validation.gatekeeper.sh\" denied the request: [ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username For more information on OPA Gatekeeper please refer to the links below: https://github.com/open-policy-agent https://open-policy-agent.github.io/gatekeeper/website/docs/ https://github.com/open-policy-agent/gatekeeper-library","title":"Example with OPA Gatekeeper"},{"location":"addons/secrets-store/","text":"Secrets Store Add-on \u00b6 The Secrets Store Add-on provisions the AWS Secrets Manager and Config Provider(ASCP) for Secret Store CSI Driver on your EKS cluster. With ASCP, you now have a plugin for the industry-standard Kubernetes Secrets Store Container Storage Interface (CSI) Driver used for providing secrets to applications operating on Amazon Elastic Kubernetes Service. With ASCP, you can securely store and manage your secrets in AWS Secrets Manager or AWS Systems Manager Parameter Store and retrieve them through your application workloads running on Kubernetes. You no longer have to write custom code for your applications. Usage \u00b6 index.ts \u00b6 import * as cdk from '@aws-cdk/core' ; import { SecretProvider ClusterAddOn , EksBlueprint , ApplicationTeam } from '@aws-quickstart/ssp-amazon-eks' ; import { ISecret , Secret } from '@aws-cdk/aws-secretsmanager' ; const addOn = new SecretsStoreAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; /* Setup application team with secrets * Here we are generating a new SecretManager secret for AuthPassword * We are also looking up a pre-existing secret in Parameter Store called GITHUB_TOKEN */ export class TeamBurnham extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), teamSecrets : [ { secretProvider : new GenerateSecretManagerProvider ( 'AuthPassword), kubernetesSecret: { secretName: ' auth - password ', data: [ { key: ' password ' } ] } }, { secretProvider: new LookupSsmSecretByAttrs(' GITHUB_TOKEN ', 1), kubernetesSecret: { secretName: ' github ' } } ] }); } } class GenerateSecretManagerProvider implements SecretProvider { construct(private secretName: string) {} provide(clusterInfo: ClusterInfo): ISecret { const secret = new Secret(clusterInfo.cluster.stack, ' AuthPassword ', { secretName: this.secretName }); // create this secret first clusterInfo.cluster.node.addDependency(secret); return secret } } const app = new cdk.App(); const teams: Array<ApplicationTeam> = [ new TeamBurnham(app) ]; new EksBlueprint(app, ' my - stack - name ' , addOns , teams }); Functionality \u00b6 Installs the Kubernetes Secrets Store CSI Driver in the kube-system namespace. Installs AWS Secrets Manager and Config Provider for Secret Store CSI Driver in the kube-system namespace. Creates an IAM access policy for scoped down to just the secrets the provided namespace should have access to. Updates IAM roles for service accounts [team-name]-sa for policies to grant read access to the provided secrets. Creates a SecretProviderClass [team-name]-aws-secrets which tells the AWS provider which secrets can be mounted in an application pod in the provided namespace. Security Considerations \u00b6 The AWS Secrets Manger and Config Provider provides compatibility for legacy applications that access secrets as mounted files in the pod. Security conscious applications should use the native AWS APIs to fetch secrets and optionally cache them in memory rather than storing them in the file system. Example \u00b6 After the Blueprint stack is deployed you can test consuming the secret from within a deployment . This sample deployment shows how to consume the secrets as mounted volumes as well as environment variables. cat << EOF >> test-secrets.yaml apiVersion: apps/v1 kind: Deployment metadata: name: app-deployment labels: app: myapp namespace: team-burnham spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: serviceAccountName: burnham-sa volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: burnham-aws-secrets containers: - name: test-secrets image: public.ecr.aws/ubuntu/ubuntu:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do sleep 30; done;\" ] resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" env: - name: PASSWORD valueFrom: secretKeyRef: name: auth-password key: password - name: GITHUB_TOKEN valueFrom: secretKeyRef: name: github key: GITHUB_TOKEN volumeMounts: - name: secrets-store-inline mountPath: /mnt/secrets-store readOnly: true EOF The values for serviceAccountName and the secretProviderClass shown in the example above are obtained from CloudFormation outputs of the blueprint stack shown in the screenshot below as burnhamsa and teamburnhamsecretproviderclass . Apply the manifest. $ kubectl apply -f test-secrets.yaml deployment.apps/app-deployment created Test that kubernetes secret burnham-github-secrets was created. kubectl get secrets -n team-burnham NAME TYPE DATA AGE auth-password Opaque 1 19s burnham-sa-token-fqjqw kubernetes.io/service-account-token 3 64m default-token-7fn69 kubernetes.io/service-account-token 3 64m github Opaque 1 19s Test that the deployment has completed and the pod is running successfully. $ kubectl get pods -n team-burnham NAME READY STATUS RESTARTS AGE app-deployment-6867fc6bd6-jzdwh 1 /1 Running 0 46s Next, test whether the secret PASSWORD is available as an environment variable from within the app-deployment pod. $ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $PASSWORD XXXXXXXXXXXXXXXXXX Test whether GITHUB_TOKEN is available as an environment variable from within the app-deployment pod. $ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $GITHUB_TOKEN ghp_XXXXXXXXXXXXXXXXXXXXXXXXXX","title":"Secrets Store"},{"location":"addons/secrets-store/#secrets-store-add-on","text":"The Secrets Store Add-on provisions the AWS Secrets Manager and Config Provider(ASCP) for Secret Store CSI Driver on your EKS cluster. With ASCP, you now have a plugin for the industry-standard Kubernetes Secrets Store Container Storage Interface (CSI) Driver used for providing secrets to applications operating on Amazon Elastic Kubernetes Service. With ASCP, you can securely store and manage your secrets in AWS Secrets Manager or AWS Systems Manager Parameter Store and retrieve them through your application workloads running on Kubernetes. You no longer have to write custom code for your applications.","title":"Secrets Store Add-on"},{"location":"addons/secrets-store/#usage","text":"","title":"Usage"},{"location":"addons/secrets-store/#indexts","text":"import * as cdk from '@aws-cdk/core' ; import { SecretProvider ClusterAddOn , EksBlueprint , ApplicationTeam } from '@aws-quickstart/ssp-amazon-eks' ; import { ISecret , Secret } from '@aws-cdk/aws-secretsmanager' ; const addOn = new SecretsStoreAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; /* Setup application team with secrets * Here we are generating a new SecretManager secret for AuthPassword * We are also looking up a pre-existing secret in Parameter Store called GITHUB_TOKEN */ export class TeamBurnham extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), teamSecrets : [ { secretProvider : new GenerateSecretManagerProvider ( 'AuthPassword), kubernetesSecret: { secretName: ' auth - password ', data: [ { key: ' password ' } ] } }, { secretProvider: new LookupSsmSecretByAttrs(' GITHUB_TOKEN ', 1), kubernetesSecret: { secretName: ' github ' } } ] }); } } class GenerateSecretManagerProvider implements SecretProvider { construct(private secretName: string) {} provide(clusterInfo: ClusterInfo): ISecret { const secret = new Secret(clusterInfo.cluster.stack, ' AuthPassword ', { secretName: this.secretName }); // create this secret first clusterInfo.cluster.node.addDependency(secret); return secret } } const app = new cdk.App(); const teams: Array<ApplicationTeam> = [ new TeamBurnham(app) ]; new EksBlueprint(app, ' my - stack - name ' , addOns , teams });","title":"index.ts"},{"location":"addons/secrets-store/#functionality","text":"Installs the Kubernetes Secrets Store CSI Driver in the kube-system namespace. Installs AWS Secrets Manager and Config Provider for Secret Store CSI Driver in the kube-system namespace. Creates an IAM access policy for scoped down to just the secrets the provided namespace should have access to. Updates IAM roles for service accounts [team-name]-sa for policies to grant read access to the provided secrets. Creates a SecretProviderClass [team-name]-aws-secrets which tells the AWS provider which secrets can be mounted in an application pod in the provided namespace.","title":"Functionality"},{"location":"addons/secrets-store/#security-considerations","text":"The AWS Secrets Manger and Config Provider provides compatibility for legacy applications that access secrets as mounted files in the pod. Security conscious applications should use the native AWS APIs to fetch secrets and optionally cache them in memory rather than storing them in the file system.","title":"Security Considerations"},{"location":"addons/secrets-store/#example","text":"After the Blueprint stack is deployed you can test consuming the secret from within a deployment . This sample deployment shows how to consume the secrets as mounted volumes as well as environment variables. cat << EOF >> test-secrets.yaml apiVersion: apps/v1 kind: Deployment metadata: name: app-deployment labels: app: myapp namespace: team-burnham spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: serviceAccountName: burnham-sa volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: burnham-aws-secrets containers: - name: test-secrets image: public.ecr.aws/ubuntu/ubuntu:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do sleep 30; done;\" ] resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" env: - name: PASSWORD valueFrom: secretKeyRef: name: auth-password key: password - name: GITHUB_TOKEN valueFrom: secretKeyRef: name: github key: GITHUB_TOKEN volumeMounts: - name: secrets-store-inline mountPath: /mnt/secrets-store readOnly: true EOF The values for serviceAccountName and the secretProviderClass shown in the example above are obtained from CloudFormation outputs of the blueprint stack shown in the screenshot below as burnhamsa and teamburnhamsecretproviderclass . Apply the manifest. $ kubectl apply -f test-secrets.yaml deployment.apps/app-deployment created Test that kubernetes secret burnham-github-secrets was created. kubectl get secrets -n team-burnham NAME TYPE DATA AGE auth-password Opaque 1 19s burnham-sa-token-fqjqw kubernetes.io/service-account-token 3 64m default-token-7fn69 kubernetes.io/service-account-token 3 64m github Opaque 1 19s Test that the deployment has completed and the pod is running successfully. $ kubectl get pods -n team-burnham NAME READY STATUS RESTARTS AGE app-deployment-6867fc6bd6-jzdwh 1 /1 Running 0 46s Next, test whether the secret PASSWORD is available as an environment variable from within the app-deployment pod. $ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $PASSWORD XXXXXXXXXXXXXXXXXX Test whether GITHUB_TOKEN is available as an environment variable from within the app-deployment pod. $ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $GITHUB_TOKEN ghp_XXXXXXXXXXXXXXXXXXXXXXXXXX","title":"Example"},{"location":"addons/ssm-agent/","text":"SSM Agent Add-on \u00b6 This add-on uses the Kubernetes DaemonSet resource type to install AWS Systems Manager Agent (SSM Agent) on all worker nodes, instead of installing it manually or replacing the Amazon Machine Image (AMI) for the nodes. DaemonSet uses a CronJob on the worker node to schedule the installation of SSM Agent. A common use-case for installing SSM Agent on the worker nodes is to be able open a terminal session on an instance without the need to create a bastion instance and without having to install SSH keys on the worker nodes. The AWS Identity and Access Management (IAM) managed role AmazonSSMManagedInstanceCore provides the required permissions for SSM Agent to run on EC2 instances. This role is automatically attached to the instances when this add-on is enabled. Limitations This add-on isn't applicable to AWS Fargate, because DaemonSets aren't supported on the Fargate platform. This add-on applies only to Linux-based worker nodes. The DaemonSet pods run in privileged mode. If the Amazon EKS cluster has a webhook that blocks pods in privileged mode, the SSM Agent will not be installed. Only latest version of SSM Agent add-on can be installed. Usage \u00b6 import { SSMAgentAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new SSMAgentAddon (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); To validate that SSM Agent is running on worker node instance: Pre-Requisite : Install the Session Manager plugin for the AWS CLI as per instructions for your OS. Get the EC2 Instance Id of a worker node instance_id = $( kubectl get nodes -o custom-columns = NAME:.metadata.name,INSTANCEID:.spec.providerID | awk -F/ 'FNR == 2 {print $5}' ) Use the start-session api to see if you can open a terminal into the instance aws ssm start-session --target $instance_id Use Case: Private Clusters \u00b6 If you are disabling public access for your EKS cluster endpoint such that the cluster endpoint is provisioned as private only i.e endpointPublicAccess=false and endpointPrivateAccess=true , then you can use one of the worker nodes as a TCP jump box to your EKS cluster api. To set up a TCP tunnel with your worker node as a jump box: Use SSM send-command api to create a TCP tunnel to Cluster API using socat : # Get the Cluster API endpoint first CLUSTER_NAME = <insert your cluster name, e.g. blueprint-construct-dev> CLUSTER_API = $( aws eks describe-cluster --name $CLUSTER_NAME | jq -r '.cluster.endpoint' | awk -F/ '{print $3}' ) aws ssm send-command \\ --instance-ids $instance_id \\ --document-name \"AWS-RunShellScript\" \\ --comment \"tcp tunnel to cluster api\" \\ --parameters commands = \"nohup sudo socat TCP-LISTEN:443\\,fork TCP: $CLUSTER_API :443 &\" Update ~/.kube/config to use port 8443 instead of 443 as your local host may not allow you to bind port 443 (depending on your machine network configuration you may not be able to bind to port 443. In such a case, you can bind to port 8443) sed -i -e \"s/https:\\/\\/ $CLUSTER_API /https:\\/\\/ $CLUSTER_API :8443/\" ~/.kube/config Update /etc/hosts so that $CLUSTER_API resolves to 127.0.0.1 . sudo echo \"127.0.0.1 $CLUSTER_API \" >> /etc/hosts 4. Start an SSM session to forward remote port 443 to local port 8443: aws ssm start-session \\ --target $instance_id \\ --document-name AWS-StartPortForwardingSession \\ --parameters '{\"portNumber\":[\"443\"], \"localPortNumber\":[\"8443\"]}' At this point you should be able to execute kubectl ... commands against your cluster API from another terminal window. Limitations This approach cannot be used for Fargate or BottleRocket based providers. socat is available on EKS optimized AMI out of the box but may have to be explicitly installed on others AMIs.","title":"SSM Agent"},{"location":"addons/ssm-agent/#ssm-agent-add-on","text":"This add-on uses the Kubernetes DaemonSet resource type to install AWS Systems Manager Agent (SSM Agent) on all worker nodes, instead of installing it manually or replacing the Amazon Machine Image (AMI) for the nodes. DaemonSet uses a CronJob on the worker node to schedule the installation of SSM Agent. A common use-case for installing SSM Agent on the worker nodes is to be able open a terminal session on an instance without the need to create a bastion instance and without having to install SSH keys on the worker nodes. The AWS Identity and Access Management (IAM) managed role AmazonSSMManagedInstanceCore provides the required permissions for SSM Agent to run on EC2 instances. This role is automatically attached to the instances when this add-on is enabled. Limitations This add-on isn't applicable to AWS Fargate, because DaemonSets aren't supported on the Fargate platform. This add-on applies only to Linux-based worker nodes. The DaemonSet pods run in privileged mode. If the Amazon EKS cluster has a webhook that blocks pods in privileged mode, the SSM Agent will not be installed. Only latest version of SSM Agent add-on can be installed.","title":"SSM Agent Add-on"},{"location":"addons/ssm-agent/#usage","text":"import { SSMAgentAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new SSMAgentAddon (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); To validate that SSM Agent is running on worker node instance: Pre-Requisite : Install the Session Manager plugin for the AWS CLI as per instructions for your OS. Get the EC2 Instance Id of a worker node instance_id = $( kubectl get nodes -o custom-columns = NAME:.metadata.name,INSTANCEID:.spec.providerID | awk -F/ 'FNR == 2 {print $5}' ) Use the start-session api to see if you can open a terminal into the instance aws ssm start-session --target $instance_id","title":"Usage"},{"location":"addons/ssm-agent/#use-case-private-clusters","text":"If you are disabling public access for your EKS cluster endpoint such that the cluster endpoint is provisioned as private only i.e endpointPublicAccess=false and endpointPrivateAccess=true , then you can use one of the worker nodes as a TCP jump box to your EKS cluster api. To set up a TCP tunnel with your worker node as a jump box: Use SSM send-command api to create a TCP tunnel to Cluster API using socat : # Get the Cluster API endpoint first CLUSTER_NAME = <insert your cluster name, e.g. blueprint-construct-dev> CLUSTER_API = $( aws eks describe-cluster --name $CLUSTER_NAME | jq -r '.cluster.endpoint' | awk -F/ '{print $3}' ) aws ssm send-command \\ --instance-ids $instance_id \\ --document-name \"AWS-RunShellScript\" \\ --comment \"tcp tunnel to cluster api\" \\ --parameters commands = \"nohup sudo socat TCP-LISTEN:443\\,fork TCP: $CLUSTER_API :443 &\" Update ~/.kube/config to use port 8443 instead of 443 as your local host may not allow you to bind port 443 (depending on your machine network configuration you may not be able to bind to port 443. In such a case, you can bind to port 8443) sed -i -e \"s/https:\\/\\/ $CLUSTER_API /https:\\/\\/ $CLUSTER_API :8443/\" ~/.kube/config Update /etc/hosts so that $CLUSTER_API resolves to 127.0.0.1 . sudo echo \"127.0.0.1 $CLUSTER_API \" >> /etc/hosts 4. Start an SSM session to forward remote port 443 to local port 8443: aws ssm start-session \\ --target $instance_id \\ --document-name AWS-StartPortForwardingSession \\ --parameters '{\"portNumber\":[\"443\"], \"localPortNumber\":[\"8443\"]}' At this point you should be able to execute kubectl ... commands against your cluster API from another terminal window. Limitations This approach cannot be used for Fargate or BottleRocket based providers. socat is available on EKS optimized AMI out of the box but may have to be explicitly installed on others AMIs.","title":"Use Case: Private Clusters"},{"location":"addons/velero/","text":"Velero Add-On \u00b6 The Velero(formerly Heptio Ark) is a tool to backup and restore your Kubernetes cluster resources and persistent volumes. Velero lets you : Take backups of your cluster and restore in case of loss. Migrate cluster resources to other clusters. Replicate your production cluster to development and testing clusters. Velero consists of: A server that runs on your cluster A command-line client that runs locally The Velero add-on installs Velero on Amazon EKS. By default it will create a private encrypted S3 Bucket to be the Velero backup destination. It leverages IAM Role for Service Accounts (IRSA) feature to enable Velero pod to make API calls with S3 and EC2 natively without the need to use kube2iam or AWS credentials. Usage \u00b6 import * as cdk from '@aws-cdk/core' ; import * as ssp from '../lib' ; const app = new cdk . App (); const addOns : Array < ssp . ClusterAddOn > = [ new ssp . addons . VeleroAddOn (), ]; new ssp . EksBlueprint ( app , { id : 'my-stack-name' , addOns , }, { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , } }); Functionality \u00b6 By default create a private S3 bucket (blocking all public access) with SSE-KMS encryption with AWS Managed Key from KMS(Encryption At Rest) for the Velero Backup destination. Configure S3 Bucket policy to enable encryption in transit. Create the IAM Role for Service Account for Velero pod to make API calls to AWS S3 and EC2 to backup and restore. Preset Velero Helm Chart Values . Allow users to pass Velero Helm Chart Values for customization purposes. Supports standard helm configuration options . Limitations \u00b6 Velero has a known bug for support of S3 with SSE-KMS encryption with Customer master key (CMK). Please refer to Velero GitHub Issue #83 . As a result of #1, Velero is unable to leverage the S3 Bucket Key feature which requires using AWS CMK to achieve \"reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS.\" Testing Velero Functionality \u00b6 The following steps will help test the backup and restore Kuberenetes resources from Velero Deploy a sample app as deployment into a newly created namespace Backup the sample app from the namespace Delete the sample app namespace Restore the sample app Deploy a sample app into a newly created namespace \u00b6 Properly configure your kubeconfig to use kubectl command. By successfully deploying the EKS cluster, you can run commands to set up your kubeconfig correctly by: aws eks update-kubeconfig --name <EKS_Cluster_Name> --region <AWS_Region> --role-arn arn:aws:iam::<AWS_ACCOUNT_ID>:role/<IRSA_Role_Name> # Create the test01 namespace $ kubectl create ns test01 namespace/test01 created # Deploy the Nginx Stateless applications on to namespace test01 $ kubectl apply -f https://k8s.io/examples/application/deployment.yaml -n test01 deployment.apps/nginx-deployment created # Check the nginx pods $ kubectl get pods -n test01 NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-qf7lc 1 /1 Running 0 53s nginx-deployment-66b6c48dd5-wvxjx 1 /1 Running 0 53s # Deploy the Stateful Nginx Application with PV to namespace nginx-example $ kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/velero/main/examples/nginx-app/with-pv.yaml namespace/nginx-example created persistentvolumeclaim/nginx-logs created deployment.apps/nginx-deployment created service/my-nginx created # Check the application and PV $ kubectl get pods -n nginx-example NAME READY STATUS RESTARTS AGE nginx-deployment-66689547d-4mqsd 2 /2 Running 0 106s haofeif@a483e70791e6 ~ $ kubectl get pv -n nginx-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-73498192-6571-4e31-b455-e7e7efbf2fb7 1Gi RWO Delete Bound nginx-example/nginx-logs gp2 110s ## EBS Volume got created Backup the sample app from the namespace \u00b6 To install Velero client Cli, please refer to the User Guide The backup will be created into the S3 bucket created or specified by the users. # Create the backup for stateless app at namespace test01 $ velero backup create test01 --include-namespaces test01 Backup request \"test01\" submitted successfully. Run ` velero backup describe test01 ` or ` velero backup logs test01 ` for more details. # Check for the backup $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR test01 Completed 0 0 2021 -09-20 12 :40:37 +1000 AEST 29d default <none> # Check the logs of the backup $ velero backup logs test01 time = \"2021-09-19T02:40:37Z\" level = info msg = \"Setting up backup temp file\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:556\" time = \"2021-09-19T02:40:37Z\" level = info msg = \"Setting up plugin manager\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:563\" time = \"2021-09-19T02:40:37Z\" level = info msg = \"Getting backup item actions\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:567\" ... # Check the backup location, the Access mode shows the S3 bucket name and its folders. $ velero backup-location get NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws my-stack-name-mystacknamevelerobackupxxx/velero/my-stack-name Available 2021 -09-20 12 :29:35 +1000 AEST ReadWrite true # Screenshot of the S3 bucket folder for the backup test01 # Create the Backup with the PV at namespace nginx-example $ velero backup create nginx-backup --include-namespaces nginx-example Backup request \"nginx-backup\" submitted successfully. Run ` velero backup describe nginx-backup ` or ` velero backup logs nginx-backup ` for more details. ## Check the backup status $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR nginx-backup Completed 0 0 2021 -09-20 12 :37:36 +1000 AEST 29d default <none> # Screenshot of the S3 bucket folder for the backup nginx-backup (with PV) Delete the sample app namespace \u00b6 # Delete the namespace test01 $ kubectl delete ns test01 # Delete the namespace nginx-example $ kubectl delete ns nginx-example # Note: Because the default reclaim policy for dynamically-provisioned PVs is \u201cDelete\u201d, these commands should trigger AWS to delete the EBS Volume that backs the PV. Deletion is asynchronous, so this may take some time. Restore the sample app \u00b6 # Restore from the backup of test01 $ velero restore create test01 --from-backup test01 Restore request \"test01\" submitted successfully. Run ` velero restore describe test01 ` or ` velero restore logs test01 ` for more details. # Check the restore status of test01 $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR test01 test01 Completed 2021 -09-20 12 :41:38 +1000 AEST <nil> 0 0 2021 -09-20 12 :41:36 +1000 AEST <none> # Check the stateless application restore completed $ kubectl get pods -n test01 NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-qf7lc 1 /1 Running 0 53s nginx-deployment-66b6c48dd5-wvxjx 1 /1 Running 0 53s # Restore from the backup of nginx-backup (With PV) after confirming EBS volume has been deleted successfully $ velero restore create --from-backup nginx-backup # Check the Restore status $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR nginx-backup-20210920124336 nginx-backup Completed 2021 -09-20 12 :43:42 +1000 AEST 2021 -09-20 12 :43:45 +1000 AEST 0 0 2021 -09-20 12 :43:40 +1000 AEST <none> # Check the status of pod and PV in namespace nginx-example $ kubectl get pods -n nginx-example NAME READY STATUS RESTARTS AGE nginx-deployment-66689547d-4mqsd 2 /2 Running 0 2m12s $ kubectl get pv -n nginx-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-73498192-6571-4e31-b455-e7e7efbf2fb7 1Gi RWO Delete Bound nginx-example/nginx-logs gp2 2m22s # EBS Volume is back","title":"Velero Add-On"},{"location":"addons/velero/#velero-add-on","text":"The Velero(formerly Heptio Ark) is a tool to backup and restore your Kubernetes cluster resources and persistent volumes. Velero lets you : Take backups of your cluster and restore in case of loss. Migrate cluster resources to other clusters. Replicate your production cluster to development and testing clusters. Velero consists of: A server that runs on your cluster A command-line client that runs locally The Velero add-on installs Velero on Amazon EKS. By default it will create a private encrypted S3 Bucket to be the Velero backup destination. It leverages IAM Role for Service Accounts (IRSA) feature to enable Velero pod to make API calls with S3 and EC2 natively without the need to use kube2iam or AWS credentials.","title":"Velero Add-On"},{"location":"addons/velero/#usage","text":"import * as cdk from '@aws-cdk/core' ; import * as ssp from '../lib' ; const app = new cdk . App (); const addOns : Array < ssp . ClusterAddOn > = [ new ssp . addons . VeleroAddOn (), ]; new ssp . EksBlueprint ( app , { id : 'my-stack-name' , addOns , }, { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , } });","title":"Usage"},{"location":"addons/velero/#functionality","text":"By default create a private S3 bucket (blocking all public access) with SSE-KMS encryption with AWS Managed Key from KMS(Encryption At Rest) for the Velero Backup destination. Configure S3 Bucket policy to enable encryption in transit. Create the IAM Role for Service Account for Velero pod to make API calls to AWS S3 and EC2 to backup and restore. Preset Velero Helm Chart Values . Allow users to pass Velero Helm Chart Values for customization purposes. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/velero/#limitations","text":"Velero has a known bug for support of S3 with SSE-KMS encryption with Customer master key (CMK). Please refer to Velero GitHub Issue #83 . As a result of #1, Velero is unable to leverage the S3 Bucket Key feature which requires using AWS CMK to achieve \"reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS.\"","title":"Limitations"},{"location":"addons/velero/#testing-velero-functionality","text":"The following steps will help test the backup and restore Kuberenetes resources from Velero Deploy a sample app as deployment into a newly created namespace Backup the sample app from the namespace Delete the sample app namespace Restore the sample app","title":"Testing Velero Functionality"},{"location":"addons/velero/#deploy-a-sample-app-into-a-newly-created-namespace","text":"Properly configure your kubeconfig to use kubectl command. By successfully deploying the EKS cluster, you can run commands to set up your kubeconfig correctly by: aws eks update-kubeconfig --name <EKS_Cluster_Name> --region <AWS_Region> --role-arn arn:aws:iam::<AWS_ACCOUNT_ID>:role/<IRSA_Role_Name> # Create the test01 namespace $ kubectl create ns test01 namespace/test01 created # Deploy the Nginx Stateless applications on to namespace test01 $ kubectl apply -f https://k8s.io/examples/application/deployment.yaml -n test01 deployment.apps/nginx-deployment created # Check the nginx pods $ kubectl get pods -n test01 NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-qf7lc 1 /1 Running 0 53s nginx-deployment-66b6c48dd5-wvxjx 1 /1 Running 0 53s # Deploy the Stateful Nginx Application with PV to namespace nginx-example $ kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/velero/main/examples/nginx-app/with-pv.yaml namespace/nginx-example created persistentvolumeclaim/nginx-logs created deployment.apps/nginx-deployment created service/my-nginx created # Check the application and PV $ kubectl get pods -n nginx-example NAME READY STATUS RESTARTS AGE nginx-deployment-66689547d-4mqsd 2 /2 Running 0 106s haofeif@a483e70791e6 ~ $ kubectl get pv -n nginx-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-73498192-6571-4e31-b455-e7e7efbf2fb7 1Gi RWO Delete Bound nginx-example/nginx-logs gp2 110s ## EBS Volume got created","title":"Deploy a sample app into a newly created namespace"},{"location":"addons/velero/#backup-the-sample-app-from-the-namespace","text":"To install Velero client Cli, please refer to the User Guide The backup will be created into the S3 bucket created or specified by the users. # Create the backup for stateless app at namespace test01 $ velero backup create test01 --include-namespaces test01 Backup request \"test01\" submitted successfully. Run ` velero backup describe test01 ` or ` velero backup logs test01 ` for more details. # Check for the backup $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR test01 Completed 0 0 2021 -09-20 12 :40:37 +1000 AEST 29d default <none> # Check the logs of the backup $ velero backup logs test01 time = \"2021-09-19T02:40:37Z\" level = info msg = \"Setting up backup temp file\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:556\" time = \"2021-09-19T02:40:37Z\" level = info msg = \"Setting up plugin manager\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:563\" time = \"2021-09-19T02:40:37Z\" level = info msg = \"Getting backup item actions\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:567\" ... # Check the backup location, the Access mode shows the S3 bucket name and its folders. $ velero backup-location get NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws my-stack-name-mystacknamevelerobackupxxx/velero/my-stack-name Available 2021 -09-20 12 :29:35 +1000 AEST ReadWrite true # Screenshot of the S3 bucket folder for the backup test01 # Create the Backup with the PV at namespace nginx-example $ velero backup create nginx-backup --include-namespaces nginx-example Backup request \"nginx-backup\" submitted successfully. Run ` velero backup describe nginx-backup ` or ` velero backup logs nginx-backup ` for more details. ## Check the backup status $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR nginx-backup Completed 0 0 2021 -09-20 12 :37:36 +1000 AEST 29d default <none> # Screenshot of the S3 bucket folder for the backup nginx-backup (with PV)","title":"Backup the sample app from the namespace"},{"location":"addons/velero/#delete-the-sample-app-namespace","text":"# Delete the namespace test01 $ kubectl delete ns test01 # Delete the namespace nginx-example $ kubectl delete ns nginx-example # Note: Because the default reclaim policy for dynamically-provisioned PVs is \u201cDelete\u201d, these commands should trigger AWS to delete the EBS Volume that backs the PV. Deletion is asynchronous, so this may take some time.","title":"Delete the sample app namespace"},{"location":"addons/velero/#restore-the-sample-app","text":"# Restore from the backup of test01 $ velero restore create test01 --from-backup test01 Restore request \"test01\" submitted successfully. Run ` velero restore describe test01 ` or ` velero restore logs test01 ` for more details. # Check the restore status of test01 $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR test01 test01 Completed 2021 -09-20 12 :41:38 +1000 AEST <nil> 0 0 2021 -09-20 12 :41:36 +1000 AEST <none> # Check the stateless application restore completed $ kubectl get pods -n test01 NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-qf7lc 1 /1 Running 0 53s nginx-deployment-66b6c48dd5-wvxjx 1 /1 Running 0 53s # Restore from the backup of nginx-backup (With PV) after confirming EBS volume has been deleted successfully $ velero restore create --from-backup nginx-backup # Check the Restore status $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR nginx-backup-20210920124336 nginx-backup Completed 2021 -09-20 12 :43:42 +1000 AEST 2021 -09-20 12 :43:45 +1000 AEST 0 0 2021 -09-20 12 :43:40 +1000 AEST <none> # Check the status of pod and PV in namespace nginx-example $ kubectl get pods -n nginx-example NAME READY STATUS RESTARTS AGE nginx-deployment-66689547d-4mqsd 2 /2 Running 0 2m12s $ kubectl get pv -n nginx-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-73498192-6571-4e31-b455-e7e7efbf2fb7 1Gi RWO Delete Bound nginx-example/nginx-logs gp2 2m22s # EBS Volume is back","title":"Restore the sample app"},{"location":"addons/vpc-cni/","text":"VPC CNI Amazon EKS Add-on \u00b6 The VPC CNI Amazon EKS Add-on adds support for Amazon VPC Container Network Interface (CNI) plugin. Amazon EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. For more information, see Pod networking (CNI) . Installing VPC CNI as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs VPC CNI as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons. Prerequisite \u00b6 Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later. Usage \u00b6 import * as ssp from '@shapirov/cdk-eks-blueprint' ; readonly vpcCni = new ssp . addons . VpcCniAddOn ( \"v1.7.5-eksbuild.2\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ vpcCni ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Configuration Options \u00b6 version : Pass in the vpc-cni plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.19, below command shows versions of the vpc-cni add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name vpc-cni \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.9.0-eksbuild.1 False v1.8.0-eksbuild.1 False ... ... v1.7.5-eksbuild.2 True ... ... v1.7.5-eksbuild.1 False v1.6.3-eksbuild.2 False Validation \u00b6 To validate that vpc-cni add-on is running, ensure that the pod is in Running state. $ kubectl get pods -n kube-system|grep aws-node NAME READY STATUS RESTARTS AGE aws-node-xzk5n 1/1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of vpc-cni add-on installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name vpc-cni \\ --query \"addon.addonVersion\" \\ --output text # Output v1.7.5-eksbuild.2 Functionality \u00b6 Applies VPC CNI add-on to Amazon EKS cluster.","title":"Vpc Cni"},{"location":"addons/vpc-cni/#vpc-cni-amazon-eks-add-on","text":"The VPC CNI Amazon EKS Add-on adds support for Amazon VPC Container Network Interface (CNI) plugin. Amazon EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. For more information, see Pod networking (CNI) . Installing VPC CNI as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs VPC CNI as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons.","title":"VPC CNI Amazon EKS Add-on"},{"location":"addons/vpc-cni/#prerequisite","text":"Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.","title":"Prerequisite"},{"location":"addons/vpc-cni/#usage","text":"import * as ssp from '@shapirov/cdk-eks-blueprint' ; readonly vpcCni = new ssp . addons . VpcCniAddOn ( \"v1.7.5-eksbuild.2\" ); // optionally specify image version to pull or empty constructor const addOns : Array < ClusterAddOn > = [ vpcCni ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, });","title":"Usage"},{"location":"addons/vpc-cni/#configuration-options","text":"version : Pass in the vpc-cni plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.19, below command shows versions of the vpc-cni add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name vpc-cni \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.9.0-eksbuild.1 False v1.8.0-eksbuild.1 False ... ... v1.7.5-eksbuild.2 True ... ... v1.7.5-eksbuild.1 False v1.6.3-eksbuild.2 False","title":"Configuration Options"},{"location":"addons/vpc-cni/#validation","text":"To validate that vpc-cni add-on is running, ensure that the pod is in Running state. $ kubectl get pods -n kube-system|grep aws-node NAME READY STATUS RESTARTS AGE aws-node-xzk5n 1/1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of vpc-cni add-on installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name vpc-cni \\ --query \"addon.addonVersion\" \\ --output text # Output v1.7.5-eksbuild.2","title":"Validation"},{"location":"addons/vpc-cni/#functionality","text":"Applies VPC CNI add-on to Amazon EKS cluster.","title":"Functionality"},{"location":"addons/xray/","text":"AWS X-Ray Add-on \u00b6 AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. The X-Ray addon provisions X-Ray daemon into an EKS cluster. This daemon exposes an internal endpoint xray-service.xray-system.svc.cluster.local:2000 that could be leveraged to aggregate and post traces to the AWS X-Ray service. For instructions on getting started with X-Ray on EKS refer to the EKS Workshop X-Ray Section . Usage \u00b6 import { XrayAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new XrayAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Once deployed, it allows applications to be instrumented with X-Ray by leveraging the X-Ray SDK. Examples of such integration can be found on GitHub . Functionality \u00b6 Creates the xray-system namespace. Deploys the xray-daemon manifests into the cluster. Configures Kubernetes service account with IRSA ( AWSXRayDaemonWriteAccess ) for communication between the cluster and the AWS X-Ray service","title":"AWS XRay"},{"location":"addons/xray/#aws-x-ray-add-on","text":"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. The X-Ray addon provisions X-Ray daemon into an EKS cluster. This daemon exposes an internal endpoint xray-service.xray-system.svc.cluster.local:2000 that could be leveraged to aggregate and post traces to the AWS X-Ray service. For instructions on getting started with X-Ray on EKS refer to the EKS Workshop X-Ray Section .","title":"AWS X-Ray Add-on"},{"location":"addons/xray/#usage","text":"import { XrayAddOn , ClusterAddOn , EksBlueprint } from '@aws-quickstart/ssp-amazon-eks' ; const addOn = new XrayAddOn (); const addOns : Array < ClusterAddOn > = [ addOn ]; const app = new cdk . App (); new EksBlueprint ( app , 'my-stack-name' , addOns , [], { env : { account : < AWS_ACCOUNT_ID > , region : < AWS_REGION > , }, }); Once deployed, it allows applications to be instrumented with X-Ray by leveraging the X-Ray SDK. Examples of such integration can be found on GitHub .","title":"Usage"},{"location":"addons/xray/#functionality","text":"Creates the xray-system namespace. Deploys the xray-daemon manifests into the cluster. Configures Kubernetes service account with IRSA ( AWSXRayDaemonWriteAccess ) for communication between the cluster and the AWS X-Ray service","title":"Functionality"},{"location":"cluster-providers/","text":"Cluster Providers \u00b6 The ssp-amazon-eks framework allows customers to easily configure the underlying EKS clusters that it provisions. This is done via Cluster Providers. Customers can leverage the Cluster Providers that the framework supports, or supply their own. The framework currently provides support for the following Cluster Providers: Cluster Provider Description AsgClusterProvider Provisions an EKS cluster with an Auto Scaling group used for compute capacity. MngClusterProvider Provisions an EKS cluster with a Managed Node group for compute capacity. FargateClusterProviders Provisions an EKS cluster which leverages AWS Fargate to run Kubernetes pods. By default, the framework will leverage the MngClusterProvider .","title":"Overview"},{"location":"cluster-providers/#cluster-providers","text":"The ssp-amazon-eks framework allows customers to easily configure the underlying EKS clusters that it provisions. This is done via Cluster Providers. Customers can leverage the Cluster Providers that the framework supports, or supply their own. The framework currently provides support for the following Cluster Providers: Cluster Provider Description AsgClusterProvider Provisions an EKS cluster with an Auto Scaling group used for compute capacity. MngClusterProvider Provisions an EKS cluster with a Managed Node group for compute capacity. FargateClusterProviders Provisions an EKS cluster which leverages AWS Fargate to run Kubernetes pods. By default, the framework will leverage the MngClusterProvider .","title":"Cluster Providers"},{"location":"cluster-providers/asg-cluster-provider/","text":"Auto Scaling Group Cluster Provider \u00b6 The AsgClusterProvider allows you to provision an EKS cluster which leverages EC2 Auto Scaling groups (ASGs) for compute capacity. An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. Usage \u00b6 const props : AsgClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceType : new InstanceType ( 'm5.large' ), machineImageType : eks.MachineImageType.AMAZON_LINUX_2 , updatePolicy : UpdatePolicy.Rolling } const clusterProvider = new ssp . AsgClusterProvider ( props ); new ssp . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider }); Configuration \u00b6 AsgClusterProvider supports the following configuration options. Prop Description name The name for the cluster. minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default min-size ). instanceType Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") machineImageType Machine Image Type for the Autoscaling Group. updatePolicy Update policy for the Autoscaling Group. vpcSubnets The subnets for the cluster. privateCluster Public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass AsgProviderClusterProps to each cluster blueprint. Bottlerocket ASG \u00b6 Bottlerocket is a Linux-based open-source operating system that is purpose-built by Amazon Web Services for running containers. Customers can leverage the AsgClusterProvider to provision EKS clusters with Bottlerocket nodes. To do so, set the machineImageType property to eks.MachineImageType.BOTTLEROCKET . const props : AsgClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceType : new InstanceType ( 'm5.large' ), machineImageType : eks.MachineImageType.BOTTLEROCKET , updatePolicy : UpdatePolicy.Rolling } const clusterProvider = new ssp . AsgClusterProvider ( props ); new ssp . EksBlueprint ( scope , { id : 'blueprint' , teams , addOns , clusterProvider });","title":"ASG Cluster Provider"},{"location":"cluster-providers/asg-cluster-provider/#auto-scaling-group-cluster-provider","text":"The AsgClusterProvider allows you to provision an EKS cluster which leverages EC2 Auto Scaling groups (ASGs) for compute capacity. An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.","title":"Auto Scaling Group Cluster Provider"},{"location":"cluster-providers/asg-cluster-provider/#usage","text":"const props : AsgClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceType : new InstanceType ( 'm5.large' ), machineImageType : eks.MachineImageType.AMAZON_LINUX_2 , updatePolicy : UpdatePolicy.Rolling } const clusterProvider = new ssp . AsgClusterProvider ( props ); new ssp . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider });","title":"Usage"},{"location":"cluster-providers/asg-cluster-provider/#configuration","text":"AsgClusterProvider supports the following configuration options. Prop Description name The name for the cluster. minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default min-size ). instanceType Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") machineImageType Machine Image Type for the Autoscaling Group. updatePolicy Update policy for the Autoscaling Group. vpcSubnets The subnets for the cluster. privateCluster Public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass AsgProviderClusterProps to each cluster blueprint.","title":"Configuration"},{"location":"cluster-providers/asg-cluster-provider/#bottlerocket-asg","text":"Bottlerocket is a Linux-based open-source operating system that is purpose-built by Amazon Web Services for running containers. Customers can leverage the AsgClusterProvider to provision EKS clusters with Bottlerocket nodes. To do so, set the machineImageType property to eks.MachineImageType.BOTTLEROCKET . const props : AsgClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceType : new InstanceType ( 'm5.large' ), machineImageType : eks.MachineImageType.BOTTLEROCKET , updatePolicy : UpdatePolicy.Rolling } const clusterProvider = new ssp . AsgClusterProvider ( props ); new ssp . EksBlueprint ( scope , { id : 'blueprint' , teams , addOns , clusterProvider });","title":"Bottlerocket ASG"},{"location":"cluster-providers/fargate-cluster-provider/","text":"Fargate Cluster Provider \u00b6 The FargateClusterProvider allows you to provision an EKS cluster which runs Kubernetes pods on AWS Fargate . To create a Fargate cluster, you must provide Fargate Profiles , which allows cluster operators to specify which Pods should be run on Fargate. Usage \u00b6 In the example below, the Fargate profile indicates that all Pods in the dynatrace namespace should run on Fargate. const fargateProfiles : Map < string , eks . FargateProfileOptions > = new Map ([ [ \"dynatrace\" , { selectors : [{ namespace : \"dynatrace\" }] }] ]); const clusterProvider = new ssp . FargateClusterProvider ({ fargateProfiles }); new ssp . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider }); Configuration \u00b6 FargateClusterProvider supports the following configuration options. Prop Description name The name for the cluster. fargateProfiles A map of Fargate profiles to use with the cluster. vpcSubnets The subnets for the cluster. privateCluster Public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations","title":"Fargate Cluster Provider"},{"location":"cluster-providers/fargate-cluster-provider/#fargate-cluster-provider","text":"The FargateClusterProvider allows you to provision an EKS cluster which runs Kubernetes pods on AWS Fargate . To create a Fargate cluster, you must provide Fargate Profiles , which allows cluster operators to specify which Pods should be run on Fargate.","title":"Fargate Cluster Provider"},{"location":"cluster-providers/fargate-cluster-provider/#usage","text":"In the example below, the Fargate profile indicates that all Pods in the dynatrace namespace should run on Fargate. const fargateProfiles : Map < string , eks . FargateProfileOptions > = new Map ([ [ \"dynatrace\" , { selectors : [{ namespace : \"dynatrace\" }] }] ]); const clusterProvider = new ssp . FargateClusterProvider ({ fargateProfiles }); new ssp . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider });","title":"Usage"},{"location":"cluster-providers/fargate-cluster-provider/#configuration","text":"FargateClusterProvider supports the following configuration options. Prop Description name The name for the cluster. fargateProfiles A map of Fargate profiles to use with the cluster. vpcSubnets The subnets for the cluster. privateCluster Public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations","title":"Configuration"},{"location":"cluster-providers/mng-cluster-provider/","text":"Managed Node Group Cluster Provider \u00b6 The MngClusterProvider allows you to provision an EKS cluster which leverages EKS managed node groups (MNGs) for compute capacity. MNGs automate the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes clusters. Usage \u00b6 const props : MngClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceTypes : [ new InstanceType ( 'm5.large' )], amiType : NodegroupAmiType.AL2_X86_64 , nodeGroupCapacityType : CapacityType.ON_DEMAND , version : KubernetesVersion.V1_20 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } const clusterProvider = new ssp . MngClusterProvider ( props ); new ssp . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider }); Configuration \u00b6 THe EC2ClusterProvider supports the following configuration options. Prop Description name The name for the cluster. minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default min-size ). instanceTypes Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") amiType The AMI type for the managed node group. amiReleaseVersion The AMI Kuberenetes release version for the node group. customAmi The custom AMI and the userData for the node group, amiType and amiReleaseVersion will be ignored if this is set. nodeGroupCapacityType The capacity type for the node group (on demand or spot). vpcSubnets The subnets for the cluster. privateCluster public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass MngProviderClusterProps to each cluster blueprint. Upgrading Worker Nodes \u00b6 Upgrading Kubernetes versions via cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, you must also update the amiReleaseVersion property. The following demonstrates how to do so. const props : MngClusterProviderProps = { version : KubernetesVersion.V1_20 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note: consult the official EKS documentation for information ion the AMI release version that matches Kubernetes versions. Creating Clusters with Spot Capacity Type \u00b6 To create clusters which leverage Spot capacity, set the nodeGroupCapacityType value to CapacityType.SPOT const props : MngClusterProviderProps = { nodeGroupCapacityType : CapacityType.SPOT , version : KubernetesVersion.V1_20 , instanceTypes : [ new InstanceType ( 't3.large' ), new InstanceType ( 'm5.large' )], amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note that two attributes in this configuration are relevant for Spot: nodeGroupCapacityType and instaceTypes . The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability. Creating Clusters with custom AMI for the node group \u00b6 To create clusters using custom AMI for the worker nodes, set the customAmi to your custom image and provide your userData for node bootstrapping. const userData = UserData . forLinux (); userData . addCommands ( `/etc/eks/bootstrap.sh ${ cluster . clusterName } ` ); const props : MngClusterProviderProps = { nodeGroupCapacityType : CapacityType.ON_DEMAND , version : KubernetesVersion.V1_20 , instanceTypes : [ new InstanceType ( 't3.large' )], customAmi : { machineImage : MachineImage.genericLinux ({ 'us-east-1' : 'ami-0be34337b485b2609' }), userData : userData , }, }","title":"MNG Cluster Provider"},{"location":"cluster-providers/mng-cluster-provider/#managed-node-group-cluster-provider","text":"The MngClusterProvider allows you to provision an EKS cluster which leverages EKS managed node groups (MNGs) for compute capacity. MNGs automate the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes clusters.","title":"Managed Node Group Cluster Provider"},{"location":"cluster-providers/mng-cluster-provider/#usage","text":"const props : MngClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceTypes : [ new InstanceType ( 'm5.large' )], amiType : NodegroupAmiType.AL2_X86_64 , nodeGroupCapacityType : CapacityType.ON_DEMAND , version : KubernetesVersion.V1_20 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } const clusterProvider = new ssp . MngClusterProvider ( props ); new ssp . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider });","title":"Usage"},{"location":"cluster-providers/mng-cluster-provider/#configuration","text":"THe EC2ClusterProvider supports the following configuration options. Prop Description name The name for the cluster. minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default min-size ). instanceTypes Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") amiType The AMI type for the managed node group. amiReleaseVersion The AMI Kuberenetes release version for the node group. customAmi The custom AMI and the userData for the node group, amiType and amiReleaseVersion will be ignored if this is set. nodeGroupCapacityType The capacity type for the node group (on demand or spot). vpcSubnets The subnets for the cluster. privateCluster public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass MngProviderClusterProps to each cluster blueprint.","title":"Configuration"},{"location":"cluster-providers/mng-cluster-provider/#upgrading-worker-nodes","text":"Upgrading Kubernetes versions via cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, you must also update the amiReleaseVersion property. The following demonstrates how to do so. const props : MngClusterProviderProps = { version : KubernetesVersion.V1_20 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note: consult the official EKS documentation for information ion the AMI release version that matches Kubernetes versions.","title":"Upgrading Worker Nodes"},{"location":"cluster-providers/mng-cluster-provider/#creating-clusters-with-spot-capacity-type","text":"To create clusters which leverage Spot capacity, set the nodeGroupCapacityType value to CapacityType.SPOT const props : MngClusterProviderProps = { nodeGroupCapacityType : CapacityType.SPOT , version : KubernetesVersion.V1_20 , instanceTypes : [ new InstanceType ( 't3.large' ), new InstanceType ( 'm5.large' )], amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note that two attributes in this configuration are relevant for Spot: nodeGroupCapacityType and instaceTypes . The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability.","title":"Creating Clusters with Spot Capacity Type"},{"location":"cluster-providers/mng-cluster-provider/#creating-clusters-with-custom-ami-for-the-node-group","text":"To create clusters using custom AMI for the worker nodes, set the customAmi to your custom image and provide your userData for node bootstrapping. const userData = UserData . forLinux (); userData . addCommands ( `/etc/eks/bootstrap.sh ${ cluster . clusterName } ` ); const props : MngClusterProviderProps = { nodeGroupCapacityType : CapacityType.ON_DEMAND , version : KubernetesVersion.V1_20 , instanceTypes : [ new InstanceType ( 't3.large' )], customAmi : { machineImage : MachineImage.genericLinux ({ 'us-east-1' : 'ami-0be34337b485b2609' }), userData : userData , }, }","title":"Creating Clusters with custom AMI for the node group"},{"location":"internal/ci/","text":"CodeBuild CI \u00b6 This example shows how to enable a CodeBuild based Continuous Integration process for the SSP blueprint. The CodeBuild project is provisioned using a CDK application. The buildspec.yml provided deploys the sample blueprint stacks provided in examples . The buildspec can be used directly if you wish to setup the CodeBuild project manually through the console or via the CLI. Optionally, you can also provide an S3 bucket location for a cdk.context.json that contains key-values for any context you want to provide to your application such as route 53 domain, domain account, subzone, IAM users, etc. Deploy CodeBuild Project \u00b6 First, clone this project. git clone https://github.com/aws-quickstart/quickstart-ssp-amazon-eks.git cd quickstart-ssp-amazon-eks Install CDK (please review and install any missing pre-requisites for your environment) npm install -g aws-cdk@1.104.0 Install the dependencies for this project. npm install Bootstrap CDK into the target AWS account and region. env CDK_NEW_BOOTSTRAP = 1 cdk bootstrap \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws://<ACCOUNT_ID>/<AWS_REGION> Connect GitHub organization to CodeBuild (as described here ). export GITHUB_TOKEN = <personal_access_token> aws codebuild import-source-credentials \\ --server-type GITHUB \\ --auth-type PERSONAL_ACCESS_TOKEN \\ --token $GITHUB_TOKEN Optionally, upload a cdk.context.json file into an S3 bucket which can be accessed by the CodeBuild project. aws s3 cp cdk.context.json \\ s3://<s3bucket>/cdk.context.json Deploy the CodeBuild Project. Use the --parameters GitHubOwner=<value> to override the value for owner used in the CodeBuild project. If you do not specify the input parameter we will try to use aws-quickstart by default. cdk deploy -a \"npx ts-node ci/index.ts\" \\ --parameters GitHubOwner = <GitHubOwner> \\ --context eks.default.context-location = s3://<s3bucket>/cdk.context.json \" After the deployment is completed the CodeBuild project will be configured to build and deploy the blueprint stack on every pull-request merge to the main branch. Note : Update build badge url the top level README . The badge url can be obtained by running the following AWS cli command. aws codebuild batch-get-projects \\ --names QuickstartSspAmazonEksBuild | \\ jq -r '.projects[0].badge.badgeRequestUrl'","title":"CodeBuild CI"},{"location":"internal/ci/#codebuild-ci","text":"This example shows how to enable a CodeBuild based Continuous Integration process for the SSP blueprint. The CodeBuild project is provisioned using a CDK application. The buildspec.yml provided deploys the sample blueprint stacks provided in examples . The buildspec can be used directly if you wish to setup the CodeBuild project manually through the console or via the CLI. Optionally, you can also provide an S3 bucket location for a cdk.context.json that contains key-values for any context you want to provide to your application such as route 53 domain, domain account, subzone, IAM users, etc.","title":"CodeBuild CI"},{"location":"internal/ci/#deploy-codebuild-project","text":"First, clone this project. git clone https://github.com/aws-quickstart/quickstart-ssp-amazon-eks.git cd quickstart-ssp-amazon-eks Install CDK (please review and install any missing pre-requisites for your environment) npm install -g aws-cdk@1.104.0 Install the dependencies for this project. npm install Bootstrap CDK into the target AWS account and region. env CDK_NEW_BOOTSTRAP = 1 cdk bootstrap \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws://<ACCOUNT_ID>/<AWS_REGION> Connect GitHub organization to CodeBuild (as described here ). export GITHUB_TOKEN = <personal_access_token> aws codebuild import-source-credentials \\ --server-type GITHUB \\ --auth-type PERSONAL_ACCESS_TOKEN \\ --token $GITHUB_TOKEN Optionally, upload a cdk.context.json file into an S3 bucket which can be accessed by the CodeBuild project. aws s3 cp cdk.context.json \\ s3://<s3bucket>/cdk.context.json Deploy the CodeBuild Project. Use the --parameters GitHubOwner=<value> to override the value for owner used in the CodeBuild project. If you do not specify the input parameter we will try to use aws-quickstart by default. cdk deploy -a \"npx ts-node ci/index.ts\" \\ --parameters GitHubOwner = <GitHubOwner> \\ --context eks.default.context-location = s3://<s3bucket>/cdk.context.json \" After the deployment is completed the CodeBuild project will be configured to build and deploy the blueprint stack on every pull-request merge to the main branch. Note : Update build badge url the top level README . The badge url can be obtained by running the following AWS cli command. aws codebuild batch-get-projects \\ --names QuickstartSspAmazonEksBuild | \\ jq -r '.projects[0].badge.badgeRequestUrl'","title":"Deploy CodeBuild Project"},{"location":"internal/readme-internal/","text":"Description \u00b6 This is an internal readme for development processes that should be followed for this repository. Local Development \u00b6 This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build. Triggering E2E Testing \u00b6 The CI system attached to the project will run all stacks under examples as end-to-end integration testing. Currently it works the following way: A human maintainer reviews the pr code to ensure it is not malicious If the code is trusted and the maintainer wishes to run e2e tests, they comment on the pr with /do-e2e-tests. This will trigger the build and test. Any visibility into the state can only occur through AWS maintainers. If job succeeds, the CI bot approves the PR. If it fails it requests changes. Details on what failed will need to be manually shared with external contributors. At present shapirov103, kcoleman731 and askulkarni2 have rights to invoke the bot. Publishing \u00b6 At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): make build (compile) npm publish (this will require credentials to npm) Submitting Changes \u00b6 For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Description"},{"location":"internal/readme-internal/#description","text":"This is an internal readme for development processes that should be followed for this repository.","title":"Description"},{"location":"internal/readme-internal/#local-development","text":"This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build.","title":"Local Development"},{"location":"internal/readme-internal/#triggering-e2e-testing","text":"The CI system attached to the project will run all stacks under examples as end-to-end integration testing. Currently it works the following way: A human maintainer reviews the pr code to ensure it is not malicious If the code is trusted and the maintainer wishes to run e2e tests, they comment on the pr with /do-e2e-tests. This will trigger the build and test. Any visibility into the state can only occur through AWS maintainers. If job succeeds, the CI bot approves the PR. If it fails it requests changes. Details on what failed will need to be manually shared with external contributors. At present shapirov103, kcoleman731 and askulkarni2 have rights to invoke the bot.","title":"Triggering E2E Testing"},{"location":"internal/readme-internal/#publishing","text":"At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): make build (compile) npm publish (this will require credentials to npm)","title":"Publishing"},{"location":"internal/readme-internal/#submitting-changes","text":"For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Submitting Changes"},{"location":"resource-providers/","text":"Resource Providers \u00b6 Terminology \u00b6 Resource A resource is a CDK construct that implements IResource interface from @aws-cdk/core which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 IHostedZone , an ACM certificate ICertificate , a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams. ResourceProvider A resource provider is a core SSP concept that enables customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint. Use Cases \u00b6 ClusterAddOn and Team implementations require AWS resources that can be shared across several constructs. For example, ExternalDnsAddOn requires an array of hosted zones that will be used for integration with Route53. NginxAddOn requires a certificate and hosted zone (for DNS validation) in order to use TLS termination. VPC may be used inside add-ons and team constructs to look up VPC CIDR and subnets. The SSP framework provides ability to register a resource provider under an arbitrary name and make it available in the resource context, which is available to all add-ons and teams. With this capability, customers can either use existing resource providers or create their own and reference the provided resources inside add-ons, teams or other resource providers. Resource providers may depend on resources provided by other resource providers. For example, CertificateResourceProvider relies on a hosted zone resource, which is expected to be supplied by another provider. Example use cases: As a platform user, I must create a VPC using my enterprise standards and leverage it for the EKS Blueprint. Solution: create an implementation of ResourceProvider<IVpc> (or leverage an existing one) and register it with the blueprint (see Usage). As a platform user, I need to use an existing hosted zone for all external DNS names used with ingress objects of my workloads. Solution: use a predefined ImportHostedZoneProvider or LookupHostedZoneProvider to reference the existing hosted zone. As a platform user, I need to create an S3 bucket and use it in one or more Team implementations. Solution: create an implementation for an S3 Bucket resource provider and use the supplied resource inside teams. Contracts \u00b6 The API contract for a resource provider is represented by the ResourceProvider interface from the spi/resource-contracts module. export declare interface ResourceProvider < T extends IResource = IResource > { provide ( context : ResourceContext ) : T ; } Example implementations: class VpcResourceProvider implements ResourceProvider < IVpc > { provide ( context : ResourceContext ) : IVpc { const scope = context . scope ; // stack ... } } class DynamoDbTableResourceProvider implements ResourceProvider < ITable > { provide ( context : ResourceContext ) : ITable { ... } } Access to registered resources from other resource providers and/or add-ons and teams: /** * Provides API to register resource providers and get access to the provided resources. */ export class ResourceContext { /** * Adds a new resource provider and specifies the name under which the provided resource will be registered, * @param name Specifies the name key under which the provided resources will be registered for subsequent look-ups. * @param provider Implementation of the resource provider interface * @returns the provided resource */ public add < T extends cdk . IResource = cdk . IResource > ( name : string , provider : ResourceProvider < T > ) : T { ... } /** * Gets the provided resource by the supplied name. * @param name under which the resource provider was registered * @returns the resource or undefined if the specified resource was not found */ public get < T extends cdk . IResource = cdk . IResource > ( name : string ) : T | undefined { ... } } Convenience API to access registered resources from add-ons: /** * Cluster info supplies all contextual information on the cluster configuration, registered resources and add-ons * which could be leveraged by the framework, add-on implementations and teams. */ export class ClusterInfo { ... /** * Provides the resource context object associated with this instance of the EKS Blueprint. * @returns resource context object */ public getResourceContext () : ResourceContext { return this . resourceContext ; } /** * Provides the resource registered under supplied name * @param name of the resource to be returned * @returns Resource object or undefined if no resource was found */ public getResource < T extends cdk . IResource > ( name : string ) : T | undefined { ... } /** * Same as {@link getResource} but will fail if the specified resource is not found * @param name of the resource to be returned * @returns Resource object (fails if not found) */ public getRequiredResource < T extends cdk . IResource > ( name : string ) : T { ... } } Usage \u00b6 Registering Resource Providers for a Blueprint Note: GlobalResources.HostedZone and GlobalResources.Certificate are provided for convenience as commonly referenced constants. const myVpcId = ...; // e.g. app.node.tryGetContext('my-vpc', 'default) will look up property my-vpc in the cdk.json ssp . EksBlueprint . builder () // Specify VPC for the cluster (if not set, a new VPC will be provisioned as per EKS Best Practices) . resourceProvider ( GlobalResources . VPC , new VpcProvider ( myVpcId ) // Register hosted zone and give it a name of GlobalResources.HostedZone . resourceProvider ( GlobalResources . HostedZone , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) // Register certificate GlobalResources.Certificate name and reference the hosted zone registered in the previous step . resourceProvider ( GlobalResources . Certificate , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , GlobalResources . HostedZone )) . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zone for External DNS . addOns ( new ExternalDnsAddon ({ hostedZoneResources : [ GlobalResources . HostedZone ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' ); Registering Multiple Hosted Zones ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) // Register zone2 under the name of MyHostedZone2 . resourceProvider ( \"MyHostedZone2\" , new ImportHostedZoneProvider ( 'hosted-zone-id2' , 'my.otherdomain.com' )) // Register certificate and reference the hosted zone1 registered in the previous steps . resourceProvider ( \"MyCert\" , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , \"MyHostedZone1\" )) . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zones for External DNS . addOns ( new ExternalDnsAddon ({ hostedZoneResources : [ \"MyHostedZone1\" , \"MyHostedZone2\" ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : \"MyCert\" , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' ); Implementing Custom Resource Providers \u00b6 Select the type of the resource that you need. Let's say it will be an S3 Bucket. Note: it must be one of the derivatives/implementations of IResource interface. Implement ResourceProvider interface: class MyResourceProvider implements ssp . ResourceProvider < s3 . IBucket > { provide ( context : ssp.ResourceContext ) : s3 . IBucket { return new s3 . Bucket ( context . scope , \"mybucket\" ); } } Register your resource provider under an arbitrary name which must be unique in the current scope across all resource providers: ssp . EksBlueprint . builder () . resourceProvider ( \"mybucket\" , new MyResourceProvider ()) . addOns (...) . teams (...) . build (); Use the resource inside a custom add-on: class MyCustomAddOn implements ssp . ClusterAddOn { deploy ( clusterInfo : ClusterInfo ) : void | Promise < cdk . Construct > { const myBucket : s3.IBucket = clusterInfo . getRequiredResource ( 'mybucket' ); // will fail if mybucket does not exist // do something with the bucket } }","title":"Resource Providers"},{"location":"resource-providers/#resource-providers","text":"","title":"Resource Providers"},{"location":"resource-providers/#terminology","text":"Resource A resource is a CDK construct that implements IResource interface from @aws-cdk/core which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 IHostedZone , an ACM certificate ICertificate , a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams. ResourceProvider A resource provider is a core SSP concept that enables customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint.","title":"Terminology"},{"location":"resource-providers/#use-cases","text":"ClusterAddOn and Team implementations require AWS resources that can be shared across several constructs. For example, ExternalDnsAddOn requires an array of hosted zones that will be used for integration with Route53. NginxAddOn requires a certificate and hosted zone (for DNS validation) in order to use TLS termination. VPC may be used inside add-ons and team constructs to look up VPC CIDR and subnets. The SSP framework provides ability to register a resource provider under an arbitrary name and make it available in the resource context, which is available to all add-ons and teams. With this capability, customers can either use existing resource providers or create their own and reference the provided resources inside add-ons, teams or other resource providers. Resource providers may depend on resources provided by other resource providers. For example, CertificateResourceProvider relies on a hosted zone resource, which is expected to be supplied by another provider. Example use cases: As a platform user, I must create a VPC using my enterprise standards and leverage it for the EKS Blueprint. Solution: create an implementation of ResourceProvider<IVpc> (or leverage an existing one) and register it with the blueprint (see Usage). As a platform user, I need to use an existing hosted zone for all external DNS names used with ingress objects of my workloads. Solution: use a predefined ImportHostedZoneProvider or LookupHostedZoneProvider to reference the existing hosted zone. As a platform user, I need to create an S3 bucket and use it in one or more Team implementations. Solution: create an implementation for an S3 Bucket resource provider and use the supplied resource inside teams.","title":"Use Cases"},{"location":"resource-providers/#contracts","text":"The API contract for a resource provider is represented by the ResourceProvider interface from the spi/resource-contracts module. export declare interface ResourceProvider < T extends IResource = IResource > { provide ( context : ResourceContext ) : T ; } Example implementations: class VpcResourceProvider implements ResourceProvider < IVpc > { provide ( context : ResourceContext ) : IVpc { const scope = context . scope ; // stack ... } } class DynamoDbTableResourceProvider implements ResourceProvider < ITable > { provide ( context : ResourceContext ) : ITable { ... } } Access to registered resources from other resource providers and/or add-ons and teams: /** * Provides API to register resource providers and get access to the provided resources. */ export class ResourceContext { /** * Adds a new resource provider and specifies the name under which the provided resource will be registered, * @param name Specifies the name key under which the provided resources will be registered for subsequent look-ups. * @param provider Implementation of the resource provider interface * @returns the provided resource */ public add < T extends cdk . IResource = cdk . IResource > ( name : string , provider : ResourceProvider < T > ) : T { ... } /** * Gets the provided resource by the supplied name. * @param name under which the resource provider was registered * @returns the resource or undefined if the specified resource was not found */ public get < T extends cdk . IResource = cdk . IResource > ( name : string ) : T | undefined { ... } } Convenience API to access registered resources from add-ons: /** * Cluster info supplies all contextual information on the cluster configuration, registered resources and add-ons * which could be leveraged by the framework, add-on implementations and teams. */ export class ClusterInfo { ... /** * Provides the resource context object associated with this instance of the EKS Blueprint. * @returns resource context object */ public getResourceContext () : ResourceContext { return this . resourceContext ; } /** * Provides the resource registered under supplied name * @param name of the resource to be returned * @returns Resource object or undefined if no resource was found */ public getResource < T extends cdk . IResource > ( name : string ) : T | undefined { ... } /** * Same as {@link getResource} but will fail if the specified resource is not found * @param name of the resource to be returned * @returns Resource object (fails if not found) */ public getRequiredResource < T extends cdk . IResource > ( name : string ) : T { ... } }","title":"Contracts"},{"location":"resource-providers/#usage","text":"Registering Resource Providers for a Blueprint Note: GlobalResources.HostedZone and GlobalResources.Certificate are provided for convenience as commonly referenced constants. const myVpcId = ...; // e.g. app.node.tryGetContext('my-vpc', 'default) will look up property my-vpc in the cdk.json ssp . EksBlueprint . builder () // Specify VPC for the cluster (if not set, a new VPC will be provisioned as per EKS Best Practices) . resourceProvider ( GlobalResources . VPC , new VpcProvider ( myVpcId ) // Register hosted zone and give it a name of GlobalResources.HostedZone . resourceProvider ( GlobalResources . HostedZone , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) // Register certificate GlobalResources.Certificate name and reference the hosted zone registered in the previous step . resourceProvider ( GlobalResources . Certificate , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , GlobalResources . HostedZone )) . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zone for External DNS . addOns ( new ExternalDnsAddon ({ hostedZoneResources : [ GlobalResources . HostedZone ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' ); Registering Multiple Hosted Zones ssp . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) // Register zone2 under the name of MyHostedZone2 . resourceProvider ( \"MyHostedZone2\" , new ImportHostedZoneProvider ( 'hosted-zone-id2' , 'my.otherdomain.com' )) // Register certificate and reference the hosted zone1 registered in the previous steps . resourceProvider ( \"MyCert\" , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , \"MyHostedZone1\" )) . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zones for External DNS . addOns ( new ExternalDnsAddon ({ hostedZoneResources : [ \"MyHostedZone1\" , \"MyHostedZone2\" ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : \"MyCert\" , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' );","title":"Usage"},{"location":"resource-providers/#implementing-custom-resource-providers","text":"Select the type of the resource that you need. Let's say it will be an S3 Bucket. Note: it must be one of the derivatives/implementations of IResource interface. Implement ResourceProvider interface: class MyResourceProvider implements ssp . ResourceProvider < s3 . IBucket > { provide ( context : ssp.ResourceContext ) : s3 . IBucket { return new s3 . Bucket ( context . scope , \"mybucket\" ); } } Register your resource provider under an arbitrary name which must be unique in the current scope across all resource providers: ssp . EksBlueprint . builder () . resourceProvider ( \"mybucket\" , new MyResourceProvider ()) . addOns (...) . teams (...) . build (); Use the resource inside a custom add-on: class MyCustomAddOn implements ssp . ClusterAddOn { deploy ( clusterInfo : ClusterInfo ) : void | Promise < cdk . Construct > { const myBucket : s3.IBucket = clusterInfo . getRequiredResource ( 'mybucket' ); // will fail if mybucket does not exist // do something with the bucket } }","title":"Implementing Custom Resource Providers"}]}